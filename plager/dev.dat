a method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping human language technology
a survey of existing methods for stopping active learning ( al ) reveals the needs for methods that are : more widely applicable ; more aggressive in saving annotations ; and more stable across changing datasets . a new method for stopping al based on stabilizing predictions is presented that addresses these needs . furthermore , stopping methods are required to handle a broad range of different annotation/performance tradeoff valuations . despite this , the existing body of work is dominated by conservative methods with little ( if any ) attention paid to providing users with control over the behavior of stopping methods . the proposed method is shown to fill a gap in the level of aggressiveness available for stopping al and supports providing users with control over stopping behavior .

animacy annotation in the hindi treebank itisree jena , riyaz ahmad bhat , sambhav jain and dipti misra sharma
in this paper , we discuss our efforts to annotate nominals in the hindi treebank with the semantic property of animacy . although the treebank already encodes lexical information at a number of levels such as morph and part of speech , the addition of animacy information seems promising given its relevance to varied linguistic phenomena . the suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution , syntactic parsing , verb classification and argument differentiation .

towards a robust framework for the semantic representation of temporal expressions in cultural legacy data national gallery of ireland
date and time descriptors play an important role in cultural record keeping . as part of digital access and information retrieval on heritage databases it is becoming increasingly important that date descriptors are not matched as strings but that their semantics are properly understood and interpreted by man and machine alike . this paper describes a prototype system designed to resolve temporal expressions from english language cultural heritage records to iso 8601 compatible date expressions . the architecture we advocate calls for a two stage resolution with a semantic layer between the input and iso 8601 output . the system is inspired by a similar system for german language records and was tested on real world data from the national gallery of ireland in dublin . results from an evaluation with two senior art and metadata experts from the gallery are reported .

extracting gene regulation networks using linear-chain conditional random fields and rules slavko zitnik marinka zitnik blaz zupan marko bajec
published literature in molecular genetics may collectively provide much information on gene regulation networks . dedicated computational approaches are required to sip through large volumes of text and infer gene interactions . we propose a novel sieve-based relation extraction system that uses linear-chain conditional random fields and rules . also , we introduce a new skip-mention data representation to enable distant relation extraction using first-order models . to account for a variety of relation types , multiple models are inferred . the system was applied to the bionlp 2013 gene regulation network shared task . our approach was ranked first of five , with a slot error rate of 0.73 .

citations in the digital library of classics : extracting canonical references by using conditional random fields the perseus project
scholars of classics cite ancient texts by using abridged citations called canonical references . in the scholarly digital library , canonical references create a complex textile of links between ancient and modern sources reflecting the deep hypertextual nature of texts in this field . this paper aims to demonstrate the suitability of conditional random fields ( crf ) for extracting this particular kind of reference from unstructured texts in order to enhance the capabilities of navigating and aggregating scholarly electronic resources . in particular , we developed a parser which recognizes word level n-grams of a text as being canonical references by using a crf model trained with both positive and negative examples .

word-level language identification using crf : code-switching shared task report of msr india system kalika bali monojit choudhury
we describe a crf based system for word-level language identification of code-mixed text . our method uses lexical , contextual , character n-gram , and special character features , and therefore , can easily be replicated across languages . its performance is benchmarked against the test sets provided by the shared task on code-mixing ( solorio et al. , 2014 ) for four language pairs , namely , englishspanish ( en-es ) , english-nepali ( en-ne ) , english-mandarin ( en-cn ) , and standard arabic-arabic ( ar-ar ) dialects . the experimental results show a consistent performance across the language pairs .

how to change a persons mind : understanding the difference between the effects and consequences of speech acts
this paper discusses a planner of the semantics of utterances , whose essential design is an epistemic theorem prover . the planner was designed for the purpose of planning communicative actions , whose effects are famously unknowable and unobservable by the doer/speaker , and depend on the beliefs of and inferences made by the recipient/hearer . the fully implemented model can achieve goals that do not match action effects , but that are rather entailed by them , which it does by reasoning about how to act : state-space planning is interwoven with theorem proving in such a way that a theorem prover uses the effects of actions as hypotheses . the planner is able to model problematic conversational situations , including felicitous and infelicitous instances of bluffing , lying , sarcasm , and stating the obvious .

towards finding and fixing fragments : using ml to identify non-sentential utterances and their antecedents in multi-party dialogue
non-sentential utterances ( e.g. , shortanswers as in who came to the party peter . ) are pervasive in dialogue . as with other forms of ellipsis , the elided material is typically present in the context ( e.g. , the question that a short answer answers ) . we present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue . we compare the performance of several learning algorithms , using a mixture of structural and lexical features , and show that the task of identifying antecedents given a fragment can be learnt successfully ( f ( 0.5 ) = .76 ) ; we discuss why the task of identifying fragments is harder ( f ( 0.5 ) = .41 ) and finally report on a combined task ( f ( 0.5 ) = .38 ) .

identification of patients with congestive heart failure using a division of medical division of medical division of medical
this paper addresses a very specific problem that happens to be common in health science research . we present a machine learning based method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes . this method relies on a perceptron neural network classifier trained on comparable amounts of positive and negative samples of clinical notes previously categorized by human experts . the documents are represented as feature vectors where features are a mix of single words and concept mappings to mesh and hicda ontologies . the method is designed and implemented to support a particular epidemiological study but has broader implications for clinical research . in this paper , we describe the method and present experimental classification results based on classification accuracy and positive predictive value .

jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach xiaofeng yu wai lam
in this paper , we investigate the problem of entity identification and relation extraction from encyclopedia articles , and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously . this modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits , as well as a great flexibility to incorporate a large collection of arbitrary , overlapping and nonindependent features . we show the parameter estimation algorithm of this model . moreover , we propose a new inference method , namely collective iterative classification ( cic ) , to find the most likely assignments for both entities and relations . we evaluate our model on real-world data from wikipedia for this task , and compare with current state-of-the-art pipeline and joint models , demonstrating the effectiveness and feasibility of our approach .

fast , cheap , and creative : evaluating translation quality using amazons mechanical turk ccb cs jhu edu
manual evaluation of translation quality is generally thought to be excessively time consuming and expensive . we explore a fast and inexpensive way of doing it using amazons mechanical turk to pay small sums to a large number of non-expert annotators . for $ 10 we redundantly recreate judgments from a wmt08 translation task . we find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality , and correlate more strongly with expert judgments than bleu does . we go on to show that mechanical turk can be used to calculate human-mediated translation edit rate ( hter ) , to conduct reading comprehension experiments with machine translation , and to create high quality reference translations .

building a statistical machine translation system from scratch : how much bang for the buck can we expect
we report on our experience with building a statistical mt system from scratch , including the creation of a small parallel tamilenglish corpus , and the results of a taskbased pilot evaluation of statistical mt systems trained on sets of ca . 1300 and ca . 5000 parallel sentences of tamil and english data . our results show that even with apparently incomprehensible system output , humans without any knowledge of tamil can achieve performance rates as high as 86 % accuracy for topic identification , 93 % recall for document retrieval , and 64 % recall on question answering ( plus an additional 14 % partially correct answers ) .

some empirical findings on dialogue management and domain ontologies in dialogue systems implications from an evaluation of birdquest
in this paper we present implications for development of dialogue systems , based on an evaluation of the system birdquest which combine dialogue interaction with information extraction . a number of issues detected during the evaluation concerning primarily dialogue management , and domain knowledge representation and use are presented and discussed .

mining user reviews : from specification to summarization ministry of education , china ministry of education , china
this paper proposes a method to extract product features from user reviews and generate a review summary . this method only relies on product specifications , which usually are easy to obtain . other resources like segmenter , pos tagger or parser are not required . at feature extraction stage , multiple specifications are clustered to extend the vocabulary of product features . hierarchy structure information and unit of measurement information are mined from the specification to improve the accuracy of feature extraction . at summary generation stage , hierarchy information in specifications is used to provide a natural conceptual view of product features .

augmenting translation models with simulated acoustic confusions for improved spoken language translation yulia tsvetkov florian metze chris dyer
we propose a novel technique for adapting text-based statistical machine translation to deal with input from automatic speech recognition in spoken language translation tasks . we simulate likely misrecognition errors using only a source language pronunciation dictionary and language model ( i.e. , without an acoustic model ) , and use these to augment the phrase table of a standard mt system . the augmented system can thus recover from recognition errors during decoding using synthesized phrases . using the outputs of five different english asr systems as input , we find consistent and significant improvements in translation quality . our proposed technique can also be used in conjunction with lattices as asr output , leading to further improvements .

the map task dialogue system : a test-bed for modelling human-like dialogue raveesh meena gabriel skantze joakim gustafson
the demonstrator presents a test-bed for collecting data on humancomputer dialogue : a fully automated dialogue system that can perform map task with a user . in a first step , we have used the test-bed to collect humancomputer map task dialogue data , and have trained various data-driven models on it for detecting feedback response locations in the users speech . one of the trained models has been tested in user interactions and was perceived better in comparison to a system using a random model . the demonstrator will exhibit three versions of the map task dialogue systemeach using a different trained data-driven model of response location detection .

crowd prefers the middle path : a new iaa metric for crowdsourcing reveals turker biases in query segmentation
query segmentation , like text chunking , is the first step towards query understanding . in this study , we explore the effectiveness of crowdsourcing for this task . through carefully designed control experiments and inter annotator agreement metrics for analysis of experimental data , we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal ( often only two ) parts . similarly , in the case of hierarchical or nested segmentation , turkers have a strong preference towards balanced binary trees .

aligning medical domain ontologies for clinical query extraction pinar wennerbergsiemens ag , munich germanytu darmstadt , darmstadt germany
often , there is a need to use the knowledge from multiple ontologies . this is particularly the case within the context of medical imag-ing , where a single ontology is not enough to provide the complementary knowledge aboutanatomy , radiology and diseases that is re-quired by the related applications . conse-quently , semantic integration of these differ-ent but related types of medical knowledge that is present in disparate domain ontologies becomes necessary . medical ontology align-ment addresses this need by identifying the semantically equivalent concepts across mul-tiple medical ontologies . the resulting alignments can then be used to annotate the medical images and related patient text data . a corresponding semantic search engine that operates on these annotations ( i.e . align-ments ) instead of simple keywords can , in this way , deliver the clinical users a coherent set of medical image and patient text data .

examining the impacts of dialogue content and system automation on affect models in a spoken tutorial dialogue system
many dialogue system developers use data gathered from previous versions of the dialogue system to build models which enable the system to detect and respond to users affect . previous work in the dialogue systems community for domain adaptation has shown that large differences between versions of dialogue systems affect performance of ported models . thus , we wish to investigate how more minor differences , like small dialogue content changes and switching from a wizarded system to a fully automated system , influence the performance of our affect detection models . we perform a post-hoc experiment where we use various data sets to train multiple models , and compare against a test set from the most recent version of our dialogue system . analyzing these results strongly suggests that these differences do impact these models performance .

a dash of sentiment beneath the surface meaning song feng jun seok kang polina kuznetsova yejin choi
understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text , as seemingly objective statements often allude nuanced sentiment of the writer , and even purposefully conjure emotion from the readers minds . the focus of this paper is drawing nuanced , connotative sentiments from even those words that are objective on the surface , such as intelligence , human , and cheesecake . we propose induction algorithms encoding a diverse set of linguistic insights ( semantic prosody , distributional similarity , semantic parallelism of coordination ) and prior knowledge drawn from lexical resources , resulting in the first broad-coverage connotation lexicon .

varro : an algorithm and toolkit for regular structure discovery in centrum voor computerlingustiek , ku leuven
the varro toolkit is a system for identifying and counting a major class of regularity in treebanks and annotated natural language data in the form of treestructures : frequently recurring unordered subtrees . this software has been designed for use in linguistics to be maximally applicable to actually existing treebanks and other stores of tree-structurable natural language data . it minimizes memory use so that moderately large treebanks are tractable on commonly available computer hardware . this article introduces condensed canonically ordered trees as a data structure for efficiently discovering frequently recurring unordered subtrees .

frequently asked questions retrieval for croatian based on semantic textual similarity mladen karan lovro mak jan najder
frequently asked questions ( faq ) are an efficient way of communicating domainspecific information to the users . unlike general purpose retrieval engines , faq retrieval engines have to address the lexical gap between the query and the usually short answer . in this paper we describe the design and evaluation of a faq retrieval engine for croatian . we frame the task as a binary classification problem , and train a model to classify each faq as either relevant or not relevant for a given query . we use a variety of semantic textual similarity features , including term overlap and vector space features . we train and evaluate on a faq test collection built specifically for this purpose . our best-performing model reaches 0.47 of mean reciprocal rank , i.e. , on average ranks the relevant answer among the top two returned answers .

new ranking algorithms for parsing and tagging : kernels over discrete structures , and the voted perceptron
this paper introduces new learning algorithms for natural language processing based on the perceptron algorithm . we show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the all subtrees ( dop ) representation described by ( bod 1998 ) , or a representation tracking all sub-fragments of a tagged sentence . we give experimental results showing significant improvements on two tasks : parsing wall street journal text , and namedentity extraction from web data .

discovering global patterns in linguistic networks through spectral analysis : a case study of the consonant inventories
recent research has shown that language and the socio-cognitive phenomena associated with it can be aptly modeled and visualized through networks of linguistic entities . however , most of the existing works on linguistic networks focus only on the local properties of the networks . this study is an attempt to analyze the structure of languages via a purely structural technique , namely spectral analysis , which is ideally suited for discovering the global correlations in a network . application of this technique to phonet , the co-occurrence network of consonants , not only reveals several natural linguistic principles governing the structure of the consonant inventories , but is also able to quantify their relative importance . we believe that this powerful technique can be successfully applied , in general , to study the structure of natural languages .

a corpus of sentence-level revisions in academic writing : a step towards understanding statement strength in communication
the strength with which a statement is made can have a significant impact on the audience . for example , international relations can be strained by how the media in one country describes an event in another ; and papers can be rejected because they overstate or understate their findings . it is thus important to understand the effects of statement strength . a first step is to be able to distinguish between strong and weak statements . however , even this problem is understudied , partly due to a lack of data . since strength is inherently relative , revisions of texts that make claims are a natural source of data on strength differences . in this paper , we introduce a corpus of sentence-level revisions from academic writing . we also describe insights gained from our annotation efforts for this task .

a domain adaption word segmenter beijing information science & beijing information science & beijing information science &
we present a chinese word segmentation system which ran on the closed track of the simplified chinese word segmentation task of cips-sighan-clp 2010 bakeoffs . our segmenter was built using a hmm . to fulfill the cross-domain segmentation task , we use semi-supervised machine learning method to get the hmm model . finally we get the mean result of four domains : p=0.719 , r=0.72

demonstration of the parlance system : a data-driven , incremental , spoken dialogue system for interactive search
the parlance system for interactive search processes dialogue at a microturn level , displaying dialogue phenomena that play a vital role in human spoken conversation . these dialogue phenomena include more natural turn-taking through rapid system responses , generation of backchannels , and user barge-ins . the parlance demonstration system differentiates from other incremental systems in that it is data-driven with an infrastructure that scales well .

automatic part-of-speech tagging for bengali : an approach for morphologically rich languages in a poor resource scenario
this paper describes our work on building part-of-speech ( pos ) tagger for bengali . we have use hidden markov model ( hmm ) and maximum entropy ( me ) based stochastic taggers . bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words . since only a small labeled training set is available ( 45,000 words ) , simple stochastic approach does not yield very good results . in this work , we have studied the effect of using a morphological analyzer to improve the performance of the tagger . we find that the use of morphology helps improve the accuracy of the tagger especially when less amount of tagged corpora are available .

estimating class priors in domain adaptation for word sense disambiguation yee seng chan and hwee tou ng
instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) . this in turn affects the accuracy of word sense disambiguation ( wsd ) systems trained and applied on different domains . this paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations . by using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in wsd accuracy .

word translation prediction for morphologically rich languages with bilingual neural networks ke tran arianna bisazza christof monz
translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models . we present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy . our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering . we report significant improvements in word translation prediction accuracy for three morphologically rich target languages . in addition , preliminary results for integrating our approach into a largescale english-russian statistical machine translation system show small but statistically significant improvements in translation quality .

automatic detection and language identification of multilingual documents marco lui , jey han lau and timothy baldwin
language identification is the task of automatically detecting the language ( s ) present in a document based on the content of the document . in this work , we address the problem of detecting documents that contain text from more than one language ( multilingual documents ) . we introduce a method that is able to detect that a document is multilingual , identify the languages present , and estimate their relative proportions . we demonstrate the effectiveness of our method over synthetic data , as well as real-world multilingual documents collected from the web .

structural semantic interconnection : a knowledge-based approach to word dipartimento di informatica , dipartimento di informatica ,
in this paper we describe the ssi algorithm , a structural pattern matching algorithm for wsd . the algorithm has been applied to the gloss disambiguation task of senseval-3 .

maarten van gompel , iris hendrickx , antal van den bosch language and translation technology team ,
we present a new cross-lingual task for semeval concerning the translation of l1 fragments in an l2 context . the task is at the boundary of cross-lingual word sense disambiguation and machine translation . it finds its application in the field of computer-assisted translation , particularly in the context of second language learning . translating l1 fragments in an l2 context allows language learners when writing in a target language ( l2 ) to fall back to their native language ( l1 ) whenever they are uncertain of the right word or phrase .

discourse relation configurations in turkish and an annotation environment berfin aktas and cem bozsahin and deniz zeyrek
in this paper , we describe an annotation environment developed for the marking of discourse structures in turkish , and the kinds of discourse relation configurations that led to its design .

unn-weps : web person search using co-present names and lexical chains newcastle upon tyne newcastle upon tyne
we describe a system , unn-weps for identifying individuals from web pages using data from semeval task 13. our system is based on using co-presence of person names to form seed clusters . these are then extended with pages that are deemed conceptually similar based on a lexical chaining analysis computed using rogets thesaurus . finally , a single link hierarchical agglomerative clustering algorithm merges the enhanced clusters for individual entity recognition . unn-weps achieved an average purity of 0.6 , and inverse purity of 0.73 .

combining task and dialogue streams in unsupervised dialogue act models aysu ezen-can and kristy elizabeth boyer
unsupervised machine learning approaches hold great promise for recognizing dialogue acts , but the performance of these models tends to be much lower than the accuracies reached by supervised models . however , some dialogues , such as task-oriented dialogues with parallel task streams , hold rich information that has not yet been leveraged within unsupervised dialogue act models . this paper investigates incorporating task features into an unsupervised dialogue act model trained on a corpus of human tutoring in introductory computer science . experimental results show that incorporating task features and dialogue history features significantly improve unsupervised dialogue act classification , particularly within a hierarchical framework that gives prominence to dialogue history . this work constitutes a step toward building high-performing unsupervised dialogue act models that will be used in the next generation of task-oriented dialogue systems .

extracting transfer rules for multiword expressions from parallel corpora division of linguistics and multilingual studies ,
this paper presents a procedure for extracting transfer rules for multiword expressions from parallel corpora for use in a rule based japanese-english mt system . we show that adding the multi-word rules improves translation quality and sketch ideas for learning more such rules .

expanding the language model in a low-resource hybrid mt system george tambouratzis sokratis sofianopoulos marina vassiliou
the present article investigates the fusion of different language models to improve translation accuracy . a hybrid mt system , recentlydeveloped in the european commissionfunded presemt project that combines example-based mt and statistical mt principles is used as a starting point . in this article , the syntactically-defined phrasal language models ( nps , vps etc . ) used by this mt system are supplemented by n-gram language models to improve translation accuracy . for specific structural patterns , n-gram statistics are consulted to determine whether the pattern instantiations are corroborated . experiments indicate improvements in translation accuracy .

a geo-coding service encompassing a geo-parsing tool and integrated digital main library building main library building
we describe a basic geo-coding service encompassing a geo-parsing tool and integrated digital gazetteer service . the development of a geo-parser comes from the need to explicitly georeference large resource collections such as the statistical accounts of scotland which currently only contain implicit georeferences in the form of placennames thus making such collections inherently geographically searchable .

cross-lingual validity of propbank in the manual annotation of french lonneke van der plas tanja samardzic
methods that re-use existing mono-lingual semantic annotation resources to annotate a new language rely on the hypothesis that the semantic annotation scheme used is cross-lingually valid . we test this hypothesis in an annotation agreement study . we show that the annotation scheme can be applied cross-lingually .

learning to detect conversation focus of threaded discussions donghui feng erin shaw jihie kim eduard hovy
in this paper we present a novel featureenriched approach that learns to detect the conversation focus of threaded discussions by combining nlp analysis and ir techniques . using the graph-based algorithm hits , we integrate different features such as lexical similarity , poster trustworthiness , and speech act analysis of human conversations with featureoriented link generation functions . it is the first quantitative study to analyze human conversation focus in the context of online discussions that takes into account heterogeneous sources of evidence . experimental results using a threaded discussion corpus from an undergraduate class show that it achieves significant performance improvements compared with the baseline system .

self-training and co-training in biomedical word sense disambiguation national library of medicine national library of medicine
word sense disambiguation ( wsd ) is an intermediate task within information retrieval and information extraction , attempting to select the proper sense of ambiguous words . due to the scarcity of training data , semi-supervised learning , which profits from seed annotated examples and a large set of unlabeled data , are worth researching . we present preliminary results of two semi-supervised learning algorithms on biomedical word sense disambiguation . both methods add relevant unlabeled examples to the training set , and optimal parameters are similar for each ambiguous word .

proximity in context : an empirically grounded computational model of proximity for processing topological spatial expressions
the paper presents a new model for contextdependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene . the paper discusses novel psycholinguistic experimental data that tests and verifies the model . the model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g . x near y ) . the model can help motivate the choice between topological and projective prepositions .

an effective method of using web based information for relation extraction yong wai keong , stanley
we propose a method that incorporates paraphrase information from the web to boost the performance of a supervised relation extraction system . contextual information is extracted from the web using a semi-supervised process , and summarized by skip-bigram overlap measures over the entire extract . this allows the capture of local contextual information as well as more distant associations . we observe a statistically significant boost in relation extraction performance . we investigate two extensions , thematic clustering and hypernym expansion . in tandem with thematic clustering to reduce noise in our paraphrase extraction , we attempt to increase the coverage of our search for paraphrases using hypernym expansion . evaluation of our method on the ace 2004 corpus shows that it out-performs the baseline svm-based supervised learning algorithm across almost all major ace relation types , by a margin of up to 31 % .

how to avoid burning ducks : combining linguistic analysis and corpus statistics for german compound processing
compound splitting is an important problem in many nlp applications which must be solved in order to address issues of data sparsity . previous work has shown that linguistic approaches for german compound splitting produce a correct splitting more often , but corpus-driven approaches work best for phrase-based statistical machine translation from german to english , a worrisome contradiction . we address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance .

adaptive information extraction for complex biomedical tasks donghui feng gully burns eduard hovy information sciences insitute
biomedical information extraction tasks are often more complex and contain uncertainty at each step during problem solving processes . we present an adaptive information extraction framework and demonstrate how to explore uncertainty using feedback integration .

unimelb nlp-core : integrating predictions from multiple domains and feature sets for estimating semantic textual similarity
in this paper we present our systems for calculating the degree of semantic similarity between two texts that we submitted to the semantic textual similarity task at semeval2013 . our systems predict similarity using a regression over features based on the following sources of information : string similarity , topic distributions of the texts based on latent dirichlet alocation , and similarity between the documents returned by an information retrieval engine when the target texts are used as queries . we also explore methods for integrating predictions using different training datasets and feature sets . our best system was ranked 17th out of 89 participating systems . in our post-task analysis , we identify simple changes to our system that further improve our results .

representation of morphosyntactic units and coordination structures in the turkish dependency treebank umut sulubacak gulsen eryigit
this paper presents our preliminary conclusions as part of an ongoing effort to construct a new dependency representation framework for turkish . we aim for this new framework to accommodate the highly agglutinative morphology of turkish as well as to allow the annotation of unedited web data , and shape our decisions around these considerations . in this paper , we firstly describe a novel syntactic representation for morphosyntactic sub-word units ( namely inflectional groups ( igs ) in turkish ) which allows inter-ig relations to be discerned with perfect accuracy without having to hide lexical information . secondly , we investigate alternative annotation schemes for coordination structures and present a better scheme ( nearly 11 % increase in recall scores ) than the one in turkish treebank ( oflazer et al , 2003 ) for both parsing accuracies and compatibility for colloquial language .

comparison of the baseline knowledge- , corpus- , and web-based similarity measures for semantic relations extraction
unsupervised methods of semantic relations extraction rely on a similarity measure between lexical units . similarity measures differ both in kinds of information they use and in the ways how this information is transformed into a similarity score . this paper is making a step further in the evaluation of the available similarity measures within the context of semantic relation extraction . we compare 21 baseline measures 8 knowledge-based , 4 corpus-based , and 9 web-based metrics with the bless dataset . our results show that existing similarity measures provide significantly different results , both in general performances and in relation distributions . we conclude that the results suggest developing a combined similarity measure .

two ways to use a noisy parallel news corpus for improving statistical souhir gahbiche-braham hele`ne bonneau-maynard
in this paper , we present two methods to use a noisy parallel news corpus to improve statistical machine translation ( smt ) systems . taking full advantage of the characteristics of our corpus and of existing resources , we use a bootstrapping strategy , whereby an existing smt engine is used both to detect parallel sentences in comparable data and to provide an adaptation corpus for translation models . mt experiments demonstrate the benefits of various combinations of these strategies .

using smaller constituents rather than sentences in active learning for japanese dependency parsing yahoo japan corporation
we investigate active learning methods for japanese dependency parsing . we propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically . furthermore , we utilize syntactic constraints of japanese to obtain more labeled examples from precious labeled ones that annotators give . experimental results show that our proposed methods improve considerably the learning curve of japanese dependency parsing . in order to achieve an accuracy of over 88.3 % , one of our methods requires only 34.4 % of labeled examples as compared to passive learning .

using derivation trees for informative treebank inter-annotator and mohamed maamouri linguistic data consortium beatrice santorini and
this paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement . this system makes for a more informative iaa evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types . we evaluate the system on two corpora - ( 1 ) a corpus of english web text , and ( 2 ) a corpus of modern british english .

the coding scheme for annotating extended nominal coreference and bridging anaphora in the prague dependency treebank
the present paper outlines an ongoing project of annotation of the extended nominal coreference and the bridging anaphora in the prague dependency treebank . we describe the annotation scheme with respect to the linguistic classification of coreferential and bridging relations and focus also on details of the annotation process from the technical point of view . we present methods of helping the annotators by a pre-annotation and by several useful features implemented in the annotation tool . our method of the inter-annotator agreement is focused on the improvement of the annotation guidelines ; we present results of three subsequent measurements of the agreement .

extracting key phrases to disambiguate personal name queries in web search danushka bollegala yutaka matsuo
assume that you are looking for information about a particular person . a search engine returns many pages for that persons name . some of these pages may be on other people with the same name . one method to reduce the ambiguity in the query and filter out the irrelevant pages , is by adding a phrase that uniquely identifies the person we are interested in from his/her namesakes . we propose an unsupervised algorithm that extracts such phrases from the web . we represent each document by a term-entity model and cluster the documents using a contextual similarity metric . we evaluate the algorithm on a dataset of ambiguous names . our method outperforms baselines , achieving over 80 % accuracy and significantly reduces the ambiguity in a web search task .

gro task : populating the gene regulation ontology with events and wellcome trust genome campus
semantic querying over the biomedical literature has gained popularity , where a semantic representation of biomedical documents is required . previous bionlp shared tasks exercised semantic event extraction with a small number of pre-defined event concepts . the gro task of the bionlp13-st imposes the challenge of dealing with over 100 gro concepts . its annotated corpus consists of 300 medline abstracts , and an analysis of interannotator agreement on the annotations by two experts shows kappa values between 43 % and 56 % . the results from the only participant are promising with f-scores 22 % ( events ) and 63 % ( relations ) , and also lead us to open issues such as the need to consider the ontology structure .

[ lvic-limsi ] : using syntactic features and multi-polarity words for sentiment analysis in twitter
this paper presents the contribution of our team at task 2 of semeval 2013 : sentiment analysis in twitter . we submitted a constrained run for each of the two subtasks . in the contextual polarity disambiguation subtask , we use a sentiment lexicon approach combined with polarity shift detection and tree kernel based classifiers . in the message polarity classification subtask , we focus on the influence of domain information on sentiment classification .

wrapping up a summary : from representation to generation josef steinberger and marco turchi and
the main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries . this is motivated by empirical evidence from tac 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries . we report encouraging preliminary results comparable to those attained by participating systems at tac 2009 .

evaluation of dependency parsers on unbounded dependencies joakim nivre laura rimell ryan mcdonald carlos gomez-rodrguez
we evaluate two dependency parsers , mstparser and maltparser , with respect to their capacity to recover unbounded dependencies in english , a type of evaluation that has been applied to grammarbased parsers and statistical phrase structure parsers but not to dependency parsers . the evaluation shows that when combined with simple post-processing heuristics , the parsers correctly recall unbounded dependencies roughly 50 % of the time , which is only slightly worse than two grammar-based parsers specifically designed to cope with such dependencies .

acquistion of the morphological structure of the lexicon based on lexical similarity and formal analogy
the paper presents a computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains . the model is purely lexemebased . the proposed morphological structure consists of ( 1 ) binary relations that connect each headword with words that are morphologically related , and especially with the members of its morphological family and its derivational series , and of ( 2 ) the analogies that hold between the words . the model has been tested on the lexicon of french using the tlfi machine readable dictionary .

a spoken dialogue interface for tv operations based on data collected by using woz method
the development of multi-channel digital broadcasting has generated a demand not only for new services but also for smart and highly functional capabilities in all broadcast-related devices . this is especially true of the television receivers on the viewer 's side . with the aim of achieving a friendly interface that anybody can use with ease , we built a prototype interface system that operates a television through voice interactions using natural language . at the current stage of our research , we are using this system to investigate the usefulness and problem areas of the spoken dialogue interface for television operations .

on the unification of syntactic annotations under the stanford dependency scheme : a case study
several incompatible syntactic annotation schemes are currently used by parsers and corpora in biomedical information extraction . the recently introduced stanford dependency scheme has been suggested to be a suitable unifying syntax formalism . in this paper , we present a step towards such unification by creating a conversion from the link grammar to the stanford scheme . further , we create a version of the bioinfer corpus with syntactic annotation in this scheme . we present an application-oriented evaluation of the transformation and assess the suitability of the scheme and our conversion to the unification of the syntactic annotations of bioinfer and the genia treebank . we find that a highly reliable conversion is both feasible to create and practical , increasing the applicability of both the parser and the corpus to information extraction .

let everything turn well in your wife : generation of adult humor using lexical constraints
we propose a method for automated generation of adult humor by lexical replacement and present empirical evaluation results of the obtained humor . we propose three types of lexical constraints as building blocks of humorous word substitution : constraints concerning the similarity of sounds or spellings of the original word and the substitute , a constraint requiring the substitute to be a taboo word , and constraints concerning the position and context of the replacement . empirical evidence from extensive user studies indicates that these constraints can increase the effectiveness of humor generation significantly .

combining em training and the mdl principle for an automatic verb classification incorporating selectional preferences
this paper presents an innovative , complex approach to semantic verb classification that relies on selectional preferences as verb properties . the probabilistic verb class model underlying the semantic classes is trained by a combination of the em algorithm and the mdl principle , providing soft clusters with two dimensions ( verb senses and subcategorisation frames with selectional preferences ) as a result . a language-model-based evaluation shows that after 10 training iterations the verb class model results are above the baseline results .

ucolorado som : extraction of drug-drug interactions from biomedical text using knowledge-rich and knowledge-poor features
in this paper , we present our approach to semeval-2013 task 9.2. it is a feature rich classification using libsvm for drug-drug interactions detection in the biomedical domain . the features are extracted considering morphosyntactic , lexical and semantic concepts . tools like opendmap and tees are used to extract semantic concepts from the corpus . the best f-score that we got for drugdrug interaction ( ddi ) detection is 50 % and 61 % and the best f-score for ddi detection and classification is 34 % and 48 % for test and development data respectively .

a rose is a roos is a ruusu : querying translations for web image search
we query web image search engines with words ( e.g. , spring ) but need images that correspond to particular senses of the word ( e.g. , flexible coil ) . querying with polysemous words often yields unsatisfactory results from engines such as google images . we build an image search engine , idiom , which improves the quality of returned images by focusing search on the desired sense . our algorithm , instead of searching for the original query , searches for multiple , automatically chosen translations of the sense in several languages . experimental results show that idiom outperforms google images and other competing algorithms returning 22 % more relevant images .

recognizing relation expression between named entities based on inherent and context-dependent features of relational words
this paper proposes a supervised learning method to recognize expressions that show a relation between two named entities , e.g. , person , location , or organization . the method uses two novel features , 1 ) whether the candidate words inherently express relations and 2 ) how the candidate words are influenced by the past relations of two entities . these features together with conventional syntactic and contextual features are organized as a tree structure and are fed into a boosting-based classification algorithm . experimental results show that the proposed method outperforms conventional methods .

hsh : estimating semantic similarity of words and short phrases with frequency normalized distance measures
this paper describes the approach of the hochschule hannover to the semeval 2013 task evaluating phrasal semantics . in order to compare a single word with a two word phrase we compute various distributional similarities , among which a new similarity measure , based on jensen-shannon divergence with a correction for frequency effects . the classification is done by a support vector machine that uses all similarities as features . the approach turned out to be the most successful one in the task .

turbo parsers : dependency parsing by approximate variational inference instituto superior tecnico instituto superior tecnico
we present a unified view of two state-of-theart non-projective dependency parsers , both approximate : the loopy belief propagation parser of smith and eisner ( 2008 ) and the relaxed linear program of martins et al ( 2009 ) . by representing the model assumptions with a factor graph , we shed light on the optimization problems tackled in each method . we also propose a new aggressive online algorithm to learn the model parameters , which makes use of the underlying variational representation . the algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions , including crfs and structured svms . experiments show state-of-the-art performance for 14 languages .

different sense granularities for different applications martha palmer , olga babko-malaya , hoa trang dang
this paper describes an hierarchical approach to wordnet sense distinctions that provides different types of automatic word sense disambiguation ( wsd ) systems , which perform at varying levels of accuracy . for tasks where fine-grained sense distinctions may not be essential , an accurate coarse-grained wsd system may be sufficient . the paper discusses the criteria behind the three different levels of sense granularity , as well as the machine learning approach used by the wsd system .

going beyond aer : an extensive analysis of word alignments and their impact on mt
this paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding mt system output . we introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation . we show that precision-oriented alignments yield better mt output ( translating more words and using longer phrases ) than recalloriented alignments .

alpage : transition-based semantic graph parsing with syntactic univ paris diderot , sorbonne paris cit
this paper describes the systems deployed by the alpage team to participate to the semeval-2014 task on broad-coverage semantic dependency parsing . we developed two transition-based dependency parsers with extended sets of actions to handle non-planar acyclic graphs . for the open track , we worked over two orthogonal axes lexical and syntactic in order to provide our models with lexical and syntactic features such as word clusters , lemmas and tree fragments of different types .

an empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework
this paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of chinese texts using bakeoff-3 data sets with a unified framework . assuming no prior knowledge about chinese , this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores . experiments show that description length gain outperforms other measures because of its strength for identifying short words . further performance improvement is also reported , achieved by proper candidate pruning and by assemble segmentation to integrate the strengths of individual measures .

random walk factoid annotation for collective discourse ben king rahul jha the new yorker magazine
in this paper , we study the problem of automatically annotating the factoids present in collective discourse . factoids are information units that are shared between instances of collective discourse and may have many different ways of being realized in words . our approach divides this problem into two steps , using a graph-based approach for each step : ( 1 ) factoid discovery , finding groups of words that correspond to the same factoid , and ( 2 ) factoid assignment , using these groups of words to mark collective discourse units that contain the respective factoids . we study this on two novel data sets : the new yorker caption contest data set , and the crossword clues data set .

dependency treelet translation : syntactically informed phrasal smt chris quirk , arul menezes colin cherry
we describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation . this method requires a source-language dependency parser , target language word segmentation and an unsupervised word alignment component . we align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree-based ordering model . we describe an efficient decoder and show that using these treebased models in combination with conventional smt models provides a promising approach that incorporates the power of phrasal smt with the linguistic generality available in a parser .

on using ensemble methods for chinese named entity recognition chia-wei wu shyh-yi jan richard tzong-han
in sequence labeling tasks , applying different machine learning models and feature sets usually leads to different results . in this paper , we exploit two ensemble methods in order to integrate multiple results generated under different conditions . one method is based on majority vote , while the other is a memory-based approach that integrates maximum entropy and conditional random field classifiers . our results indicate that the memory-based method can outperform the individual classifiers , but the majority vote method can not .

hello emily , how are you today personalised dialogue in a toy to engage children
in line with the growing interest in conversational agents as companions , we are developing a toy companion for children that is capable of engaging interactions and of developing a long-term relationship with them , and is extensible so as to evolve with them . in this paper , we investigate the importance of personalising interaction both for engagement and for long-term relationship development . in particular , we propose a framework for representing , gathering and using personal knowledge about the child during dialogue interaction .

a structural support vector method for extracting contexts and answers of questions from online forums
this paper addresses the issue of extracting contexts and answers of questions from post discussion of online forums . we propose a novel and unified model by customizing the structural support vector machine method . our customization has several attractive properties : ( 1 ) it gives a comprehensive graphical representation of thread discussion . ( 2 ) it designs special inference algorithms instead of generalpurpose ones . ( 3 ) it can be readily extended to different task preferences by varying loss functions . experimental results on a real data set show that our methods are both promising and flexible .

the ngram statistics package ( text : :nsp ) - a flexible tool for identifying
the ngram statistics package ( text : :nsp ) is freely available open-source software that identifies ngrams , collocations and word associations in text . it is implemented in perl and takes advantage of regular expressions to provide very flexible tokenization and to allow for the identification of non-adjacent ngrams . it includes a wide range of measures of association that can be used to identify collocations .

reranking with linguistic and semantic features for arabic optical character recognition pradeep dasigi mona diab
optical character recognition ( ocr ) systems for arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize fluency . in this paper we incorporate linguistically and semantically motivated features to an existing ocr system . to do so we follow an n-best list reranking approach that exploits recent advances in learning to rank techniques . we achieve 10.1 % and 11.4 % reduction in recognition word error rate ( wer ) relative to a standard baseline system on typewritten and handwritten arabic respectively .

ner systems that suit users preferences : adjusting the recall-precision trade-off for entity extraction
we describe a method based on tweaking an existing learned sequential classifier to change the recall-precision tradeoff , guided by a user-provided performance criterion . this method is evaluated on the task of recognizing personal names in email and newswire text , and proves to be both simple and effective .

financial keyword expansion via continuous word vector representations program in digital content and technology
this paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon . in order to capture more keywords , we also incorporate syntactic information into the continuous bag-ofwords ( cbow ) model . experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .

incorporating syntactic dependency information towards improved coding of lengthy medical concepts in clinical reports
medical concepts in clinical reports can be found with a high degree of variability of expression . normalizing medical concepts to standardized vocabularies is a common way of accounting for this variability . one of the challenges in medical concept normalization is the difficulty in comparing two concepts which are orthographically different in representation but are identical in meaning . in this work we describe a method to compare medical phrases by utilizing the information found in syntactic dependencies . we collected a large corpus of radiology reports from our university medical center . a shallow semantic parser was used to identify anatomical phrases . we performed a series of transformations to convert the anatomical phrase into a normalized syntactic dependency representation . the new representation provides an easy intuitive way of comparing the phrases for the purpose of concept normalization .

using the web as an implicit training set : application to structural ambiguity resolution
recent work has shown that very large corpora can act as training data for nlp algorithms even without explicit labels . in this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks . using unsupervised algorithms , we achieve 84 % precision on pp-attachment and 80 % on noun compound coordination .

tweet conversation annotation tool with a focus on an arabic dialect , moroccan darija
this paper presents the datool , a graphical tool for annotating conversations consisting of short messages ( i.e. , tweets ) , and the results we obtain in using it to annotate tweets for darija , an historically unwritten arabic dialect spoken by millions but not taught in schools and lacking standardization and linguistic resources . with the datool , a native-darija speaker annotated hundreds of mixedlanguage and mixed-script conversations at approximately 250 tweets per hour . the resulting corpus was used in developing and evaluating arabic dialect classifiers described briefly herein . the datool supports downstream discourse analysis of tweeted conversations by mapping extracted relations such as , who tweets to whom in which language , into graph markup formats for analysis in network visualization tools .

learning stochastic ot grammars : a bayesian approach using data augmentation and gibbs sampling
stochastic optimality theory ( boersma , 1997 ) is a widely-used model in linguistics that did not have a theoretically sound learning method previously . in this paper , a markov chain monte-carlo method is proposed for learning stochastic ot grammars . following a bayesian framework , the goal is finding the posterior distribution of the grammar given the relative frequencies of input-output pairs . the data augmentation algorithm allows one to simulate a joint posterior distribution by iterating two conditional sampling steps . this gibbs sampler constructs a markov chain that converges to the joint distribution , and the target posterior can be derived as its marginal distribution .

systematic comparison of professional and crowdsourced reference translations for machine translation raytheon bbn technologies
we present a systematic study of the effect of crowdsourced translations on machine translation performance . we compare machine translation systems trained on the same data but with translations obtained using amazons mechanical turk vs. professional translations , and show that the same performance is obtained from mechanical turk translations at 1/5th the cost . we also show that adding a mechanical turk reference translation of the development set improves parameter tuning and output evaluation .

evaluating contribution of deep syntactic information to shallow semantic analysis sumire uematsu junichi tsujii
this paper presents shallow semantic parsing based only on hpsg parses . an hpsg-framenet map was constructed from a semantically annotated corpus , and semantic parsing was performed by mapping hpsg dependencies to framenet relations . the semantic parsing was evaluated in a senseval-3 task ; the results suggested that there is a high contribution of syntactic information to semantic analysis .

tcdscss : dimensionality reduction to evaluate texts of varying lengths - an ir approach
this paper provides system description of the cross-level semantic similarity task for the semeval-2014 workshop . crosslevel semantic similarity measures the degree of relatedness between texts of varying lengths such as paragraph to sentence and sentence to phrase . latent semantic analysis was used to evaluate the cross-level semantic relatedness between the texts to achieve above baseline scores , tested on the training and test datasets . we also tried using a bag-of-vectors approach to evaluate the semantic relatedness . this bag-of-vectors approach however did not produced encouraging results .

building and evaluating a distributional memory for croatian jan snajder sebastian pado zeljko agic
we report on the first structured distributional semantic model for croatian , dm.hr . it is constructed after the model of the english distributional memory ( baroni and lenci , 2010 ) , from a dependencyparsed croatian web corpus , and covers about 2m lemmas . we give details on the linguistic processing and the design principles . an evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns . the resource is freely available .

which side are you on identifying perspectives at the document and intelligent systems program
in this paper we investigate a new problem of identifying the perspective from which a document is written . by perspective we mean a point of view , for example , from the perspective of democrats or republicans . can computers learn to identify the perspective of a document not every sentence is written strongly from a perspective . can computers learn to identify which sentences strongly convey a particular perspective we develop statistical models to capture how perspectives are expressed at the document and sentence levels , and evaluate the proposed models on articles about the israeli-palestinian conflict . the results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy .

automated planning for situated natural language generation cluster of excellence multimodal computing and interaction
we present a natural language generation approach which models , exploits , and manipulates the non-linguistic context in situated communication , using techniques from ai planning . we show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions , and how to generate referring expressions with context-dependent adjectives . we implement and evaluate our approach in the framework of the challenge on generating instructions in virtual environments , finding that it performs well even under the constraints of realtime generation .

robust approach to abbreviating terms : a discriminative latent variable model with global information
the present paper describes a robust approach for abbreviating terms . first , in order to incorporate non-local information into abbreviation generation tasks , we present both implicit and explicit solutions : the latent variable model , or alternatively , the label encoding approach with global information . although the two approaches compete with one another , we demonstrate that these approaches are also complementary . by combining these two approaches , experiments revealed that the proposed abbreviation generator achieved the best results for both the chinese and english languages . moreover , we directly apply our generator to perform a very different task from tradition , the abbreviation recognition . experiments revealed that the proposed model worked robustly , and outperformed five out of six state-of-the-art abbreviation recognizers .

experiments with dbpedia , wordnet and sentiwordnet as resources for sentiment analysis in micro-blogging
sentiment analysis in twitter has become an important task due to the huge user-generated content published over such media . such analysis could be useful for many domains such as marketing , finance , politics , and social . we propose to use many features in order to improve a trained classifier of twitter messages ; these features extend the feature vector of uni-gram model by the concepts extracted from dbpedia , the verb groups and the similar adjectives extracted from wordnet , the sentifeatures extracted using sentiwordnet and some useful domain specific features . we also built a dictionary for emotion icons , abbreviation and slang words in tweets which is useful before extending the tweets with different features . adding these features has improved the f-measure accuracy 2 % with svm and 4 % with naivebayes .

aspect-oriented opinion mining from user reviews in croatian goran glava damir korencic jan najder
aspect-oriented opinion mining aims to identify product aspects ( features of products ) about which opinion has been expressed in the text . we present an approach for aspect-oriented opinion mining from user reviews in croatian . we propose methods for acquiring a domain-specific opinion lexicon , linking opinion clues to product aspects , and predicting polarity and rating of reviews . we show that a supervised approach to linking opinion clues to aspects is feasible , and that the extracted clues and aspects improve polarity and rating predictions .

depends on what the french say spoken corpus annotation with and beyond syntactic functions
we present a syntactic annotation scheme for spoken french that is currently used in the rhapsodie project . this annotation is dependency-based and includes coordination and disfluency as analogously encoded types of paradigmatic phenomena . furthermore , we attempt a thorough definition of the discourse units required by the systematic annotation of other phenomena beyond usual sentence boundaries , which are typical for spoken language . this includes so called macrosyntactic phenomena such as dislocation , parataxis , insertions , grafts , and epexegesis .

enhanced free text access to anatomically-indexed data mrc human genetics unit western general hospital
we describe our use of an existing resource , the mouse anatomical nomenclature , to improve a symbolic interface to anatomically-indexed gene expression data . the goal is to reduce user effort in specifying anatomical structures of interest and increase precision and recall .

point-of-view mining and cognitive presence in moocs : a ( computational ) linguistics perspective
this paper explores the cognitive presence of the learners in moocs through using a ( computational ) linguistic analysis of the learners point-of-view as an indicator for cognitive presence . the linguistic analysis of the written language as a medium of interaction by the students in the context of moocs shows hallmarks of cognitive disengagement and low cognitive presence by the learners .

n-gram-based statistical machine translation versus syntax augmented machine translation : comparison and system combination
in this paper we compare and contrast two approaches to machine translation ( mt ) : the cmu-uka syntax augmented machine translation system ( samt ) and upc-talp n-gram-based statistical machine translation ( smt ) . samt is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree . in n-gram-based smt , the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximumentropy framework . we provide a stepby-step comparison of the systems and report results in terms of automatic evaluation metrics and required computational resources for a smaller arabic-to-english translation task ( 1.5m tokens in the training corpus ) . human error analysis clarifies advantages and disadvantages of the systems under consideration . finally , we combine the output of both systems to yield significant improvements in translation quality .

automatic adaptation of annotation standards for dependency parsing using projected treebank as source corpus
we describe for dependency parsing an annotation adaptation strategy , which can automatically transfer the knowledge from a source corpus with a different annotation standard to the desired target parser , with the supervision by a target corpus annotated in the desired standard . furthermore , instead of a hand-annotated one , a projected treebank derived from a bilingual corpus is used as the source corpus . this benefits the resource-scarce languages which havent different handannotated treebanks . experiments show that the target parser gains significant improvement over the baseline parser trained on the target corpus only , when the target corpus is smaller .

data-driven computational linguistics at famaf-unc , argentina laura alonso i alemany and gabriel infante-lopez
this paper provides a survey of some ongoing research projects in computational linguistics within the group of natural language processing at the university of cordoba , argentina . we outline our future plans and spotlight some opportunities for collaboration .

effective adaptation of a hidden markov model-based named entity recognizer for biomedical domain
in this paper , we explore how to adapt a general hidden markov model-based named entity recognizer effectively to biomedical domain . we integrate various features , including simple deterministic features , morphological features , pos features and semantic trigger features , to capture various evidences especially for biomedical named entity and evaluate their contributions . we also present a simple algorithm to solve the abbreviation problem and a rule-based method to deal with the cascaded phenomena in biomedical domain . our experiments on genia v3.0 and genia v1.1 achieve the 66.1 and 62.5 f-measure respectively , which outperform the previous best published results by 8.1 f-measure when using the same training and testing data .

minimalist parsing of subjects displaced from embedded clauses in free word order languages
in sayeed and szpakowicz ( 2004 ) , we proposed a parser inspired by some aspects of the minimalist program . this incremental parser was designed specifically to handle discontinuous constituency phenomena for nps in latin . we take a look at the application of this parser to a specific kind of apparent island violation in latin involving the extraction of constituents , including subjects , from tensed embedded clauses . we make use of ideas about the left periphery from rizzi ( 1997 ) to modify our parser in order to handle apparently violated subject islands and similar phenomena .

supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification
this work investigates design choices in modeling a discourse scheme for improving opinion polarity classification . for this , two diverse global inference paradigms are used : a supervised collective classification framework and an unsupervised optimization framework . both approaches perform substantially better than baseline approaches , establishing the efficacy of the methods and the underlying discourse scheme . we also present quantitative and qualitative analyses showing how the improvements are achieved .

encoding of compounds in swedish framenet karin friberg heppin miriam r l petruck
constructing a lexical resource for swedish , where compounding is highly productive , requires a well-structured policy of encoding . this paper presents the treatment and encoding of a certain class of compounds in swedish framenet , and proposes a new approach for the automatic analysis of swedish compounds , i.e . one that leverages existing framenet ( ruppenhofer et al. , 2010 ) and swedish framenet ( borin et al . 2010 ) , as well as proven techniques for automatic semantic role labeling ( johansson et al. , 2012 ) .

are some speech recognition errors easier to detect than others and electrical engineering
this study investigates whether some speech recognition ( sr ) errors are easier to detect and what patterns can be identified from those errors . specifically , sr errors were examined from both nonlinguistic and linguistic perspectives . the analyses of non-linguistic properties revealed that high error ratios and consecutive errors lowered the ease of error detection . the analyses of linguistic properties showed that ease of error detection was associated with changing parts-of-speech of reference words in sr errors . additionally , syntactic relations themselves and the change of syntactic relations had impact on the ease of error detection .

pamini : a framework for assembling mixed-initiative human-robot interaction from generic interaction patterns
dialog modeling in robotics suffers from lack of generalizability , due to the fact that the dialog is heavily influenced by the tasks the robot is able to perform . we introduce interleaving interaction patterns together with a general protocol for task communication which enables us to systematically specify the relationship between dialog structure and task structure . we argue that this approach meets the requirements of advanced dialog modeling on robots and at the same time exhibits a better scalability than existing concepts .

weblog classification for fast splog filtering : a url language model segmentation approach
this paper shows that in the context of statistical weblog classification for splog filtering based on n-grams of tokens in the url , further segmenting the urls beyond the standard punctuation is helpful . many splog urls contain phrases in which the words are glued together in order to avoid splog filtering techniques based on punctuation segmentation and unigrams . a technique which segments long tokens into the words forming the phrase is proposed and evaluated . the resulting tokens are used as features for a weblog classifier whose accuracy is similar to that of humans ( 78 % vs. 76 % ) and reaches 93.3 % of precision in identifying splogs with recall of 50.9 % .

shared task system description : measuring the compositionality of bigrams using statistical methodologies
the measurement of relative compositionality of bigrams is crucial to identify multi-word expressions ( mwes ) in natural language processing ( nlp ) tasks . the article presents the experiments carried out as part of the participation in the shared task distributional semantics and compositionality ( disco ) organized as part of the disco workshop in aclhlt 2011. the experiments deal with various collocation based statistical approaches to compute the relative compositionality of three types of bigram phrases ( adjective-noun , verbsubject and verb-object combinations ) . the experimental results in terms of both fine-grained and coarse-grained compositionality scores have been evaluated with the human annotated gold standard data . reasonable results have been obtained in terms of average point difference and coarse precision .

learning phrase boundaries for hierarchical phrase-based translation zhongjun he yao meng hao yu
hierarchical phrase-based models provide a powerful mechanism to capture non-local phrase reorderings for statistical machine translation ( smt ) . however , many phrase reorderings are arbitrary because the models are weak on determining phrase boundaries for patternmatching . this paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information . we use phrase boundaries , which indicate the beginning/ending of phrase reordering , as soft constraints for decoding . experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale chineseto-english translation .

hierarchical spectral partitioning of bipartite graphs to cluster dialects and identify distinguishing features
in this study we apply hierarchical spectral partitioning of bipartite graphs to a dutch dialect dataset to cluster dialect varieties and determine the concomitant sound correspondences . an important advantage of this clustering method over other dialectometric methods is that the linguistic basis is simultaneously determined , bridging the gap between traditional and quantitative dialectology . besides showing that the results of the hierarchical clustering improve over the flat spectral clustering method used in an earlier study ( wieling and nerbonne , 2009 ) , the values of the second singular vector used to generate the two-way clustering can be used to identify the most important sound correspondences for each cluster . this is an important advantage of the hierarchical method as it obviates the need for external methods to determine the most important sound correspondences for a geographical cluster .

solving the whos mark johnson puzzle : information extraction based cross document coreference
cross document coreference ( cdc ) is the problem of resolving the underlying identity of entities across multiple documents and is a major step for document understanding . we develop a framework to efficiently determine the identity of a person based on extracted information , which includes unary properties such as gender and title , as well as binary relationships with other named entities such as co-occurrence and geo-locations . at the heart of our approach is a suite of similarity functions ( specialists ) for matching relationships and a relational density-based clustering algorithm that delineates name clusters based on pairwise similarity . we demonstrate the effectiveness of our methods on the weps benchmark datasets and point out future research directions .

automatic identification of bengali noun-noun compounds using random forest vivekananda gayen kamal sarkar
this paper presents a supervised machine learning approach that uses a machine learning algorithm called random forest for recognition of bengali noun-noun compounds as multiword expression ( mwe ) from bengali corpus . our proposed approach to mwe recognition has two steps : ( 1 ) extraction of candidate multi-word expressions using chunk information and various heuristic rules and ( 2 ) training the machine learning algorithm to recognize a candidate multi-word expression as multi-word expression or not . a variety of association measures , syntactic and linguistic clues are used as features for identifying mwes . the proposed system is tested on a bengali corpus for identifying noun-noun compound mwes from the corpus .

power of confidence : how poll scores impact topic dynamics in political debates
in this paper , we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants . we perform this study on the us presidential debates and show that a candidates power , modeled after their poll scores , affects how often he/she attempts to shift topics and whether he/she succeeds . we ensure the validity of topic shifts by confirming , through a simple but effective method , that the turns that shift topics provide substantive topical content to the interaction . a paper describing this work is published in the acl 2014 joint workshop on social dynamics and personal attributes in social media . 49

knowledge-rich word sense disambiguation rivaling supervised systems simone paolo ponzetto dipartimento di informatica
one of the main obstacles to highperformance word sense disambiguation ( wsd ) is the knowledge acquisition bottleneck . in this paper , we present a methodology to automatically extend wordnet with large amounts of semantic relations from an encyclopedic resource , namely wikipedia . we show that , when provided with a vast amount of high-quality semantic relations , simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervisedwsd systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets .

annotation of regular polysemy and underspecification hector martnez alonso , bolette sandford pedersen
we present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in english , danish and spanish . this article describes the annotation process , the results in terms of inter-encoder agreement , and the sense distributions obtained with two methods : majority voting with a theory-compliant backoff strategy , and mace , an unsupervised system to choose the most likely sense from all the annotations .

what lies beneath : semantic and syntactic analysis of manually reconstructed spontaneous speech
spontaneously produced speech text often includes disfluencies which make it difficult to analyze underlying structure . successful reconstruction of this text would transform these errorful utterances into fluent strings and offer an alternate mechanism for analysis . our investigation of naturally-occurring spontaneous speaker errors aligned to corrected text with manual semanticosyntactic analysis yields new insight into the syntactic and structural semantic differences between spoken and reconstructed language .

an automated method to build a corpus of rhetorically-classified sentences in biomedical texts
the rhetorical classification of sentences in biomedical texts is an important task in the recognition of the components of a scientific argument . generating supervised machine learned models to do this recognition requires corpora annotated for

incorporating extra-linguistic information into reference resolution in ryu iida shumpei kobayashi takenobu tokunaga
this paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information . recently , investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers ( di eugenio et al , 2000 ; byron , 2005 ; van deemter , 2007 ; spanger et al , 2009 ) . in order to create an accurate reference resolution model , we need to handle extra-linguistic information as well as textual information examined by existing approaches ( soon et al , 2001 ; ng and cardie , 2002 , etc . ) . in this paper , we incorporate extra-linguistic information into an existing corpus-based reference resolution model , and investigate its effects on reference resolution problems within a corpus of japanese dialogues . the results demonstrate that our proposed model achieves an accuracy of 79.0 % for this task .

automatic acquisition of grammatical types for nouns nria bel sergio espeja montserrat marimon
the work1 we present here is concerned with the acquisition of deep grammatical information for nouns in spanish . the aim is to build a learner that can handle noise , but , more interestingly , that is able to overcome the problem of sparse data , especially important in the case of nouns . we have based our work on two main points . firstly , we have used distributional evidences as features . secondly , we made the learner deal with all occurrences of a word as a single complex unit . the obtained results show that grammatical features of nouns is a level of generalization that can be successfully approached with a decision tree learner .

a framework based on graphical models with logic for chinese named entity recognition
chinese named entity recognition ( ner ) has recently been viewed as a classification or sequence labeling problem , and many approaches have been proposed . however , they tend to address this problem without considering linguistic information in chinese nes . we propose a new framework based on probabilistic graphical models with firstorder logic for chinese ner . first , we use conditional random fields ( crfs ) , a standard and theoretically well-founded machine learning method based on undirected graphical models as a base system . second , we introduce various types of domain knowledge into markov logic networks ( mlns ) , an effective combination of first-order logic and probabilistic graphical models for validation and error correction of entities . experimental results show that our framework of probabilistic graphical models with first-order logic significantly outperforms the state-of-the-art models for solving this task .

the utility of a graphical representation of discourse structure in spoken dialogue systems
in this paper we explore the utility of the navigation map ( nm ) , a graphical representation of the discourse structure . we run a user study to investigate if users perceive the nm as helpful in a tutoring spoken dialogue system . from the users perspective , our results show that the nm presence allows them to better identify and follow the tutoring plan and to better integrate the instruction . it was also easier for users to concentrate and to learn from the system if the nm was present . our preliminary analysis on objective metrics further strengthens these findings .

contextual bearing on linguistic variation in social media marina del rey , ca
microtexts , like sms messages , twitter posts , and facebook status updates , are a popular medium for real-time communication . in this paper , we investigate the writing conventions that different groups of users use to express themselves in microtexts . our empirical study investigates properties of lexical transformations as observed within twitter microtexts . the study reveals that different populations of users exhibit different amounts of shortened english terms and different shortening styles . the results reveal valuable insights into how human language technologies can be effectively applied to microtexts .

names and similarities on the web : fact extraction in the fast lane
in a new approach to large-scale extraction of facts from unstructured text , distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns , and the validation and ranking of candidate facts . the evaluation measures the quality and coverage of facts extracted from one hundred million web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .

bleu in characters : towards automatic mt evaluation in languages without word delimiters
automatic evaluation metrics for machine translation ( mt ) systems , such as bleu or nist , are now well established . yet , they are scarcely used for the assessment of language pairs like english-chinese or english-japanese , because of the word segmentation problem . this study establishes the equivalence between the standard use of bleu in word n-grams and its application at the character level . the use of bleu at the character level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical mt systems which usually segment their outputs .

japanese dependency analysis using the ancestor-descendant relation akihiro tamura hiroya takamura manabu okumura
we propose a novel method for japanese dependency analysis , which is usually reduced to the construction of a dependency tree . in deterministic approaches to this task , dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed . conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent-child relation or not . however , tree structures include relations between two nodes other than the parent-child relation . therefore , we use ancestor-descendant relations in addition to parent-child relations , so that the added redundancy helps errors be corrected . experimental results show that the proposed method achieves higher accuracy .

ontology driven content extraction using interlingual annotation of texts in the omnia project
omnia is an on-going project that aims to retrieve images accompanied with multilingual texts . in this paper , we propose a generic method ( language and domain independent ) to extract conceptual information from such texts and spontaneous user requests . first , texts are labelled with interlingual annotation , then a generic extractor taking a domain ontology as a parameter extract relevant conceptual information . implementation is also presented with a first experiment and preliminary results .

out-of-domain spoken dialogs in the car : a woz study speech dialogue systems
mobile internet access via smartphones puts demands on in-car infotainment systems , as more and more drivers like to access the internet while driving . spoken dialog systems ( sds ) distract drivers less than visual/haptic-based dialog systems . however , in conversational sdss drivers might speak utterances which are not in the domain of the sds and thus can not be understood . in a wizard of oz study , we evaluate the effects of out-of-domain utterances on cognitive load , driving performance , and usability . the results show that an sds which reacts as expected by the driver , is a good approach to control incar infotainment systems , whereas unexpected sds reactions might cause severe accidents . we evaluate how a dialog initiative switch , which guides the user and enables him to reach his task goal , performs .

rare dialogue acts common in oncology consultations mary mcgee wood , richard craggs
dialogue acts ( das ) which explicitly ensure mutual understanding are frequent in dialogues between cancer patients and health professionals . we present examples , and argue that this arises from the health- critical nature of these dialogues .

meaningful clustering of senses helps boost word sense disambiguation performance dipartimento di informatica
fine-grained sense distinctions are one of the major obstacles to successful word sense disambiguation . in this paper , we present a method for reducing the granularity of the wordnet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the oxford dictionary of english . we assess the quality of the mapping and the induced clustering , and evaluate the performance of coarse wsd systems in the senseval-3 english all-words task .

enhanced search with wildcards and morphological inflections in the google books ngram viewer
we present a new version of the google books ngram viewer , which plots the frequency of words and phrases over the last five centuries ; its data encompasses 6 % of the worlds published books . the new viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization . these additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .

a measure of term representativeness based on the number of co-occurring salient words
we propose a novel measure of the representativeness ( i.e. , indicativeness or topic specificity ) of a term in a given corpus . the measure embodies the idea that the distribution of words co-occurring with a representative term should be biased according to the word distribution in the whole corpus . the bias of the word distribution in the co-occurring words is defined as the number of distinct words whose occurrences are saliently biased in the co-occurring words . the saliency of a word is defined by a threshold probability that can be automatically defined using the whole corpus . comparative evaluation clarified that the measure is clearly superior to conventional measures in finding topic-specific words in the newspaper archives of different sizes .

cora : a web-based annotation tool for historical and other non-standard language data
we present cora , a web-based annotation tool for manual annotation of historical and other non-standard language data . it allows for editing the primary data and modifying token boundaries during the annotation process . further , it supports immediate retraining of taggers on newly annotated data .

answering list questions using web as a corpus patrcia nunes goncalves , ant
this paper supports the demo of lxlistquestion , a web question answering system that exploit the redundancy of information available in the web to answer list questions in the form of word cloud .

toward multimedia : a string pattern-based passage ranking model for video question answering
in this paper , we present a new string pattern matching-based passage ranking algorithm for extending traditional textbased qa toward videoqa . users interact with our videoqa system through natural language questions , while our system returns passage fragments with corresponding video clips as answers . we collect 75.6 hours videos and 253 chinese questions for evaluation . the experimental results showed that our method outperformed six top-performed ranking models . it is 10.16 % better than the second best method ( language model ) in relatively mrr score and 6.12 % in precision rate . besides , we also show that the use of a trained chinese word segmentation tool did decrease the overall videoqa performance where most ranking algorithms dropped at least 10 % in relatively mrr , precision , and answer pattern recall rates .

tucsage : grammar rule induction for spoken dialogue systems via probabilistic candidate selection
we describe the grammar induction system for spoken dialogue systems ( sds ) submitted to semeval14 : task 2. a statistical model is trained with a rich feature set and used for the selection of candidate rule fragments . posterior probabilities produced by the fragment selection model are fused with estimates of phraselevel similarity based on lexical and contextual information . domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages .

semantic consistency : a local subspace based method for distant supervised relation extraction
one fundamental problem of distant supervision is the noisy training corpus problem . in this paper , we propose a new distant supervision method , called semantic consistency , which can identify reliable instances from noisy instances by inspecting whether an instance is located in a semantically consistent region . specifically , we propose a semantic consistency model , which first models the local subspace around an instance as a sparse linear combination of training instances , then estimate the semantic consistency by exploiting the characteristics of the local subspace . experimental results verified the effectiveness of our method .

using morphosemantic information in construction of a pilot lexical semantic resource for turkish
morphological units carry vast amount of semantic information for languages with rich inflectional and derivational morphology . in this paper we show how morphosemantic information available for morphologically rich languages can be used to reduce manual effort in creating semantic resources like propbank and verbnet ; to increase performance of word sense disambiguation , semantic role labeling and related tasks . we test the consistency of these features in a pilot study for turkish and show that ; 1 ) case markers are related with semantic roles and 2 ) morphemes that change the valency of the verb follow a predictable pattern .

two stage constraint based hybrid approach to free word order language dependency parsing
the paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing . we define the two stages and show how different grammatical construct are parsed at appropriate stages . this division leads to selective identification and resolution of specific dependency relations at the two stages . furthermore , we show how the use of hard constraints and soft constraints helps us build an efficient and robust hybrid parser . finally , we evaluate the implemented parser on hindi and compare the results with that of two data driven dependency parsers .

modeling morphosyntactic agreement in constituency-based parsing of modern hebrew reut tsarfatyand khalil simaan
we show that nave modeling of morphosyntactic agreement in a constituency-based ( cb ) statistical parsing model is worse than none , whereas a linguistically adequate way of modeling inflectional morphology in cb parsing leads to improved performance . in particular , we show that an extension of the relational-realizational ( rr ) model that incorporates agreement features is superior to cb models that treat morphosyntax as statesplits ( sp ) , and that the rr model benefits more from inflectional features . we focus on parsing hebrew and report the best result to date , f184.13 for parsing off of gold-tagged text , 5 % error reduction from previous results .

grammatical error correction using hybrid systems and type filtering helen yannakoudakis ekaterina kochmar
this paper describes our submission to the conll 2014 shared task on grammatical error correction using a hybrid approach , which includes both a rule-based and an smt system augmented by a large webbased language model . furthermore , we demonstrate that correction type estimation can be used to remove unnecessary corrections , improving precision without harming recall . our best hybrid system achieves state-of-the-art results , ranking first on the original test set and second on the test set with alternative annotations .

the human language project : building a universal corpus of the worlds languages
we present a grand challenge to build a corpus that will include all of the worlds languages , in a consistent structure that permits large-scale cross-linguistic processing , enabling the study of universal linguistics . the focal data types , bilingual texts and lexicons , relate each language to one of a set of reference languages . we propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language . we call on the computational linguistics community to begin work on this universal corpus , pursuing the many strands of activity described here , as their contribution to the global effort to document the worlds linguistic heritage before more languages fall silent .

exploiting subjective annotations human media interaction rieks op den akker human media interaction
many interesting phenomena in conversation can only be annotated as a subjective task , requiring interpretative judgements from annotators . this leads to data which is annotated with lower levels of agreement not only due to errors in the annotation , but also due to the differences in how annotators interpret conversations . this paper constitutes an attempt to find out how subjective annotations with a low level of agreement can profitably be used for machine learning purposes . we analyse the ( dis ) agreements between annotators for two different cases in a multimodal annotated corpus and explicitly relate the results to the way machinelearning algorithms perform on the annotated data . finally we present two new concepts , namely subjective entity classifiers resp . consensus objective classifiers , and give recommendations for using subjective data in machine-learning applications .

an implemented description of japanese : the lexeed dictionary and the hinoki treebank
in this paper we describe the current state of a new japanese lexical resource : the hinoki treebank . the treebank is built from dictionary definition sentences , and uses an hpsg based japanese grammar to encode both syntactic and semantic information . it is combined with an ontology based on the definition sentences to give a detailed sense level description of the most familiar 28,000 words of japanese .

schwa : pete using ccg dependencies with the c & c parser
this paper describes the schwa system entered by the university of sydney in semeval 2010 task 12 parser evaluation using textual entailments ( yuret et al , 2010 ) . our system achieved an overall accuracy of 70 % in the task evaluation . we used the c & c parser to build ccg dependency parses of the truth and hypothesis sentences . we then used partial match heuristics to determine whether the system should predict entailment . heuristics were used because the dependencies generated by the parser are construction specific , making full compatibility unlikely . we also manually annotated the development set with ccg analyses , establishing an upper bound for our entailment system of 87 % .

hcaminer : mining concept associations for knowledge discovery through concept chain queries
this paper presents hcaminer , a system focusing on detecting how concepts are linked across multiple documents . a traditional search involving , for example , two person names will attempt to find documents mentioning both these individuals . this research focuses on a different interpretation of such a query : what is the best concept chain across multiple documents that connects these individuals a new robust framework is presented , based on ( i ) generating concept association graphs , a hybrid content representation , ( ii ) performing concept chain queries ( ccq ) to discover candidate chains , and ( iii ) subsequently ranking chains according to the significance of relationships suggested . these functionalities are implemented using an interactive visualization paradigm which assists users for a better understanding and interpretation of discovered relationships .

inferring the semantics of temporal prepositions in italian tommaso caselli valeria quochi
in this work we report on the results of a preliminary corpus study of italian on the semantics of temporal prepositions , which is part of a wider project on the automatic recognition of temporal relations . the corpus data collected supports our hypothesis that each temporal preposition can be associated with one prototypical temporal relation , and that deviations from the prototype can be explained as determined by the occurrence of different semantic patterns . the motivation behind this approach is to improve methods for temporal annotation of texts for content based access to information . the corpus study described in this paper led to the development of a preliminary set of heuristics for automatic annotation of temporal relations in text/discourse .

corpus creation for new genres : a crowdsourced approach to pp attachment
this paper explores the task of building an accurate prepositional phrase attachment corpus for new genres while avoiding a large investment in terms of time and money by crowdsourcing judgments . we develop and present a system to extract prepositional phrases and their potential attachments from ungrammatical and informal sentences and pose the subsequent disambiguation tasks as multiple choice questions to workers from amazons mechanical turk service . our analysis shows that this two-step approach is capable of producing reliable annotations on informal and potentially noisy blog text , and this semi-automated strategy holds promise for similar annotation projects in new genres .

comparison of classification and ranking approaches to pronominal anaphora resolution in czech
in this paper we compare two machine learning approaches to the task of pronominal anaphora resolution : a conventional classification system based on c5.0 decision trees , and a novel perceptron-based ranker . we use coreference links annotated in the prague dependency treebank 2.0 for training and evaluation purposes . the perceptron system achieves f-score 79.43 % on recognizing coreference of personal and possessive pronouns , which clearly outperforms the classifier and which is the best result reported on this data set so far .

topic model analysis of metaphor frequency for psycholinguistic stimuli vicky tzuyin lai
psycholinguistic studies of metaphor processing must control their stimuli not just for word frequency but also for the frequency with which a term is used metaphorically . thus , we consider the task of metaphor frequency estimation , which predicts how often target words will be used metaphorically . we develop metaphor classifiers which represent metaphorical domains through latent dirichlet allocation , and apply these classifiers to the target words , aggregating their decisions to estimate the metaphorical frequencies . training on only 400 sentences , our models are able to achieve 61.3 % accuracy on metaphor classification and 77.8 % accuracy on high vs. low metaphorical frequency estimation .

easy web search results clustering : when baselines can reach state-of-the-art algorithms
this work discusses the evaluation of baseline algorithms for web search results clustering . an analysis is performed over frequently used baseline algorithms and standard datasets . our work shows that competitive results can be obtained by either fine tuning or performing cascade clustering over well-known algorithms . in particular , the latter strategy can lead to a scalable and real-world solution , which evidences comparative results to recent text-based state-of-the-art algorithms .

making conversational structure explicit : identification of initiation-response pairs within online discussions
in this paper we investigate how to identify initiation-response pairs in asynchronous , multi-threaded , multi-party conversations . we formulate the task of identifying initiation-response pairs as a pairwise ranking problem . a novel variant of latent semantic analysis ( lsa ) is proposed to overcome a limitation of standard lsa models , namely that uncommon words , which are critical for signaling initiation-response links , tend to be deemphasized as it is the more frequent terms that end up closer to the latent factors selected through singular value decomposition . we present experimental results demonstrating significantly better performance of the novel variant of lsa over standard lsa .

learning lexical alignment policies for generating referring expressions in spoken dialogue systems
we address the problem that different users have different lexical knowledge about problem domains , so that automated dialogue systems need to adapt their generation choices online to the users domain knowledge as it encounters them . we approach this problem using policy learning in markov decision processes ( mdp ) . in contrast to related work we propose a new statistical user model which incorporates the lexical knowledge of different users . we evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users , and that these policies are significantly better than adaptive hand-coded policies for this problem . the learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies .

a salience driven approach to robust input interpretation in multimodal conversational systems
to improve the robustness in multimodal input interpretation , this paper presents a new salience driven approach . this approach is based on the observation that , during multimodal conversation , information from deictic gestures ( e.g. , point or circle ) on a graphical display can signal a part of the physical world ( i.e. , representation of the domain and task ) of the application which is salient during the communication . this salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding , thus improving overall input interpretation . our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation .

using parse features for preposition selection and error detection educational testing service
we evaluate the effect of adding parse features to a leading model of preposition usage . results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an esl error detection task . analysis of the parser output indicates that it is robust enough in the face of noisy non-native writing to extract useful information .

techlimed system description for the shared task on automatic arabic error correction
this article is a system description paper and reports on the participation of techlimed in the qalb-2014 shared task on evaluation of automatic arabic error correction systems organized in conjunction with the emnlp 2014 workshop on arabic natural language processing . correcting automatically texts in arabic is a challenging task due to the complexity and rich morphology of the arabic language and the lack of appropriate resources , ( e.g . publicly available corpora and tools ) . to develop our systems , we considered several approaches from rule based systems to statistical methods . our results on the development set show that the statistical system outperforms the lexicon driven approach with a precision of 71 % , a recall of 50 % and a f-measure of 59 % .

generation of referring expression using prefix tree structure sibabrata paladhi sivaji bandyopadhyay
this paper presents a prefix tree ( trie ) based model for generation of referring expression ( gre ) . the existing algorithms in gre lie in two extremities . incremental algorithm is simple and speedy but less expressive in nature whereas others are complex and exhaustive but more expressive in nature . our prefix tree based model not only incorporates all relevant features of gre ( like describing set , generating boolean and context sensitive description etc . ) but also try to attain simplicity and speed properties of incremental algorithm . thus this model provides a simple and linguistically rich approach to gre .

evaluation of several phonetic similarity algorithms on the task of cognate identification
we investigate the problem of measuring phonetic similarity , focusing on the identification of cognates , words of the same origin in different languages . we compare representatives of two principal approaches to computing phonetic similarity : manually-designed metrics , and learning algorithms . in particular , we consider a stochastic transducer , a pair hmm , several dbn models , and two constructed schemes . we test those approaches on the task of identifying cognates among indoeuropean languages , both in the supervised and unsupervised context . our results suggest that the averaged context dbn model and the pair hmm achieve the highest accuracy given a large training set of positive examples .

sentence ordering with event-enriched semantics and twolayered clustering for multi-document news summarization
we propose an event-enriched model to alleviate the semantic deficiency problem in the ir-style text processing and apply it to sentence ordering for multi-document news summarization . the ordering algorithm is built on event and entity coherence , both locally and globally . to accommodate the eventenriched model , a novel lsa-integrated two-layered clustering approach is adopted . the experimental result shows clear advantage of our model over event-agonistic models .

lingpars , a linguistically inspired , language-independent machine learner for dependency treebanks
this paper presents a constraint grammarinspired machine learner and parser , ling pars , that assigns dependencies to morpho logically annotated treebanks in a functioncentred way . the system not only bases at tachment probabilities for pos , case , mood , lemma on those features ' function probabili ties , but also uses topological features like function/pos n-grams , barrier tags and daughter-sequences . in the conll shared task , performance was below average on at tachment scores , but a relatively higher score for function tags/deprels in isolation suggests that the system 's strengths were not fully exploited in the current architecture .

atypical prosodic structure as an indicator of reading level and text difficulty
automatic assessment of reading ability builds on applying speech recognition tools to oral reading , measuring words correct per minute . this work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study . experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase , i.e . in less appropriate locations . the results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions .

improved statistical machine translation using paraphrases chris callison-burch philipp koehn miles osborne
parallel corpora are crucial for training smt systems . however , for many language pairs they are available only in very limited quantities . for these language pairs a huge portion of phrases encountered at run-time will be unknown . we show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases . our results show that augmenting a stateof-the-art smt system with paraphrases leads to significantly improved coverage and translation quality . for a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48 % to 90 % , with more than half of the newly covered items accurately translated , as opposed to none in current approaches .

polyglot : distributed word representations for multilingual nlp rami al-rfou bryan perozzi
distributed word representations ( word embeddings ) have recently contributed to competitive performance in language modeling and several nlp tasks . in this work , we train word embeddings for more than 100 languages using their corresponding wikipedias . we quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages . we find their performance to be competitive with near state-of-art methods in english , danish and swedish . moreover , we investigate the semantic features captured by these embeddings through the proximity of word groupings . we will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications .

harnessing different knowledge sources to measure semantic relatedness under a uniform model
measuring semantic relatedness between words or concepts is a crucial process to many natural language processing tasks . exiting methods exploit semantic evidence from a single knowledge source , and are predominantly evaluated only in the general domain . this paper introduces a method of harnessing different knowledge sources under a uniform model for measuring semantic relatedness between words or concepts . using wikipedia and wordnet as examples , and evaluated in both the general and biomedical domains , it successfully combines strengths from both knowledge sources and outperforms stateof-the-art on many datasets .

semantic retrieval for the accurate identification of relational concepts in massive textbases
this paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts . prior to retrieval , all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer . during the run time , user requests are converted into queries of region algebra on these annotations . structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts . this framework was applied to a text retrieval system for medline . experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .

a web-enabled and speech-enhanced parallel corpus of greek - bulgarian cultural texts
this paper reports on completed work carried out in the framework of an eu-funded project aimed at ( a ) developing a bilingual collection of cultural texts in greek and bulgarian , ( b ) creating a number of accompanying resources that will facilitate study of the primary texts across languages , and ( c ) integrating a system which aims to provide web-enabled and speech-enhanced access to digitized bilingual cultural heritage resources . this simple user interface , which incorporates advanced search mechanisms , also offers innovative accessibility for visually impaired greek and bulgarian users . the rationale behind the work ( and the relative resource ) was to promote the comparative study of the cultural heritage of the two countries .

refinements to interactive translation prediction based on search graphs and herve saint-amand
we propose a number of refinements to the canonical approach to interactive translation prediction . by more permissive matching criteria , placing emphasis on matching the last word of the user prefix , and dealing with predictions to partially typed words , we observe gains in both word prediction accuracy ( +5.4 % ) and letter prediction accuracy ( +9.3 % ) .

unitor-core typed : combining text similarity and semantic filters through sv regression
this paper presents the unitor system that participated in the *sem 2013 shared task on semantic textual similarity ( sts ) . the task is modeled as a support vector ( sv ) regression problem , where a similarity scoring function between text pairs is acquired from examples . the proposed approach has been implemented in a system that aims at providing high applicability and robustness , in order to reduce the risk of over-fitting over a specific datasets . moreover , the approach does not require any manually coded resource ( e.g . wordnet ) , but mainly exploits distributional analysis of unlabeled corpora . a good level of accuracy is achieved over the shared task : in the typed sts task the proposed system ranks in 1st and 2nd position .

hierarchical directed acyclic graph kernel : methods for structured natural language data
this paper proposes the hierarchical directed acyclic graph ( hdag ) kernel for structured natural language data . the hdag kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the hdags . we applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function . the results of the experiments demonstrate that the hdag kernel is superior to other kernel functions and baseline methods .

minimum bayes risk based answer re-ranking for question answering natural language computing
this paper presents two minimum bayes risk ( mbr ) based answer re-ranking ( mbrar ) approaches for the question answering ( qa ) task . the first approach re-ranks single qa systems outputs by using a traditional mbr model , by measuring correlations between answer candidates ; while the second approach reranks the combined outputs of multiple qa systems with heterogenous answer extraction components by using a mixture model-based mbr model . evaluations are performed on factoid questions selected from two different domains : jeopardy ! and web , and significant improvements are achieved on all data sets .

class-driven attribute extraction benjamin van durme , ting qian and lenhart schubert
we report on the large-scale acquisition of class attributes with and without the use of lists of representative instances , as well as the discovery of unary attributes , such as typically expressed in english through prenominal adjectival modification . our method employs a system based on compositional language processing , as applied to the british national corpus . experimental results suggest that documentbased , open class attribute extraction can produce results of comparable quality as those obtained using web query logs , indicating the utility of exploiting explicit occurrences of class labels in text .

collocation extraction : needs , feeds and results of an extraction system
this paper provides a specification of requirements for collocation extraction systems , taking as an example the extraction of noun + verb collocations from german texts . a hybrid approach to the extraction of habitual collocations and idioms is presented , aiming at a detailed description of collocations and their morphosyntax for natural language generation systems as well as to support learner lexicography .

lexical normalisation of short text messages : makn sens a # twitter
twitter provides access to large volumes of data in real time , but is notoriously noisy , hampering its utility for nlp . in this paper , we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words . our method uses a classifier to detect ill-formed words , and generates correction candidates based on morphophonemic similarity . both word similarity and context are then exploited to select the most probable correction candidate for the word . the proposed method doesnt require any annotations , and achieves state-of-the-art performance over an sms corpus and a novel dataset based on twitter .

query-focused multi-document summarization : combining a topic model with graph-based semi-supervised learning
graph-based learning algorithms have been shown to be an effective approach for query-focused multi-document summarization ( mds ) . in this paper , we extend the standard graph ranking algorithm by proposing a two-layer ( i.e . sentence layer and topic layer ) graph-based semi-supervised learning approach based on topic modeling techniques . experimental results on tac datasets show that by considering topic information , we can effectively improve the summary performance .

who evoked that frame some thoughts on context effects and event types
lexical substitution is an annotation task in which annotators provide one-word paraphrases ( lexical substitutes ) for individual target words in a sentence context . lexical substitution yields a fine-grained characterization of word meaning that can be done by non-expert annotators . we discuss results of a recent lexical substitution annotation effort , where we found strong contextual modulation effects : many substitutes were not synonyms , hyponyms or hypernyms of the targets , but were highly specific to the situation at hand . this data provides some food for thought for framesemantic analysis .

utd-hlt-cg : semantic architecture for metonymy resolution and classification of nominal relations
in this paper we present a semantic architecture that was employed for processing two different semeval 2007 tasks : task 4 ( classification of semantic relations between nominals ) and task 8 ( metonymy resolution ) . the architecture uses multiple forms of syntactic , lexical , and semantic information to inform a classification-based approach that generates a different model for each machine learning algorithm that implements the classification . we used decision trees , decision rules , logistic regression and lazy classifiers . a voting module selects the best performing module for each task evaluated in semeval 2007. the paper details the results obtained when using the semantic architecture .

viterbi training for pcfgs : hardness results and competitiveness of uniform initialization
we consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar , the problem approximately solved by viterbi training . we show that solving and even approximating viterbi training for pcfgs is np-hard . we motivate the use of uniformat-random initialization for viterbi em as an optimal initializer in absence of further information about the correct model parameters , providing an approximate bound on the log-likelihood .

semi-supervised structured output learning based on a hybrid generative and discriminative approach
this paper proposes a framework for semi-supervised structured output learning ( sol ) , specifically for sequence labeling , based on a hybrid generative and discriminative approach . we define the objective function of our hybrid model , which is written in log-linear form , by discriminatively combining discriminative structured predictor ( s ) with generative model ( s ) that incorporate unlabeled data . then , unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation . experiments on named entity recognition ( conll-2003 ) and syntactic chunking ( conll-2000 ) data show that our hybrid model significantly outperforms the stateof-the-art performance obtained with supervised sol methods , such as conditional random fields ( crfs ) .

refining the notions of depth and density in wordnet-based semantic similarity measures
we re-investigate the rationale for and the effectiveness of adopting the notions of depth and density in wordnet-based semantic similarity measures . we show that the intuition for including these notions in wordnet-based similarity measures does not always stand up to empirical examination . in particular , the traditional definitions of depth and density as ordinal integer values in the hierarchical structure of wordnet does not always correlate with human judgment of lexical semantic similarity , which imposes strong limitations on their contribution to an accurate similarity measure . we thus propose several novel definitions of depth and density , which yield significant improvement in degree of correlation with similarity . when used in wordnet-based semantic similarity measures , the new definitions consistently improve performance on a task of correlating with human judgment .

a study on the semantic relatedness of query and document terms in
the use of lexical semantic knowledge in information retrieval has been a field of active study for a long time . collaborative knowledge bases like wikipedia and wiktionary , which have been applied in computational methods only recently , offer new possibilities to enhance information retrieval . in order to find the most beneficial way to employ these resources , we analyze the lexical semantic relations that hold among query and document terms and compare how these relations are represented by a measure for semantic relatedness . we explore the potential of different indicators of document relevance that are based on semantic relatedness and compare the characteristics and performance of the knowledge bases wikipedia , wiktionary and wordnet .

the crotal srl system : a generic tool based on tree-structured crf
we present the crotal system , used in the conll09 shared task . it is based on xcrf , a highly configurable crf library which can take into account hierarchical relations . this system had never been used in such a context thus the performance is average , but we are confident that there is room for progression .

emotion analysis using latent affective folding and embedding speech & language technologies
though data-driven in nature , emotion analysis based on latent semantic analysis still relies on some measure of expert knowledge in order to isolate the emotional keywords or keysets necessary to the construction of affective categories . this makes it vulnerable to any discrepancy between the ensuing taxonomy of affective states and the underlying domain of discourse . this paper proposes a more general strategy which leverages two distincts semantic levels , one that encapsulates the foundations of the domain considered , and one that specifically accounts for the overall affective fabric of the language . exposing the emergent relationship between these two levels advantageously informs the emotion classification process . empirical evidence suggests that this is a promising solution for automatic emotion detection in text .

grammar approximation by representative sublanguage : a new model for language learning
we propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics , along with basic assumptions about natural language syntax . we show that the search space for grammar induction is a complete grammar lattice , which guarantees the uniqueness of the learned grammar .

two step chinese named entity recognition based on conditional random fields models
this paper mainly describes a chinese named entity recognition ( ner ) system ner @ iscas , which integrates text , partof-speech and a small-vocabularycharacter-lists feature and heristic postprocess rules for msra ner open track under the framework of conditional random fields ( crfs ) model .

arcade : an arabic corpus of auditory dictation errors sciences & disorders
we present a new corpus of word-level listening errors collected from 62 native english speakers learning arabic designed to inform models of spell checking for this learner population . while we use the corpus to assist in automated detection and correction of auditory errors in electronic dictionary lookup , the corpus can also be used as a phonological error layer , to be combined with a composition error layer in a more complex spell-checking system for non-native speakers . the corpus may be useful to instructors of arabic as a second language , and researchers who study second language phonology and listening perception .

using gene expression programming to construct sentence ranking functions for text summarization
in this paper , we consider the automatic text summarization as a challenging task of machine learning . we proposed a novel summarization system architecture which employs gene expression programming technique as its learning mechanism . the preliminary experimental results have shown that our prototype system outperforms the baseline systems .

the impact of dimensionality on natural language route directions in unconstrained dialogue
in this paper we examine the influence of dimensionality on natural language route directions in dialogue . specifically , we show that giving route instructions in a quasi-3d environment leads to experiential descriptive accounts , as manifested by a higher proportion of location descriptions , lack of chunking , use of 1st person singular personal pronouns , and more frequent use of temporal and spatial deictic terms . 2d scenarios lead to informative instructions , as manifested by a frequent use of motion expressions , chunking of route elements , and use of mainly 2nd person singular personal pronouns .

matching inconsistently spelled names in automatic speech recognizer output for information retrieval
many proper names are spelled inconsistently in speech recognizer output , posing a problem for applications where locating mentions of named entities is critical . we model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel . the models follow the framework of the ibm translation models . the model is trained using a parallel text of closed caption and automatic speech recognition output . we also test a string edit distance based method . the effectiveness of these models is evaluated on a name query retrieval task . our methods result in a 60 % improvement in f1 . we also demonstrate why the problem has not been critical in trec and tdt tasks .

mitextexplorer : linked brushing and mutual information for exploratory text data analysis
in this paper i describe a preliminary experimental system , mitextexplorer , for textual linked brushing , which allows an analyst to interactively explore statistical relationships between ( 1 ) terms , and ( 2 ) document metadata ( covariates ) . an analyst can graphically select documents embedded in a temporal , spatial , or other continuous space , and the tool reports terms with strong statistical associations for the region . the user can then drill down to specific term and term groupings , viewing further associations , and see how terms are used in context . the goal is to rapidly compare language usage across interesting document covariates . i illustrate examples of using the tool on several datasets : geo-located twitter messages , presidential state of the union addresses , the acl anthology , and the king james bible .

a basic framework to build a test collection for the vietnamese text
the aim of this paper is to present a basic framework to build a test collection for a vietnamese text categorization . the presented content includes our evaluations of some popular text categorization test collections , our researches on the requirements , the proposed model and the techniques to build the bktexts - test collection for a vietnamese text categorization . the xml specification of both text and metadata of vietnamese documents in the bktexts also is presented . our bktexts test collection is built with the xml specification and currently has more than 17100 vietnamese text documents collected from e-newspapers .

a generic approach to parallel chart parsing with an application to lingo
multi-processor systems are becoming more commonplace and affordable . based on analyses of actual parsings , we argue that to exploit the capabilities of such machines , unification-based grammar parsers should distribute work at the level of individual unification operations . we present a generic approach to parallel chart parsing that meets this requirement , and show that an implementation of this technique for lingo achieves considerable speedups .

morphological analysis for japanese noisy text based on character-level and word-level normalization
social media texts are often written in a non-standard style and include many lexical variants such as insertions , phonetic substitutions , abbreviations that mimic spoken language . the normalization of such a variety of non-standard tokens is one promising solution for handling noisy text . a normalization task is very difficult to conduct in japanese morphological analysis because there are no explicit boundaries between words . to address this issue , in this paper we propose a novel method for normalizing and morphologically analyzing japanese noisy text . we generate both character-level and word-level normalization candidates and use discriminative methods to formulate a cost function . experimental results show that the proposed method achieves acceptable levels in both accuracy and recall for word segmentation , pos tagging , and normalization . these levels exceed those achieved with the conventional rule-based system .

a tree adjoining grammar analysis of the syntax and semantics of
in this paper , we argue that in it-clefts as in it was ohno who won , the cleft pronoun ( it ) and the cleft clause ( who won ) form a discontinuous syntactic constituent , and a semantic unit as a definite description , presenting arguments from percus ( 1997 ) and hedberg ( 2000 ) . we propose a syntax of it-clefts using tree-local multicomponent tree adjoining grammar and a compositional semantics on the proposed syntax using synchronous tree adjoining grammar .

assessing readability of italian texts with a view to text simplification
in this paper , we propose a new approach to readability assessment with a specific view to the task of text simplification : the intended audience includes people with low literacy skills and/or with mild cognitive impairment . readit represents the first advanced readability assessment tool for what concerns italian , which combines traditional raw text features with lexical , morpho-syntactic and syntactic information . in readit readability assessment is carried out with respect to both documents and sentences where the latter represents an important novelty of the proposed approach creating the prerequisites for aligning the readability assessment step with the text simplification process . readit shows a high accuracy in the document classification task and promising results in the sentence classification scenario .

sense information for disambiguation : confluence of supervised and unsupervised methods
for senseval-2 , we disambiguated the lexical sample using two different sense inventories . official senseval-2 results were generated using wordnet , and separately using the new oxford dictionary of english ( node ) . since our initial submission , we have implemented additional routines and have now examined the differences in the features used for making sense selections . we report here the contribution of default sense selection , idiomatic usage , syntactic and semantic clues , subcategorization patterns , word forms , syntactic usage , context , selectional preferences , and topics or subject fields . we also compare the differences between wordnet and node . finally , we compare these features to those identified as significant in supervised learning approaches .

dynamic feature selection for dependency parsing he he hal daume iii
feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing . we propose a faster framework of dynamic feature selection , where features are added sequentially as needed , edges are pruned early , and decisions are made online for each sentence . we model this as a sequential decision-making problem and solve it by imitation learning techniques . we test our method on 7 languages . our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features , while computing fewer than 30 % of the feature templates .

learning emotion indicators from tweets : hashtags , hashtag patterns ,
we present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : affection , anger/rage , fear/anxiety , joy , and sadness/disappointment . starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns . this process then repeats in a bootstrapping framework . emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers . we show that the learned set of emotion indicators yields a substantial improvement in f-scores , ranging from + % 5 to + % 18 over baseline classifiers .

fipscoview : on-line visualisation of collocations extracted from multilingual parallel corpora
we introduce fipscoview , an on-line interface for dictionary-like visualisation of collocations detected from parallel corpora using a syntactically-informed extraction method .

bioex : a novel user-interface that accesses images from abstract sentences
images ( i.e. , figures or tables ) are important experimental results that are typically reported in bioscience full-text articles . biologists need to access the images to validate research facts and to formulate or to test novel research hypotheses . we designed , evaluated , and implemented a novel user-interface , bioex , that allows biologists to access images that appear in a full-text article directly from the abstract of the article .

adapting naturally occurring test suites for evaluation of clinical question answering
this paper describes the structure of a test suite for evaluation of clinical question answering systems ; presents several manually compiled resources found useful for test suite generation ; and describes the adaptation of these resources for evaluation of a clinical question answering system .

cmu-at : semantic distance and background knowledge for identifying semantic relations
this system uses a background knowledge base to identify semantic relations between base noun phrases in english text , as evaluated in semeval 2007 , task 4. training data for each relation is converted to statements in the scone knowledge representation language . at testing time a new scone statement is created for the sentence under scrutiny , and presence or absence of a relation is calculated by comparing the total semantic distance between the new statement and all positive examples to the total distance between the new statement and all negative examples .

extracting bilingual dictionary from comparable corpora with kun yu junichi tsujii
this paper proposes an approach for bilingual dictionary extraction from comparable corpora . the proposed approach is based on the observation that a word and its translation share similar dependency relations . experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates .

integrating phrase-based reordering features into a chart-based decoder for machine translation
hiero translation models have two limitations compared to phrase-based models : 1 ) limited hypothesis space ; 2 ) no lexicalized reordering model . we propose an extension of hiero called phrasalhiero to address hieros second problem . phrasal-hiero still has the same hypothesis space as the original hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder . the work consists of two parts : 1 ) for each hiero translation derivation , find its corresponding discontinuous phrase-based path . 2 ) extend the chart decoder to incorporate features from the phrase-based path . we achieve significant improvement over both hiero and phrase-based baselines for arabicenglish , chinese-english and germanenglish translation .

adaptive development data selection for log-linear model in statistical machine translation
this paper addresses the problem of dynamic model parameter selection for loglinear model based statistical machine translation ( smt ) systems . in this work , we propose a principled method for this task by transforming it to a test data dependent development set selection problem . we present two algorithms for automatic development set construction , and evaluated our method on several nist data sets for the chinese-english translation task . experimental results show that our method can effectively adapt log-linear model parameters to different test data , and consistently achieves good translation performance compared with conventional methods that use a fixed model parameter setting across different data sets .

generalizing dimensionality in combinatory categorial grammar iccs , division of informatics
we extend combinatory categorial grammar ( ccg ) with a generalized notion of multidimensional sign , inspired by the types of representations found in constraint-based frameworks like hpsg or lfg . the generalized sign allows multiple levels to share information , but only in a resource-bounded way through a very restricted indexation mechanism . this improves representational perspicuity without increasing parsing complexity , in contrast to full-blown unification used in hpsg and lfg . well-formedness of a linguistic expressions remains entirely determined by the ccg derivation . we show how the multidimensionality and perspicuity of the generalized signs lead to a simplification of previous ccg accounts of how word order and prosody can realize information structure .

unsupervised event coreference resolution with rich linguistic features cosmin adrian bejan
this paper examines how a new class of nonparametric bayesian models can be effectively applied to an open-domain event coreference task . designed with the purpose of clustering complex linguistic objects , these models consider a potentially infinite number of features and categorical outcomes . the evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task .

citius : a naive-bayes strategy for sentiment analysis on english tweets
this article describes a strategy based on a naive-bayes classifier for detecting the polarity of english tweets . the experiments have shown that the best performance is achieved by using a binary classifier between just two sharp polarity categories : positive and negative . in addition , in order to detect tweets with and without polarity , the system makes use of a very basic rule that searchs for polarity words within the analysed tweets/texts . when the classifier is provided with a polarity lexicon and multiwords it achieves 63 % f-score .

using mined coreference chains as a resource for a semantic task
we propose to use coreference chains extracted from a large corpus as a resource for semantic tasks . we extract three million coreference chains and train word embeddings on them . then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve f 1 on the task of antonym classification by up to .09 .

sentence ordering driven by local and global coherence for summary generation
in summarization , sentence ordering is conducted to enhance summary readability by accommodating text coherence . we propose a grouping-based ordering framework that integrates local and global coherence concerns . summary sentences are grouped before ordering is applied on two levels : group-level and sentence-level . different algorithms for grouping and ordering are discussed . the preliminary results on single-document news datasets demonstrate the advantage of our method over a widely accepted method .

evaluation of the bible as a resource for cross-language information retrieval
an area of recent interest in crosslanguage information retrieval ( clir ) is the question of which parallel corpora might be best suited to tasks in clir , or even to what extent parallel corpora can be obtained or are necessary . one proposal , which in our opinion has been somewhat overlooked , is that the bible holds a unique value as a multilingual corpus , being ( among other things ) widely available in a broad range of languages and having a high coverage of modern-day vocabulary . in this paper , we test empirically whether this claim is justified through a series of validation tests on various information retrieval tasks . our results appear to indicate that our methodology may significantly outperform others recently proposed .

the linguists search engine : an overview philip resnik aaron elkiss
the linguists search engine ( lse ) was designed to provide an intuitive , easy-touse interface that enables language researchers to seek linguistically interesting examples on the web , based on syntactic and lexical criteria . we briefly describe its user interface and architecture , as well as recent developments that include lse search capabilities for chinese .

umigon : sentiment analysis for tweets based on lexicons and heuristics
umigon is developed since december 2012 as a web application providing a service of sentiment detection in tweets . it has been designed to be fast and scalable . umigon also provides indications for additional semantic features present in the tweets , such as time indications or markers of subjectivity .

boosting n-gram coverage for unsegmented languages using multiple text segmentation approach
automatic word segmentation errors , for languages having a writing system without word boundaries , negatively affect the performance of language models . as a solution , the use of multiple , instead of unique , segmentation has recently been proposed . this approach boosts n-gram counts and generates new n-grams . however , it also produces bad n-grams that affect the language models ' performance . in this paper , we study more deeply the contribution of our multiple segmentation approach and experiment on an efficient solution to minimize the effect of adding bad n-grams .

grammatical error detection and correction using a single maximum entropy model
this paper describes the system of shanghai jiao tong unvierity team in the conll-2014 shared task . error correction operations are encoded as a group of predefined labels and therefore the task is formulized as a multi-label classification task . for training , labels are obtained through a strict rule-based approach . for decoding , errors are detected and corrected according to the classification results . a single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm . our system achieved precision of 29.83 , recall of 5.16 and f 0.5 of 15.24 in the official evaluation .

exploring the effects of gaze and pauses in situated human-robot interaction
in this paper , we present a user study where a robot instructs a human on how to draw a route on a map , similar to a map task . this setup has allowed us to study user reactions to the robots conversational behaviour in order to get a better understanding of how to generate utterances in incremental dialogue systems . we have analysed the participants ' subjective rating , task completion , verbal responses , gaze behaviour , drawing activity , and cognitive load . the results show that users utilise the robots gaze in order to disambiguate referring expressions and manage the flow of the interaction . furthermore , we show that the users behaviour is affected by how pauses are realised in the robots speech .

effect of domain-specific corpus in compositional translation estimation for technical terms
this paper studies issues on compiling a bilingual lexicon for technical terms . in the task of estimating bilingual term correspondences of technical terms , it is usually quite difficult to find an existing corpus for the domain of such technical terms . in this paper , we take an approach of collecting a corpus for the domain of such technical terms from the web . as a method of translation estimation for technical terms , we propose a compositional translation estimation technique . through experimental evaluation , we show that the domain/topic specific corpus contributes to improving the performance of the compositional translation estimation .

feeding owl : extracting and representing the content of pathology reports
this paper reports on an ongoing project that combines nlp with semantic web technologies to support a content-based storage and retrieval of medical pathology reports . we describe the nlp component of the project ( a robust parser ) and the background knowledge component ( a domain ontology represented in owl ) , and how they work together during extraction of domain specific information from natural language reports . the system provides a good example of how nlp techniques can be used to populate the semantic web .

automation and evaluation of the keyword method for second language learning
in this paper , we combine existing nlp techniques with minimal supervision to build memory tips according to the keyword method , a well established mnemonic device for second language learning . we present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task . the results demonstrate that nlp techniques can effectively support the development of resources for second language learning .

growing related words from seed via user behaviors : a re-ranking
motivated by google sets , we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of chinese input method . our proposed method is motivated by the observation that the more frequently two words cooccur in user records , the more related they are . first , we utilize user behaviors to generate candidate words . then , we utilize search engine to enrich candidate words with adequate semantic features . finally , we reorder candidate words according to their semantic relatedness to the seed word . experimental results on a chinese input method dataset show that our method gains better performance .

dcu-uvt : word-level language classification with code-mixed data and jennifer foster
this paper describes the dcu-uvt teams participation in the language identification in code-switched data shared task in the workshop on computational approaches to code switching . wordlevel classification experiments were carried out using a simple dictionary-based method , linear kernel support vector machines ( svms ) with and without contextual clues , and a k-nearest neighbour approach . based on these experiments , we select our svm-based system with contextual clues as our final system and present results for the nepali-english and spanish-english datasets .

testing and performance evaluation of machine transliteration system for tamil language
machine translation ( mt ) is a science fiction that was converted into reality with the enormous contributions from the mt research community . we can not expect any text without named entities ( ne ) . such nes are crucial in deciding the quality of mt . nes are to be recognized from the text and transliterated accordingly into the target language in order to ensure the quality of mt . in the present paper we present various technical issues encountered during handling the shared task of ne transliteration for tamil .

verb phrase ellipsis detection using automatically parsed text leif arda nielsen
this paper describes a verb phrase ellipsis ( vpe ) detection system , built for robustness , accuracy and domain independence . the system is corpus-based , and uses a variety of machine learning techniques on free text that has been automatically parsed using two different parsers . tested on a mixed corpus comprising a range of genres , the system achieves a 72 % f1-score . it is designed as the first stage of a complete vpe resolution system that is input free text , detects vpes , and proceeds to find the antecedents and resolve them .

online learning of approximate dependency parsing algorithms ryan mcdonald fernando pereira
in this paper we extend the maximum spanning tree ( mst ) dependency parsing framework of mcdonald et al ( 2005c ) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word . we show that those extensions can make the mst framework computationally intractable , but that the intractability can be circumvented with new approximate parsing algorithms . we conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for czech and danish .

adapting a medical speech to speech translation system ( medslt )
we describe the adaptation for arabic of the grammar-based medslt medical speech system . the system supports simple medical diagnosis questions about headaches using vocabulary of 322 words . we show that the medslt architecture based on motivated general grammars produces very good results , with a limited effort . based on the grammars for other languages covered by the system , it is in fact very easy to develop an arabic grammar and to specialize it efficiently for the different system tasks . in this paper , we focus on generation .

an improved extraction pattern representation model for automatic ie pattern acquisition
several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction . each approach is based on a particular model for the patterns to be acquired , such as a predicate-argument structure or a dependency chain . the effect of these alternative models has not been previously studied . in this paper , we compare the prior models and introduce a new model , the subtree model , based on arbitrary subtrees of dependency trees . we describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using subtree patterns .

evaluation of features for sentence extraction on different types of corpora
we report evaluation results for our summarization system and analyze the resulting summarization data for three different types of corpora . to develop a robust summarization system , we have created a system based on sentence extraction and applied it to summarize japanese and english newspaper articles , obtained some of the top results at two evaluation workshops . we have also created sentence extraction data from japanese lectures and evaluated our system with these data . in addition to the evaluation results , we analyze the relationships between key sentences and the features used in sentence extraction . we find that discrete combinations of features match distributions of key sentences better than sequential combinations .

word representations : a simple and general method for semi-supervised learning
if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston ( 2008 ) embeddings , and hlbl ( mnih & hinton , 2009 ) embeddings of words on both ner and chunking . we use near state-of-the-art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off-the-shelf use in existing nlp systems , as well as our code , here : http : //metaoptimize . com/projects/wordreprs/

applying a grammar-based language model to a simplified broadcast-news transcription task
we propose a language model based on a precise , linguistically motivated grammar ( a hand-crafted head-driven phrase structure grammar ) and a statistical model estimating the probability of a parse tree . the language model is applied by means of an n-best rescoring step , which allows to directly measure the performance gains relative to the baseline system without rescoring . to demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks , we applied it to a simplified german broadcast-news transcription task . we report a significant reduction in word error rate compared to a state-of-the-art baseline system .

discriminative language modeling with conditional random fields and the perceptron algorithm
this paper describes discriminative language modeling for a large vocabulary speech recognition task . we contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( crfs ) . the models are encoded as deterministic weighted finite state automata , and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer . the perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data . however , using the feature set output from the perceptron algorithm ( initialized with their weights ) , crf training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 % .

a structured model for joint learning of masayuki asahara yuji matsumoto
in predicate-argument structure analysis , it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments . however , no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments . in this paper we propose a structured model that overcomes the limitation of existing approaches ; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument . in experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure .

an analysis of frequency- and memory-based processing costs marten van schijndel
the frequency of words and syntactic constructions has been observed to have a substantial effect on language processing . this begs the question of what causes certain constructions to be more or less frequent . a theory of grounding ( phillips , 2010 ) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs . this paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times . measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing .

adapting an ner-system for german to the biomedical domain duisburg germany
in this paper , we report the adaptation of a named entity recognition ( ner ) system to the biomedical domain in order to participate in the shared task bio-entity recognition . the system is originally developed for german ner that shares characteristics with the biomedical task . to facilitate adaptability , the system is knowledge-poor and utilizes unlabeled data . investigating the adaptability of the single components and the enhancements necessary , we get insights into the task of bioentity recognition .

learning foci for question answering over topic maps and rani pinchuk
this paper introduces the concepts of asking point and expected answer type as variations of the question focus . they are of particular importance for qa over semistructured data , as represented by topic maps , owl or custom xml formats . we describe an approach to the identification of the question focus from questions asked to a question answering system over topic maps by extracting the asking point and falling back to the expected answer type when necessary . we use known machine learning techniques for expected answer type extraction and we implement a novel approach to the asking point extraction . we also provide a mathematical model to predict the performance of the system .

a flexible stand-off data model with query language for multi-level annotation
we present an implemented xml data model and a new , simplified query language for multi-level annotated corpora . the new query language involves automatic conversion of queries into the underlying , more complicated mmaxql query language . it supports queries for sequential and hierarchical , but also associative ( e.g . coreferential ) relations . the simplified query language has been designed with non-expert users in mind .

computational analysis to explore authors depiction of characters cecilia ovesdotter alm
this study involves automatically identifying the sociolinguistic characteristics of fictional characters in plays by analyzing their written speech . we discuss three binary classification problems : predicting the characters gender ( male vs. female ) , age ( young vs. old ) , and socioeconomic standing ( upper-middle class vs. lower class ) . the text corpus used is an annotated collection of august strindberg and henrik ibsen plays , translated into english , which are in the public domain . these playwrights were chosen for their known attention to relevant socioeconomic issues in their work . linguistic and textual cues are extracted from the characters lines ( turns ) for modeling purposes . we report on the dataset as well as the performance and important features when predicting each of the sociolinguistic characteristics , comparing intra- and inter-author testing .

automatic cluster stopping with criterion functions and the gap statistic
senseclusters is a freely available system that clusters similar contexts . it can be applied to a wide range of problems , although here we focus on word sense and name discrimination . it supports several different measures for automatically determining the number of clusters in which a collection of contexts should be grouped . these can be used to discover the number of senses in which a word is used in a large corpus of text , or the number of entities that share the same name . there are three measures based on clustering criterion functions , and another on the gap statistic .

go climb a dependency tree and correct the grammatical errors
state-of-art systems for grammar error correction often correct errors based on word sequences or phrases . in this paper , we describe a grammar error correction system which corrects grammatical errors at tree level directly . we cluster all error into two groups and divide our system into two modules correspondingly : the general module and the special module . in the general module , we propose a treenode language model to correct errors related to verbs and nouns . the treenode language model is easy to train and the decoding is efficient . in the special module , two extra classification models are trained to correct errors related to determiners and prepositions . experiments show that our system outperforms the state-of-art systems and improves the f 1 score .

tagging icelandic text using a linguistic and a statistical tagger
we describe our linguistic rule-based tagger icetagger , and compare its tagging accuracy to the tnt tagger , a state-of-theart statistical tagger , when tagging icelandic , a morphologically complex language . evaluation shows that the average tagging accuracy is 91.54 % and 90.44 % , obtained by icetagger and tnt , respectively . when tag profile gaps in the lexicon , used by the tnt tagger , are filled with tags produced by our morphological analyser icemorphy , tnts tagging accuracy increases to 91.18 % .

the ( non ) utility of predicate-argument frequencies for pronoun
state-of-the-art pronoun interpretation systems rely predominantly on morphosyntactic contextual features . while the use of deep knowledge and inference to improve these models would appear technically infeasible , previous work has suggested that predicate-argument statistics mined from naturally-occurring data could provide a useful approximation to such knowledge . we test this idea in several system configurations , and conclude from our results and subsequent error analysis that such statistics offer little or no predictive information above that provided by morphosyntax .

better arabic parsing : baselines , evaluations , and analysis
in this paper , we offer broad insight into the underperformance of arabic constituency parsing by analyzing the interplay of linguistic phenomena , annotation choices , and model design . first , we identify sources of syntactic ambiguity understudied in the existing parsing literature . second , we show that although the penn arabic treebank is similar to other treebanks in gross statistical terms , annotation consistency remains problematic . third , we develop a human interpretable grammar that is competitive with a latent variable pcfg . fourth , we show how to build better models for three different parsers . finally , we show that in application settings , the absence of gold segmentation lowers parsing performance by 25 % f1 .

modeling wisdom of crowds using latent mixture of discriminative experts
in many computational linguistic scenarios , training labels are subjectives making it necessary to acquire the opinions of multiple annotators/experts , which is referred to as wisdom of crowds . in this paper , we propose a new approach for modeling wisdom of crowds based on the latent mixture of discriminative experts ( lmde ) model that can automatically learn the prototypical patterns and hidden dynamic among different experts . experiments show improvement over state-of-the-art approaches on the task of listener backchannel prediction in dyadic conversations .

swiss-chocolate : sentiment detection using sparse svms and part-of-speech n-grams
we describe a classifier to predict the message-level sentiment of english microblog messages from twitter . this paper describes the classifier submitted to the semeval-2014 competition ( task 9b ) . our approach was to build up on the system of the last years winning approach by nrc canada 2013 , with some modifications and additions of features , and additional sentiment lexicons . furthermore , we used a sparse ( ` 1 -regularized ) svm , instead of the more commonly used ` 2 -regularization , resulting in a very sparse linear classifier .

reformulating discourse connectives for non-expert readers advaith siddharthan napoleon katsos
in this paper we report a behavioural experiment documenting that different lexicosyntactic formulations of the discourse relation of causation are deemed more or less acceptable by different categories of readers . we further report promising results for automatically selecting the formulation that is most appropriate for a given category of reader using supervised learning . this investigation is embedded within a longer term research agenda aimed at summarising scientific writing for lay readers using appropriate paraphrasing .

improving word translation disambiguation by capturing multiword expressions with dictionaries
the paper describes a method for identifying and translating multiword expressions using a bi-directional dictionary . while a dictionarybased approach suffers from limited recall , precision is high ; hence it is best employed alongside an approach with complementing properties , such as an n-gram language model . we evaluate the method on data from the english-german translation part of the crosslingual word sense disambiguation task in the 2010 semantic evaluation exercise ( semeval ) . the output of a baseline disambiguation system based on n-grams was substantially improved by matching the target words and their immediate contexts against compound and collocational words in a dictionary .

towards free-text semantic parsing : a unified framework based on
this article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : framenet , verbnet and propbank . the framenet corpus contains the examples annotated with semantic roles whereas the verbnet lexicon provides the knowledge about the syntactic behavior of the verbs . we connect verbnet and framenet by mapping the framenet frames to the verbnet intersective levin classes . the propbank corpus , which is tightly connected to the verbnet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach . the results indicate that our model is an interesting step towards the design of free-text semantic parsers .

transforming projective bilexical dependency grammars into efficiently-parsable cfgs with unfold-fold
this paper shows how to use the unfoldfold transformation to transform projective bilexical dependency grammars ( pbdgs ) into ambiguity-preserving weakly equivalent context-free grammars ( cfgs ) . these cfgs can be parsed in o ( n3 ) time using a cky algorithm with appropriate indexing , rather than the o ( n5 ) time required by a naive encoding . informally , using the cky algorithm with such a cfg mimics the steps of the eisner-satta o ( n3 ) pbdg parsing algorithm . this transformation makes all of the techniques developed for cfgs available to pbdgs . we demonstrate this by describing a maximum posterior parse decoder for pbdgs .

experimental support for a categorical compositional distributional model of meaning
modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists . we implement the abstract categorical model of coecke et al ( 2010 ) using data from the bnc and evaluate it . the implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments . the evaluation is based on the word disambiguation task developed by mitchell and lapata ( 2008 ) for intransitive sentences , and on a similar new experiment designed for transitive sentences . our model matches the results of its competitors in the first experiment , and betters them in the second . the general improvement in results with increase in syntactic complexity showcases the compositional power of our model .

a unified framework for automatic evaluation using n-gram co-occurrence statistics
in this paper we propose a unified framework for automatic evaluation of nlp applications using n-gram co-occurrence statistics . the automatic evaluation metrics proposed to date for machine translation and automatic summarization are particular instances from the family of metrics we propose . we show that different members of the same family of metrics explain best the variations obtained with human evaluations , according to the application being evaluated ( machine translation , automatic summarization , and automatic question answering ) and the evaluation guidelines used by humans for evaluating such applications .

dependency parser adaptation with subtrees from auto-parsed target domain data
in this paper , we propose a simple and effective approach to domain adaptation for dependency parsing . this is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data . to demonstrate the effectiveness of the proposed approach , we evaluate it on three pairs of source-target data , compared with several common baseline systems and previous approaches . our approach achieves significant improvement on all the three pairs of data sets .

automatic paragraph identification : a study across languages and domains
in this paper we investigate whether paragraphs can be identified automatically in different languages and domains . we propose a machine learning approach which exploits textual and discourse cues and we assess how well humans perform on this task . our best models achieve an accuracy that is significantly higher than the best baseline and , for most data sets , comes to within 6 % of human performance .

turning on the turbo : fast third-order non-projective turbo parsers
we present fast , accurate , direct nonprojective dependency parsers with thirdorder features . our approach uses ad3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models . experiments in fourteen languages yield parsing speeds competitive to projective parsers , with state-ofthe-art accuracies for the largest datasets ( english , czech , and german ) .

deepak ravichandran , eduard hovy , and franz josef och
in this paper , we show that we can obtain a good baseline performance for question answering ( qa ) by using only 4 simple features . using these features , we contrast two approaches used for a maximum entropy based qa system . we view the qa problem as a classification problem and as a reranking problem . our results indicate that the qa system viewed as a reranker clearly outperforms the qa system used as a classifier . both systems are trained using the same data .

combining lexical and syntactic features for supervised word sense disambiguation
the success of supervised learning approaches to word sense disambiguation is largely dependent on the features used to represent the context in which an ambiguous word occurs . previous work has reached mixed conclusions ; some suggest that combinations of syntactic and lexical features will perform most effectively . however , others have shown that simple lexical features perform well on their own . this paper evaluates the effect of using different lexical and syntactic features both individually and in combination . we show that it is possible for a very simple ensemble that utilizes a single lexical feature and a sequence of part of speech features to result in disambiguation accuracy that is near state of the art .

coupling ccg and hybrid logic dependency semantics division of informatics
categorial grammar has traditionally used the -calculus to represent meaning . we present an alternative , dependency-based perspective on linguistic meaning and situate it in the computational setting . this perspective is formalized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism . finally , we show how we can couple this formalization to combinatory categorial grammar to produce interpretations compositionally .

integrating discourse markers into a pipelined natural language generation architecture
pipelined natural language generation ( nlg ) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions , lexical choice , and revision . this has given rise to discussions about the relative placement of these new modules in the overall architecture . recent work on another aspect of multi-paragraph text , discourse markers , indicates it is time to consider where a discourse marker insertion algorithm fits in . we present examples which suggest that in a pipelined nlg architecture , the best approach is to strongly tie it to a revision component . finally , we evaluate the approach in a working multi-page system .

automatic indexing of specialized documents : national library of medicine
the shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized . in the biomedical domain , continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as medline . in this paper , we evaluate two statistical methods of producing mesh indexing recommendations for the genetics literature , including recommendations involving subheadings , which is a novel application for the methods . we show that a generic representation of the documents yields both better precision and recall . we also find that a domainspecific representation of the documents can contribute to enhancing recall .

structuring operative notes using active learning national library of medicine
we present an active learning method for placing the event mentions in an operative note into a pre-specified event structure . event mentions are first classified into action , peripheral action , observation , and report events . the actions are further classified into their appropriate location within the event structure . we examine how utilizing active learning significantly reduces the time needed to completely annotate a corpus of 2,820 appendectomy notes .

separating fact from fear : tracking flu infections on twitter
twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza . however , previous work has relied on simple content analysis , which conflates flu tweets that report infection with those that express concerned awareness of the flu . by discriminating these categories , as well as tweets about the authors versus about others , we demonstrate significant improvements on influenza surveillance using twitter .

where 's the verb correcting machine translation during question answering
when a multi-lingual question-answering ( qa ) system provides an answer that has been incorrectly translated , it is very likely to be regarded as irrelevant . in this paper , we propose a novel method for correcting a deletion error that affects overall understanding of the sentence . our post-editing technique uses information available at query time : examples drawn from related documents determined to be relevant to the query . our results show that 4 % -7 % of mt sentences are missing the main verb and on average , 79 % of the modified sentences are judged to be more comprehensible . the qa performance also benefits from the improved mt : 7 % of irrelevant response sentences become relevant .

language independent ner using a unified model of internal and
this paper investigates the use of a language independent model for named entity recognition based on iterative learning in a co-training fashion , using word-internal and contextual information as independent evidence sources . its bootstrapping process begins with only seed entities and seed contexts extracted from the provided annotated corpus . f-measure exceeds 77 in spanish and 72 in dutch .

from grammar-independent construction enumeration to lexical types in computational grammars
the paper presents a code for enumerating verb-construction templates , from which lexical type inventories of computational grammars can be derived , and test suites can be systematically developed . the templates also serve for descriptive and typological research . the code is string-based , with divisions into slots providing modularity and flexibility of specification .

identifying argumentative discourse structures in persuasive essays and iryna gurevych
in this paper , we present a novel approach for identifying argumentative discourse structures in persuasive essays . the structure of argumentation consists of several components ( i.e . claims and premises ) that are connected with argumentative relations . we consider this task in two consecutive steps . first , we identify the components of arguments using multiclass classification . second , we classify a pair of argument components as either support or non-support for identifying the structure of argumentative discourse . for both tasks , we evaluate several classifiers and propose novel feature sets including structural , lexical , syntactic and contextual features . in our experiments , we obtain a macro f1-score of 0.726 for identifying argument components and 0.722 for argumentative relations .

lda based similarity modeling for question answering speech technology and
we present an exploration of generative modeling for the question answering ( qa ) task to rank candidate passages . we investigate latent dirichlet allocation ( lda ) models to obtain ranking scores based on a novel similarity measure between a natural language question posed by the user and a candidate passage . we construct two models each one introducing deeper evaluations on latent characteristics of passages together with given question . with the new representation of topical structures on qa datasets , using a limited amount of world knowledge , we show improvements on performance of a qa ranking system .

combination of machine learning methods for optimum chinese word segmentation
this article presents our recent work for participation in the second international chinese word segmentation bakeoff . our system performs two procedures : out-ofvocabulary extraction and word segmentation . we compose three out-of-vocabulary extraction modules : character-based tagging with different classifiers maximum entropy , support vector machines , and conditional random fields . we also compose three word segmentation modules character-based tagging by maximum entropy classifier , maximum entropy markov model , and conditional random fields . all modules are based on previously proposed methods . we submitted three systems which are different combination of the modules .

multi-dimensional annotation and alignment in an english-german computational linguistics &
this paper presents the compilation of the croco corpus , an english-german translation corpus . corpus design , annotation and alignment are described in detail . in order to guarantee the searchability and exchangeability of the corpus , xml stand-off mark-up is used as representation format for the multi-layer annotation . on this basis it is shown how the corpus can be queried using xquery . furthermore , the generalisation of results in terms of linguistic and translational research questions is briefly discussed .

linguistically annotated btg for statistical machine translation human language technology
bracketing transduction grammar ( btg ) is a natural choice for effective integration of desired linguistic knowledge into statistical machine translation ( smt ) . in this paper , we propose a linguistically annotated btg ( labtg ) for smt . it conveys linguistic knowledge of source-side syntax structures to btg hierarchical structures through linguistic annotation . from the linguistically annotated data , we learn annotated btg rules and train linguistically motivated phrase translation model and reordering model . we also present an annotation algorithm that captures syntactic information for btg nodes . the experiments show that the labtg approach significantly outperforms a baseline btgbased system and a state-of-the-art phrasebased system on the nistmt-05 chineseto-english translation task . moreover , we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering .

optimization-based content selection for opinion summarization jackie chi kit cheung
we introduce a content selection method for opinion summarization based on a well-studied , formal mathematical model , the p-median clustering problem from facility location theory . our method replaces a series of local , myopic steps to content selection with a global solution , and is designed to allow content and realization decisions to be naturally integrated . we evaluate and compare our method against an existing heuristic-based method on content selection , using human selections as a gold standard . we find that the algorithms perform similarly , suggesting that our content selection method is robust enough to support integration with other aspects of summarization .

topic adaptation for lecture translation through bilingual latent semantic models
this work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing probabilistic latent semantic analysis ( plsa ) during model training . during inference , documents containing only the source language can be used to infer a full topic-word distribution on all words in the target languages vocabulary , from which we perform minimum discrimination information ( mdi ) adaptation on a background language model ( lm ) . we apply our approach on the english-french iwslt 2010 ted talk exercise , and report a 15 % reduction in perplexity and relative bleu and nist improvements of 3 % and 2.4 % , respectively over a baseline only using a 5-gram background lm over the entire translation task . our topic modeling approach is simpler to construct than its counterparts .

insight galway : syntactic and lexical features for aspect based
this work analyses various syntactic and lexical features for sentence level aspect based sentiment analysis . the task focuses on detection of a writers sentiment towards an aspect which is explicitly mentioned in a sentence . the target sentiment polarities are positive , negative , conflict and neutral . we use a supervised learning approach , evaluate various features and report accuracies which are much higher than the provided baselines . best features include unigrams , clauses , dependency relations and sentiwordnet polarity scores .

spred : large-scale harvesting of semantic predicates dipartimento di informatica
we present spred , a novel method for the creation of large repositories of semantic predicates . we start from existing collocations to form lexical predicates ( e.g. , break ) and learn the semantic classes that best fit the argument . to do this , we extract all the occurrences in wikipedia which match the predicate and abstract its arguments to general semantic classes ( e.g. , break body part , break agreement , etc . ) . our experiments show that we are able to create a large collection of semantic predicates from the oxford advanced learners dictionary with high precision and recall , and perform well against the most similar approach .

unal-nlp : cross-lingual phrase sense disambiguation with syntactic dependency trees
in this paper we describe our participation in the semeval 2014 , task 5 , consisting of the construction of a translation assistance system that translates l1 fragments , written in l2 context , to their correct l2 translation . our approach consists of a bilingual parallel corpus , a system of syntactic features extraction and a statistical memory-based classification algorithm . our system ranked 4th and 6th among the 10 participating systems that used the english-spanish data set .

rosetta stone linguistic problems international linguistics olympiad international linguistics olympiad
this paper describes the process of composing problems that are suitable for competitions in linguistics . the type of problems described is rosetta stonea bilingual problem where typically one of the languages is unknown , and the other is the native language of the person solving the problem . the process includes selecting phenomena , composing and arranging the data and assignments in order to illustrate the phenomena , and verifying the solvability and complexity of the problem .

hierarchical phrase-based translation representations gonzalo iglesias cyril allauzen william byrne
this paper compares several translation representations for a synchronous context-free grammar parse including cfgs/hypergraphs , finite-state automata ( fsa ) , and pushdown automata ( pda ) . the representation choice is shown to determine the form and complexity of target lm intersection and shortest-path algorithms that follow . intersection , shortest path , fsa expansion and rtn replacement algorithms are presented for pdas . chinese-toenglish translation experiments using hifst and hipdt , fsa and pda-based decoders , are presented using admissible ( or exact ) search , possible for hifst with compact scfg rulesets and hipdt with compact lms . for large rulesets with large lms , we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance .

dale : a word sense disambiguation system for biomedical documents
automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings . these ambiguities can still be found when language is restricted to a particular domain , such as biomedicine . word sense disambiguation ( wsd ) systems attempt to resolve these ambiguities but are often only able to identify the meanings for a small set of ambiguous terms . dale ( disambiguation using automatically labeled examples ) is a supervised wsd system that can disambiguate a wide range of ambiguities found in biomedical documents . dale uses the umls metathesaurus as both a sense inventory and as a source of information for automatically generating labeled training examples . dale is able to disambiguate biomedical documents with the coverage of unsupervised approaches and accuracy of supervised methods .

a large scale ranker-based system for search query spelling correction
this paper makes three significant extensions to a noisy channel speller designed for standard written text to target the challenging domain of search queries . first , the noisy channel model is subsumed by a more general ranker , which allows a variety of features to be easily incorporated . second , a distributed infrastructure is proposed for training and applying web scale n-gram language models . third , a new phrase-based error model is presented . this model places a probability distribution over transformations between multi-word phrases , and is estimated using large amounts of query-correction pairs derived from search logs . experiments show that each of these extensions leads to significant improvements over the stateof-the-art baseline methods .

a study of information retrieval weighting schemes for sentiment analysis
most sentiment analysis approaches use as baseline a support vector machines ( svm ) classifier with binary unigram weights . in this paper , we explore whether more sophisticated feature weighting schemes from information retrieval can enhance classification accuracy . we show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy , especially when using a sublinear function for term frequency weights and document frequency smoothing . the techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge .

focus to emphasize tone structures for prosodic analysis in spoken
we analyze the concept of focus in speech and the relationship between focus and speech acts for prosodic generation . we determine how the speakers utterances are influenced by speakers intention . the relationship between speech acts and focus information is used to define which parts of the sentence serve as the focus parts . we propose the focus to emphasize tones ( fet ) structure to analyze the focus components . we also design the fet grammar to analyze the intonation patterns and produce tone marks as a result of our analysis . we present a proof-of-the-concept working example to validate our proposal . more comprehensive evaluations are part of our current work .

lexicalization in crosslinguistic probabilistic parsing : the case of french
this paper presents the first probabilistic parsing results for french , using the recently released french treebank . we start with an unlexicalized pcfg as a baseline model , which is enriched to the level of collins model 2 by adding lexicalization and subcategorization . the lexicalized sister-head model and a bigram model are also tested , to deal with the flatness of the french treebank . the bigram model achieves the best performance : 81 % constituency f-score and 84 % dependency accuracy . all lexicalized models outperform the unlexicalized baseline , consistent with probabilistic parsing results for english , but contrary to results for german , where lexicalization has only a limited effect on parsing performance .

now we stronger than ever : african-american syntax in twitter
african american english ( aae ) is a well-established dialect that exhibits a distinctive syntax , including constructions like habitual be . using data mined from the social media service twitter , the proposed senior thesis project intends to study the demographic distribution of a subset of aae syntactic constructions . this study expands on previous sociolinguistic twitter work ( eisenstein et al. , 2011 ) by adding part-of-speech tags to the data , thus enabling detection of short-range syntactic features . through an analysis of ethnic and gender data associated with aae tweets , this project will provide a more accurate description of the dialects speakers and distribution .

the importance of discourse context for statistical natural language generation
surface realization in statistical natural language generation is based on the idea that when there are many ways to say the same thing , the most frequent option based on corpus counts is the best . based on data from english and finnish , we argue instead that all options are not equivalent , and the most frequent one can be incoherent in some contexts . a statistical nlg system where word order choice is based only on frequency counts of forms can not capture the contextually-appropriate use of word order . we describe an alternative method for word order selection and show how it outperforms a frequency-only approach .

abductive reasoning with a large knowledge base for discourse processing
this paper presents a discourse processing framework based on weighted abduction . we elaborate on ideas described in hobbs et al ( 1993 ) and implement the abductive inference procedure in a system called mini-tacitus . particular attention is paid to constructing a large and reliable knowledge base for supporting inferences . for this purpose we exploit such lexical-semantic resources as wordnet and framenet . we test the proposed procedure and the obtained knowledge base on the recognizing textual entailment task using the data sets from the rte-2 challenge for evaluation . in addition , we provide an evaluation of the semantic role labeling produced by the system taking the frame-annotated corpus for textual entailment as a gold standard .

hit-ir-wsd : a wsd system for english lexical sample task
hit-ir-wsd is a word sense disambiguation ( wsd ) system developed for english lexical sample task ( task 11 ) of semeval 2007 by information retrieval lab , harbin institute of technology . the system is based on a supervised method using an svm classifier . multi-resources including words in the surrounding context , the partof-speech of neighboring words , collocations and syntactic relations are used . the final micro-avg raw score achieves 81.9 % on the test set , the best one among participating runs .

bounce : sentiment classification in twitter using rich feature sets
the widespread use of twitter makes it very interesting to determine the opinions and the sentiments expressed by its users . the shortness of the length and the highly informal nature of tweets render it very difficult to automatically detect such information . this paper reports the results to a challenge , set forth by semeval-2013 task 2 , to determine the positive , neutral , or negative sentiments of tweets . two systems are explained : system a for determining the sentiment of a phrase within a tweet and system b for determining the sentiment of a tweet . both approaches rely on rich feature sets , which are explained in detail .

using comparable corpora to solve problems difficult for human translators
in this paper we present a tool that uses comparable corpora to find appropriate translation equivalents for expressions that are considered by translators as difficult . for a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions . in the paper we discuss the method and present results of human evaluation of the performance of the tool , which highlight its usefulness when dictionary solutions are lacking .

generating english determiners in phrase-based translation with synthetic translation options
we propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation optionsphrasal translations that are generated by auxiliary translation and postediting processesto augment the default phrase inventory learned from parallel data . we apply our technique to the problem of producing english determiners when translating from russian and czech , languages that lack definiteness morphemes . our approach augments the english side of the phrase table using a classifier to predict where english articles might plausibly be added or removed , and then we decode as usual . doing so , we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier .

determining the specificity of terms using compositional and contextual information
this paper introduces new specificity determining methods for terms using compositional and contextual information . specificity of terms is the quantity of domain specific information that is contained in the terms . the methods are modeled as information theory like measures . as the methods dont use domain specific information , they can be applied to other domains without extra processes . experiments showed very promising result with the precision of 82.0 % when the methods were applied to the terms in mesh thesaurus .

survey in sentiment , polarity and function analysis of citation
in this paper we proposed a survey in sentiment , polarity and function analysis of citations . this is an interesting area that has had an increased development in recent years but still has plenty of room for growth and further research . the amount of scientific information in the web makes it necessary innovate the analysis of the influence of the work of peers and leaders in the scientific community . we present an overview of general concepts , review contributions to the solution of related problems such as context identification , function and polarity classification , identify some trends and suggest possible future research directions .

mining transliterations from web query results : an incremental approach
we study an adaptive learning framework for phonetic similarity modeling ( psm ) that supports the automatic acquisition of transliterations by exploiting minimum prior knowledge about machine transliteration to mine transliterations incrementally from the live web . we formulate an incremental learning strategy for the framework based on bayesian theory for psm adaptation . the idea of incremental learning is to benefit from the continuously developing history to update a static model towards the intended reality . in this way , the learning process refines the psm incrementally while constructing a transliteration lexicon at the same time on a development corpus . we further demonstrate that the proposed learning framework is reliably effective in mining live transliterations from web query results .

a memorybased learning approach to event extraction in biomedical texts
in this paper we describe the memory-based machine learning system that we submitted to the bionlp shared task on event extraction . we modeled the event extraction task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding . the results obtained by our system ( 30.58 f-score in task 1 and 29.27 in task 2 ) suggest that the approach and the system need further adaptation to the complexity involved in extracting biomedical events .

development of the korean resource grammar : towards grammar customization
the korean resource grammar ( krg ) is a computational open-source grammar of korean ( kim and yang , 2003 ) that has been constructed within the delph-in consortium since 2003. this paper reports the second phase of the krg development that moves from a phenomenabased approach to grammar customization using the lingo grammar matrix . this new phase of development not only improves the parsing efficiency but also adds generation capacity , which is necessary for many nlp applications .

a link to the past : constructing historical social networks
to assist in the research of social networks in history , we develop machine-learning-based tools for the identification and classification of personal relationships . our case study focuses on the dutch social movement between 1870 and 1940 , and is based on biographical texts describing the lives of notable people in this movement . we treat the identification and the labeling of relations between two persons into positive , neutral , and negative both as a sequence of two tasks and as a single task . we observe that our machine-learning classifiers , support vector machines , produce better generalization performance on the single task . we show how a complete social network can be built from these classifications , and provide a qualitative analysis of the induced network using expert judgements on samples of the network .

learning a compositional semantic parser using an existing syntactic parser
we present a new approach to learning a semantic parser ( a system that maps natural language sentences into logical form ) . unlike previous methods , it exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation . the resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control .

exploiting comparable corpora and bilingual dictionaries for cross-language text categorization
cross-language text categorization is the task of assigning semantic classes to documents written in a target language ( e.g . english ) while the system is trained using labeled documents in a source language ( e.g . italian ) . in this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible . the core technique relies on the automatic acquisition of multilingual domain models from comparable corpora . experiments show the effectiveness of our approach , providing a low cost solution for the cross language text categorization task . in particular , when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization .

a second language acquisition model using ari rappoport vera sheinman
we present a computational model of acquiring a second language from example sentences . our learning algorithms build a construction grammar language model , and generalize using form-based patterns and the learners conceptual system . we use a unique professional language learning corpus , and show that substantial reliable learning can be achieved even though the corpus is very small . the model is applied to assisting the authoring of japanese language learning corpora .

efcient parsing of highly ambiguous context-free grammars with bit vectors
an efficient bit-vector-based cky-style parser for context-free parsing is presented . the parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences . the parser uses bit-vector operations to parallelise the basic parsing operations . the parser is particularly useful when all analyses are needed rather than just the most probable one .

pu-bcd : exponential family models for the coarse- and fine-grained
this paper describes an exponential family model of word sense which captures both occurrences and co-occurrences of words and senses in a joint probability distribution . this statistical framework lends itself to the task of word sense disambiguation . we evaluate the performance of the model in its participation on the semeval-2007 coarse- and fine-grained all-words tasks under a variety of parameters .

challenges for annotating images for sense disambiguation cecilia ovesdotter alm
we describe an unusual data set of thousands of annotated images with interesting sense phenomena . natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text . these issues are discussed and illustrated , including the distinction between word senses and iconographic senses .

using bilingual comparable corpora and semi-supervised clustering for topic tracking
we address the problem dealing with skewed data , and propose a method for estimating effective training stories for the topic tracking task . for a small number of labelled positive stories , we extract story pairs which consist of positive and its associated stories from bilingual comparable corpora . to overcome the problem of a large number of labelled negative stories , we classify them into some clusters . this is done by using k-means with em . the results on the tdt corpora show the effectiveness of the method .

umcc_dlsi- ( eps ) : paraphrases detection based on semantic
this paper describes the specifications and results of umcc_dlsi- ( eps ) system , which participated in the first evaluating phrasal semantics of semeval-2013 . our supervised system uses different kinds of semantic features to train a bagging classifier used to select the correct similarity option . related to the different features we can highlight the resource wordnet used to extract semantic relations among words and the use of different algorithms to establish semantic similarities . our system obtains promising results with a precision value around 78 % for the english corpus and 71.84 % for the italian corpus .

the role of lexical resources in cjk natural language processing
the role of lexical resources is often understated in nlp research . the complexity of chinese , japanese and korean ( cjk ) poses special challenges to developers of nlp tools , especially in the area of word segmentation ( ws ) , information retrieval ( ir ) , named entity extraction ( ner ) , and machine translation ( mt ) . these difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography , especially in japanese . this paper summarizes some of the major linguistic issues in the development nlp applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of nlp tools .

a structured vector space model for word meaning in context
we address the task of computing vector space representations for the meaning of word occurrences , which can vary widely according to context . this task is a crucial step towards a robust , vector-based compositional account of sentence meaning . we argue that existing models for this task do not take syntactic structure sufficiently into account . we present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words argument positions . this makes it possible to integrate syntax into the computation of word meaning in context . in addition , the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases .

docent : a document-level decoder for phrase-based statistical machine translation
we describe docent , an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units . by taking translation to the document level , our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware smt models .

identifying collocations to measure compositionality : shared task system description
this paper describes three systems from the university of minnesota , duluth that participated in the disco 2011 shared task that evaluated distributional methods of measuring semantic compositionality . all three systems approached this as a problem of collocation identification , where strong collocates are assumed to be minimally compositional . duluth1 relies on the t-score , whereas duluth-2 and duluth-3 rely on pointwise mutual information ( pmi ) . duluth-1 was the top ranked system overall in coarsegrained scoring , which was a 3-way category assignment where pairs were assigned values of high , medium , or low compositionality .

integrating multiple dependency corpora for inducing wide-coverage japanese ccg resources
this paper describes a method of inducing wide-coverage ccg resources for japanese . while deep parsers with corpusinduced grammars have been emerging for some languages , those for japanese have not been widely studied , mainly because most japanese syntactic resources are dependency-based . our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into ccg derivations . the method is empirically evaluated in terms of the coverage of the obtained lexicon and the accuracy of parsing .

a study on automatically extracted keywords in text categorization
this paper presents a study on if and how automatically extracted keywords can be used to improve text categorization . in summary we show that a higher performance as measured by micro-averaged f-measure on a standard text categorization collection is achieved when the full-text representation is combined with the automatically extracted keywords . the combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords . we also present results for experiments in which the keywords are the only input to the categorizer , either represented as unigrams or intact . of these two experiments , the unigrams have the best performance , although neither performs as well as headlines only .

building a large chinese corpus annotated with semantic dependency
at present most of corpora are annotated mainly with syntactic knowledge . in this paper , we attempt to build a large corpus and annotate semantic knowledge with dependency grammar . we believe that words are the basic units of semantics , and the structure and meaning of a sentence consist mainly of a series of semantic dependencies between individual words . a 1,000,000-wordscale corpus annotated with semantic dependency has been built . compared with syntactic knowledge , semantic knowledge is more difficult to annotate , for ambiguity problem is more serious . in the paper , the strategy to improve consistency is addressed , and congruence is defined to measure the consistency of tagged corpus.. finally , we will compare our corpus with other well-known corpora .

proactive learning for building machine translation systems for minority
building machine translation ( mt ) for many minority languages in the world is a serious challenge . for many minor languages there is little machine readable text , few knowledgeable linguists , and little money available for mt development . for these reasons , it becomes very important for an mt system to make best use of its resources , both labeled and unlabeled , in building a quality system . in this paper we argue that traditional active learning setup may not be the right fit for seeking annotations required for building a syntax based mt system for minority languages . we posit that a relatively new variant of active learning , proactive learning , is more suitable for this task .

alternative phrases and natural language information division of informatics
this paper presents a formal analysis for a large class of words called alternative markers , which includes other ( than ) , such ( as ) , and besides . these words appear frequently enough in dialog to warrant serious attention , yet present natural language search engines perform poorly on queries containing them . i show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engines operational semantics . the value of this approach is that as the operational semantics of natural language applications improve , even larger improvements are possible .

parsing with soft and hard constraints on dependency length
in lexicalized phrase-structure or dependency parses , a words modifiers tend to fall near it in the string . we show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in english and chinese , with more mixed results on german . we then show similar improvements by imposing hard bounds on dependency length and ( additionally ) modeling the resulting sequence of parse fragments . this simple vine grammar formalism has only finite-state power , but a context-free parameterization with some extra parameters for stringing fragments together . we exhibit a linear-time chart parsing algorithm with a low grammar constant .

metadata-aware measures for answer summarization in community question answering
this paper presents a framework for automatically processing information coming from community question answering ( cqa ) portals with the purpose of generating a trustful , complete , relevant and succinct summary in response to a question . we exploit the metadata intrinsically present in user generated content ( ugc ) to bias automatic multi-document summarization techniques toward high quality information . we adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap . experimental results on data drawn from yahoo ! answers demonstrate the effectiveness of our method in terms of rouge scores . we show that the information contained in the best answers voted by users of cqa portals can be successfully complemented by our method .

deriving an ambiguous words part-of-speech distribution from unannotated text
a distributional method for part-of-speech induction is presented which , in contrast to most previous work , determines the part-of-speech distribution of syntactically ambiguous words without explicitly tagging the underlying text corpus . this is achieved by assuming that the word pair consisting of the left and right neighbor of a particular token is characteristic of the part of speech at this position , and by clustering the neighbor pairs on the basis of their middle words as observed in a large corpus . the results obtained in this way are evaluated by comparing them to the part-of-speech distributions as found in the manually tagged brown corpus .

measuring semantic relatedness with vector space models and random
both vector space models and graph randomwalk models can be used to determine similarity between concepts . noting that vectors can be regarded as local views of a graph , we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings , concept categorization , and semantic priming , varying the size of the dataset from which vector space and graph are extracted .

unsupervised coreference resolution by utilizing the most informative relations
in this paper we present a novel method for unsupervised coreference resolution . we introduce a precision-oriented inference method that scores a candidate entity of a mention based on the most informative mention pair relation between the given mention entity pair . we introduce an informativeness score for determining the most precise relation of a mention entity pair regarding the coreference decisions . the informativeness score is learned robustly during few iterations of the expectation maximization algorithm . the proposed unsupervised system outperforms existing unsupervised methods on all benchmark data sets .

fully parsing the penn treebank ryan gabbard mitchell marcus
we present a two stage parser that recovers penn treebank style syntactic analyses of new sentences including skeletal syntactic structure , and , for the first time , both function tags and empty categories . the accuracy of the first-stage parser on the standard parseval metric matches that of the ( collins , 2003 ) parser on which it is based , despite the data fragmentation caused by the greatly enriched space of possible node labels . this first stage simultaneously achieves near state-of-theart performance on recovering function tags with minimal modifications to the underlying parser , modifying less than ten lines of code . the second stage achieves state-of-the-art performance on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods .

a multi-resolution framework for information extraction from free text
extraction of relations between entities is an important part of information extraction on free text . previous methods are mostly based on statistical correlation and dependency relations between entities . this paper re-examines the problem at the multiresolution layers of phrase , clause and sentence using dependency and discourse relations . our multi-resolution framework are ( anchor and relation ) uses clausal relations in 2 ways : 1 ) to filter noisy dependency paths ; and 2 ) to increase reliability of dependency path extraction . the resulting system outperforms the previous approaches by 3 % , 7 % , 4 % on muc4 , muc6 and ace rdc domains respectively .

multi-document summarization of evaluative text deptartment of computer science
we present and compare two approaches to the task of summarizing evaluative arguments . the first is a sentence extractionbased approach while the second is a language generation-based approach . we evaluate these approaches in a user study and find that they quantitatively perform equally well . qualitatively , however , we find that they perform well for different but complementary reasons . we conclude that an effective method for summarizing evaluative arguments must effectively synthesize the two approaches .

non-monotonic parsing of fluent umm i mean disfluent sentences
parsing disfluent sentences is a challenging task which involves detecting disfluencies as well as identifying the syntactic structure of the sentence . while there have been several studies recently into solely detecting disfluencies at a high performance level , there has been relatively little work into joint parsing and disfluency detection that has reached that state-ofthe-art performance in disfluency detection . we improve upon recent work in this joint task through the use of novel features and learning cascades to produce a model which performs at 82.6 f-score . it outperforms the previous best in disfluency detection on two different evaluations .

lexflow : a system for cross-fertilization of computational lexicons
this demo presents lexflow , a workflow management system for crossfertilization of computational lexicons . borrowing from techniques used in the domain of document workflows , we model the activity of lexicon management as a set of workflow types , where lexical entries move across agents in the process of being dynamically updated . a prototype of lexflow has been implemented with extensive use of xml technologies ( xslt , xpath , xforms , svg ) and open-source tools ( cocoon , tomcat , mysql ) . lexflow is a web-based application that enables the cooperative and distributed management of computational lexicons .

a chinese word segmentation system based on cascade model
this paper introduces the system of word segmentation and analyzes its evaluation results in the fourth sighan bakeoff1 . a novel method has been used in the system , which main idea is : firstly , the main problems of ws have been classified , and then a cascaded model has been used to gradually optimize the system . the core of this ws system is the segmentation of ambiguous words and the internal information extraction of unknown words . the experiments show that the performance is satisfying , with the riv-measure 96.8 % in ncc open test in the sighan bakeoff 2007 .

ucf-ws : domain word sense disambiguation using web selectors
this paper studies the application of the web selectors word sense disambiguation system on a specific domain . the system was primarily applied without any domain tuning , but the incorporation of domain predominant sense information was explored . results indicated that the system performs relatively the same with domain predominant sense information as without , scoring well above a random baseline , but still 5 percentage points below results of using the first sense .

issues in translating verb-particle constructions from german to english
in this paper , we investigate difficulties in translating verb-particle constructions from german to english . we analyse the structure of german vpcs and compare them to vpcs in english . in order to find out if and to what degree the presence of vpcs causes problems for statistical machine translation systems , we collected a set of 59 verb pairs , each consisting of a german vpc and a synonymous simplex verb . with this data , we constructed a test suite of 236 sentences where the simplex verb and vpc are completely substitutable . we then translated this dataset to english using google translate and bing translator . through an analysis of the resulting translations we are able to show that the quality decreases when translating sentences that contain vpcs instead of simplex verbs . the test suite is made freely available to the community .

high-performance tagging on medical texts udo hahn joachim wermter
we ran both brills rule-based tagger and tnt , a statistical tagger , with a default german newspaper-language model on a medical text corpus . supplied with limited lexicon resources , tnt outperforms the brill tagger with state-of-the-art performance figures ( close to 97 % accuracy ) . we then trained tnt on a large annotated medical text corpus , with a slightly extended tagset that captures certain medical language particularities , and achieved 98 % tagging accuracy . hence , statistical off-the-shelf pos taggers can not only be immediately reused for medical nlp , but they also when trained on medical corpora achieve a higher performance level than for the newspaper genre .

the integration of dependency relation classification and semantic role
this paper describes a system to solve the joint learning of syntactic and semantic dependencies . an directed graphical model is put forward to integrate dependency relation classification and semantic role labeling . we present a bilayer directed graph to express probabilistic relationships between syntactic and semantic relations . maximum entropy markov models are implemented to estimate conditional probability distribution and to do inference . the submitted model yields 76.28 % macro-average f1 performance , for the joint task , 85.75 % syntactic dependencies las and 66.61 % semantic dependencies f1 .

evaluating an off-the-shelf pos-tagger on early modern german text
the goal of this study is to evaluate an offthe-shelf pos-tagger for modern german on historical data from the early modern period ( 1650-1800 ) . with no specialised tagger available for this particular stage of the language , our findings will be of particular interest to smaller , humanities-based projects wishing to add pos annotations to their historical data but which lack the means or resources to train a pos tagger themselves . our study assesses the effects of spelling variation on the performance of the tagger , and investigates to what extent tagger performance can be improved by using normalised input , where spelling variants in the corpus are standardised to a modern form . our findings show that adding such a normalisation layer improves tagger performance considerably .

correlation between rouge and human evaluation of extractive meeting
automatic summarization evaluation is critical to the development of summarization systems . while rouge has been shown to correlate well with human evaluation for content match in text summarization , there are many characteristics in multiparty meeting domain , which may pose potential problems to rouge . in this paper , we carefully examine how well the rouge scores correlate with human evaluation for extractive meeting summarization . our experiments show that generally the correlation is rather low , but a significantly better correlation can be obtained by accounting for several unique meeting characteristics , such as disfluencies and speaker information , especially when evaluating system-generated summaries .

a cascaded machine learning approach to interpreting temporal expressions
a new architecture for identifying and interpreting temporal expressions is introduced , in which the large set of complex hand-crafted rules standard in systems for this task is replaced by a series of machine learned classifiers and a much smaller set of context-independent semantic composition rules . experiments with the tern 2004 data set demonstrate that overall system performance is comparable to the state-of-the-art , and that normalization performance is particularly good .

automatic clustering of collocation for detecting practical sense boundary
this paper talks about the deciding practical sense boundary of homonymous words . the important problem in dictionaries or thesauri is the confusion of the sense boundary by each resource . this also becomes a bottleneck in the practical language processing systems . this paper proposes the method about discovering sense boundary using the collocation from the large corpora and the clustering methods . in the experiments , the proposed methods show the similar results with the sense boundary from a corpus-based dictionary and sense-tagged corpus .

safe in-vehicle dialogue using learned predictions of user utterances
we present a multimodal in-vehicle dialogue system which uses learned predictions of user answers to enable shorter , more efficient , and thus safer natural language dialogues .

improving word alignment with language model based confidence scores
this paper describes the statistical machine translation systems submitted to the acl-wmt 2008 shared translation task . systems were submitted for two translation directions : englishspanish and spanishenglish . using sentence pair confidence scores estimated with source and target language models , improvements are observed on the newscommentary test sets . genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated .

a syntactic n-gram language model from a big corpora
we describe our approach to grammatical error correction presented in the conll shared task 2014. our work is focused on error detection in sentences with a language model based on syntactic tri-grams and bi-grams extracted from dependency trees generated from 90 % of the english wikipedia . also , we add a nave module to error correction that outputs a set of possible answers , those sentences are scored using a syntactic n-gram language model . the sentence with the best score is the final suggestion of the system . the system was ranked 11th , evidently this is a very simple approach , but since the beginning our main goal was to test the syntactic n-gram language model with a big corpus to future comparison .

pre- and postprocessing for statistical machine translation into germanic
in this thesis proposal i present my thesis work , about pre- and postprocessing for statistical machine translation , mainly into germanic languages . i focus my work on four areas : compounding , definite noun phrases , reordering , and error correction . initial results are positive within all four areas , and there are promising possibilities for extending these approaches . in addition i also focus on methods for performing thorough error analysis of machine translation output , which can both motivate and evaluate the studies performed .

a cascaded linear model for joint chinese word segmentation
we propose a cascaded linear model for joint chinese word segmentation and partof-speech tagging . with a character-based perceptron as the core , combined with realvalued features such as language models , the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly . experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging . on the penn chinese treebank 5.0 , we obtain an error reduction of 18.5 % on segmentation and 12 % on joint segmentation and part-of-speech tagging over the perceptron-only baseline .

determining compositionality of word expressions using word space models
this research focuses on determining semantic compositionality of word expressions using word space models ( wsms ) . we discuss previous works employing wsms and present differences in the proposed approaches which include types of wsms , corpora , preprocessing techniques , methods for determining compositionality , and evaluation testbeds . we also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components . the vectors were obtained by latent semantic analysis ( lsa ) applied to the ukwac corpus . our results outperform those of all the participants in the distributional semantics and compositionality ( disco ) 2011 shared task .

developing german semantics on the basis of parallel lfg
this paper reports on the development of a core semantics for german which was implemented on the basis of an english semantics that converts lfg f-structures to flat meaning representations in a neo-davidsonian style . thanks to the parallel design of the broad-coverage lfg grammars written in the context of the pargram project ( butt et al , 2002 ) and the general surface independence of lfg f-structure analyses , the development process was substantially facilitated . we also discuss the overall architecture of the semantic conversion system from a crosslinguistic , theoretical perspective .

palinka : a highly customisable tool for discourse annotation
annotation of discourse phenomena is a notoriously difficult task which can not be carried out without the help of annotation tools . in this paper we present a perspicuous and adjustable links annotator ( palinka ) , a tool successfully used in several of our projects . we also briefly describe three types of discourse annotations applied using the tool .

weakly supervised learning for hedge classification in scientific literature
we investigate automatic classification of speculative language ( hedging ) , in biomedical text using weakly supervised machine learning . our contributions include a precise description of the task with annotation guidelines , analysis and discussion , a probabilistic weakly supervised learning model , and experimental evaluation of the methods presented . we show that hedge classification is feasible using weakly supervised ml , and point toward avenues for future research .

relaxed marginal inference and its application to dependency parsing
recently , relaxation approaches have been successfully used for map inference on nlp problems . in this work we show how to extend the relaxation approach to marginal inference used in conditional likelihood training , posterior decoding , confidence estimation , and other tasks . we evaluate our approach for the case of second-order dependency parsing and observe a tenfold increase in parsing speed , with no loss in accuracy , by performing inference over a small subset of the full factor graph . we also contribute a bound on the error of the marginal probabilities by a sub-graph with respect to the full graph . finally , while only evaluated with bp in this paper , our approach is general enough to be applied with any marginal inference method in the inner loop .

mining context specific similarity relationships using the world wide
we have studied how context specific web corpus can be automatically created and mined for discovering semantic similarity relationships between terms ( words or phrases ) from a given collection of documents ( target collection ) . these relationships between terms can be used to adjust the standard vectors space representation so as to improve the accuracy of similarity computation between text documents in the target collection . our experiments with a standard test collection ( reuters ) have revealed the reduction of similarity errors by up to 50 % , twice as much as the improvement by using other known techniques .

a tabular method for dynamic oracles in transition-based parsing
we develop parsing oracles for two transition-based dependency parsers , including the arc-standard parser , solving a problem that was left open in ( goldberg and nivre , 2013 ) . we experimentally show that using these oracles during training yields superior parsing accuracies on many languages .

compiling a massive , multilingual dictionary via probabilistic inference
can we automatically compose a large set of wiktionaries and translation dictionaries to yield a massive , multilingual dictionary whose coverage is substantially greater than that of any of its constituent dictionaries the composition of multiple translation dictionaries leads to a transitive inference problem : if word a translates to word b which in turn translates to word c , what is the probability that c is a translation of a the paper introduces a novel algorithm that solves this problem for 10,000,000 words in more than 1,000 languages . the algorithm yields pandictionary , a novel multilingual dictionary . pandictionary contains more than four times as many translations than in the largest wiktionary at precision 0.90 and over 200,000,000 pairwise translations in over 200,000 language pairs at precision 0.8 .

multilingual term extraction from domain-specific corpora using morphological structure
morphologically complex terms composed from greek or latin elements are frequent in scientific and technical texts . word forming units are thus relevant cues for the identification of terms in domainspecific texts . this article describes a method for the automatic extraction of terms relying on the detection of classical prefixes and word-initial combining forms . word-forming units are identified using a regular expression . the system then extracts terms by selecting words which either begin or coalesce with these elements . next , terms are grouped in families which are displayed as a weighted list in html format .

analysis of asl motion capture data towards identification of
this paper provides a preliminary analysis of american sign language predicate motion signatures , obtained using a motion capture system , toward identification of a predicates event structure as telic or atelic . the pilot data demonstrates that production differences between signed predicates can be used to model the probabilities of a predicate belonging to telic or atelic classes based on their motion signature in 3d , using either maximal velocity achieved within the sign , or maximal velocity and minimal acceleration data from each predicate . the solution to the problem of computationally identifying predicate types in asl video data could significantly simplify the task of identifying verbal complements , arguments and modifiers , which compose the rest of the sentence , and ultimately contribute to solving the problem of automatic asl recognition .

a hybrid approach to word segmentation and pos tagging
in this paper , we present a hybrid method for word segmentation and pos tagging . the target languages are those in which word boundaries are ambiguous , such as chinese and japanese . in the method , word-based and character-based processing is combined , and word segmentation and pos tagging are conducted simultaneously . experimental results on multiple corpora show that the integrated method has high accuracy .

reestimation of reified rules in semiring parsing and biparsing
we show that reifying the rules from hyperedge weights to first-class graph nodes automatically gives us rule expectations in any kind of grammar expressible as a deductive system , without any explicit algorithm for calculating rule expectations ( such as the insideoutside algorithm ) . this gives us expectation maximization training for any grammar class with a parsing algorithm that can be stated as a deductive system , for free . having such a framework in place accelerates turnover time for experimenting with new grammar classes and parsing algorithmsto implement a grammar learner , only the parse forest construction has to be implemented .

a statistical semantic parser that integrates syntax and semantics
we introduce a learning semantic parser , scissor , that maps natural-language sentences to a detailed , formal , meaningrepresentation language . it first uses an integrated statistical parser to produce a semantically augmented parse tree , in which each non-terminal node has both a syntactic and a semantic label . a compositional-semantics procedure is then used to map the augmented parse tree into a final meaning representation . we evaluate the system in two domains , a natural-language database interface and an interpreter for coaching instructions in robotic soccer . we present experimental results demonstrating that scissor produces more accurate semantic representations than several previous approaches .

exact maximum inference for the fertility hidden markov model
the notion of fertility in word alignment ( the number of words emitted by a single state ) is useful but difficult to model . initial attempts at modeling fertility used heuristic search methods . recent approaches instead use more principled approximate inference techniques such as gibbs sampling for parameter estimation . yet in practice we also need the single best alignment , which is difficult to find using gibbs . building on recent advances in dual decomposition , this paper introduces an exact algorithm for finding the single best alignment with a fertility hmm . finding the best alignment appears important , as this model leads to a substantial improvement in alignment quality .

combining context features by canonical belief network for chinese
part-of-speech ( pos ) tagging is the essential basis of natural language processing ( nlp ) . in this paper , we present an algorithm that combines a variety of context features , e.g . the pos tags of the words next to the word a that needs to be tagged and the context lexical information of a by canonical belief network to together determine the pos tag of a. experiments on a chinese corpus are conducted to compare our algorithm with the standard hmm-based pos tagging and the pos tagging software ictclas3.0 . the experimental results show that our algorithm is more effective .

viterbi based alignment between text images and their transcripts
an alignment method based on the viterbi algorithm is proposed to find mappings between word images of a given handwritten document and their respective ( ascii ) words on its transcription . the approach takes advantage of the underlying segmentation made by viterbi decoding in handwritten text recognition based on hidden markov models ( hmms ) . two hmms modelling schemes are evaluated : one using 78-hmms ( one hmm per character class ) and other using a unique hmm to model all the characters and another to model blank spaces . according to various metrics used to measure the quality of the alignments , encouraging results are obtained .

heuristic search in a cognitive model of human parsing
we present a cognitive process model of human sentence comprehension based on generalized left-corner parsing . a search heuristic based upon previouslyparsed corpora derives garden path effects , garden path paradoxes , and the local coherence effect .

towards a unified approach for opinion question answering and
the aim of this paper is to present an approach to tackle the task of opinion question answering and text summarization . following the guidelines tac 2008 opinion summarization pilot task , we propose new methods for each of the major components of the process . in particular , for the information retrieval , opinion mining and summarization stages . the performance obtained improves with respect to the state of the art by approximately 12.50 % , thus concluding that the suggested approaches for these three components are adequate .

score distribution based term specific thresholding spoken term detection
the spoken term detection ( std ) task aims to return relevant segments from a spoken archive that contain the query terms . this paper focuses on the decision stage of an std system . we propose a term specific thresholding ( tst ) method that uses per query posterior score distributions . the std system described in this paper indexes word-level lattices produced by an lvcsr system using weighted finite state transducers ( wfsts ) . the target application is a sign dictionary where precision is more important than recall . experiments compare the performance of different thresholding techniques . the proposed approach increases the maximum precision attainable by the system .

punctuation processing for projective dependency parsing and jingbo zhu
modern statistical dependency parsers assign lexical heads to punctuations as well as words . punctuation parsing errors lead to low parsing accuracy on words . in this work , we propose an alternative approach to addressing punctuation in dependency parsing . rather than assigning lexical heads to punctuations , we treat punctuations as properties of their neighbouring words , used as features to guide the parser to build the dependency graph . integrating our method with an arc-standard parser yields a 93.06 % unlabelled attachment score , which is the best accuracy by a single-model transition-based parser reported so far .

bootstrapping semantic parsers from conversations computer science & engineering
conversations provide rich opportunities for interactive , continuous learning . when something goes wrong , a system can ask for clarification , rewording , or otherwise redirect the interaction to achieve its goals . in this paper , we present an approach for using conversational interactions of this type to induce semantic parsers . we demonstrate learning without any explicit annotation of the meanings of user utterances . instead , we model meaning with latent variables , and introduce a loss function to measure how well potential meanings match the conversation . this loss drives the overall learning approach , which induces a weighted ccg grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system . experiments on darpa communicator conversational logs demonstrate effective learning , despite requiring no explicit meaning annotations .

latent variable models of selectional preference diarmuid o seaghdha
this paper describes the application of so-called topic models to selectional preference induction . three models related to latent dirichlet allocation , a proven method for modelling document-word cooccurrences , are presented and evaluated on datasets of human plausibility judgements . compared to previously proposed techniques , these models perform very competitively , especially for infrequent predicate-argument combinations where they exceed the quality of web-scale predictions while using relatively little data .

the role of interactivity in human-machine conversation for automatic
motivated by the psycholinguistic finding that human eye gaze is tightly linked to speech production , previous work has applied naturally occurring eye gaze for automatic vocabulary acquisition . however , unlike in the typical settings for psycholinguistic studies , eye gaze can serve different functions in human-machine conversation . some gaze streams do not link to the content of the spoken utterances and thus can be potentially detrimental to word acquisition . to address this problem , this paper investigates the incorporation of interactivity in identifying the close coupling of speech and gaze streams for word acquisition . our empirical results indicate that automatic identification of closely coupled gaze-speech streams leads to significantly better word acquisition performance .

distributional semantic models for the evaluation of disordered language
atypical semantic and pragmatic expression is frequently reported in the language of children with autism . although this atypicality often manifests itself in the use of unusual or unexpected words and phrases , the rate of use of such unexpected words is rarely directly measured or quantified . in this paper , we use distributional semantic models to automatically identify unexpected words in narrative retellings by children with autism . the classification of unexpected words is sufficiently accurate to distinguish the retellings of children with autism from those with typical development . these techniques demonstrate the potential of applying automated language analysis techniques to clinically elicited language data for diagnostic purposes .

k-nn for local probability estimation in generative parsing models
we describe a history-based generative parsing model which uses a k-nearest neighbour ( k-nn ) technique to estimate the models parameters . taking the output of a base n-best parser we use our model to re-estimate the log probability of each parse tree in the n-best list for sentences from the penn wall street journal treebank . by further decomposing the local probability distributions of the base model , enriching the set of conditioning features used to estimate the models parameters , and using k-nn as opposed to the witten-bell estimation of the base model , we achieve an f-score of 89.2 % , representing a 4 % relative decrease in f-score error over the 1-best output of the base parser .

integrating logical representations with probabilistic information using markov logic
first-order logic provides a powerful and flexible mechanism for representing natural language semantics . however , it is an open question of how best to integrate it with uncertain , probabilistic knowledge , for example regarding word meaning . this paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of statistical relational ai . specifically , we show how discourse representation structures can be combined with distributional models for word meaning inside a markov logic network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context .

an annotation scheme for cross-cultural argumentation and persuasion dialogues
we present a novel annotation scheme for cross-cultural argumentation and persuasion dialogues . this scheme is an adaptation of existing coding schemes on negotiation , following a review of literature on cross-cultural differences in negotiation styles . the scheme has been refined through application to coding both two-party and multi-party negotiation dialogues in three different domains , and is general enough to be applicable to different domains with few if any extensions . dialogues annotated with the scheme have been used to successfully learn culture-specific dialogue policies for argumentation and persuasion .

usp-each frequency-based greedy attribute selection for referring expressions generation
both greedy and domain-oriented reg algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by , e.g. , dice scores . in this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the dale & reiter incremental algorithm in the reg challenge 2008 , and the results in both furniture and people domains .

classification of semantic relationships between nominals using pattern clusters
there are many possible different semantic relationships between nominals . classification of such relationships is an important and difficult task ( for example , the well known noun compound classification task is a special case of this problem ) . we propose a novel pattern clusters method for nominal relationship ( nr ) classification . pattern clusters are discovered in a large corpus independently of any particular training set , in an unsupervised manner . each of the extracted clusters corresponds to some unspecified semantic relationship . the pattern clusters are then used to construct features for training and classification of specific inter-nominal relationships . our nr classification evaluation strictly follows the acl semeval-07 task 4 datasets and protocol , obtaining an f-score of 70.6 , as opposed to 64.8 of the best previous work that did not use the manually provided wordnet sense disambiguation tags .

k-qard : a practical korean question answering framework for
we present a korean question answering framework for restricted domains , called k-qard . k-qard is developed to achieve domain portability and robustness , and the framework is successfully applied to build question answering systems for several domains .

self- or pre-tuning deep linguistic processing of language variants
this paper proposes a design strategy for deep language processing grammars to appropriately handle language variants . it allows a grammar to be restricted as to what language variant it is tuned to , but also to detect the variant a given input pertains to . this is evaluated and compared to results obtained with an alternative strategy by which the relevant variant is detected with current language identification methods in a preprocessing step .

a semi-supervised word alignment algorithm with partial manual alignments
we present a word alignment framework that can incorporate partial manual alignments . the core of the approach is a novel semi-supervised algorithm extending the widely used ibm models with a constrained em algorithm . the partial manual alignments can be obtained by human labelling or automatically by high-precision-low-recall heuristics . we demonstrate the usages of both methods by selecting alignment links from manually aligned corpus and apply links generated from bilingual dictionary on unlabelled data . for the first method , we conduct controlled experiments on chineseenglish and arabic-english translation tasks to compare the quality of word alignment , and to measure effects of two different methods in selecting alignment links from manually aligned corpus . for the second method , we experimented with moderate-scale chinese-english translation task . the experiment results show an average improvement of 0.33 bleu point across 8 test sets .

unifying synchronous tree-adjoining grammars and tree transducers via bimorphisms
we place synchronous tree-adjoining grammars and tree transducers in the single overarching framework of bimorphisms , continuing the unification of synchronous grammars and tree transducers initiated by shieber ( 2004 ) . along the way , we present a new definition of the tree-adjoining grammar derivation relation based on a novel direct inter-reduction of tag and monadic macro tree transducers .

coordinate noun phrase disambiguation in a generative parsing model
in this paper we present methods for improving the disambiguation of noun phrase ( np ) coordination within the framework of a lexicalised history-based parsing model . as well as reducing noise in the data , we look at modelling two main sources of information for disambiguation : symmetry in conjunct structure , and the dependency between conjunct lexical heads . our changes to the baseline model result in an increase in np coordination dependency f-score from 69.9 % to 73.8 % , which represents a relative reduction in f-score error of 13 % .

the utility of parse-derived features for automatic discourse segmentation
we investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach , including features derived from either finite-state or contextfree annotations . we achieve the best reported performance on this task , and demonstrate that our spade-inspired context-free features are critical to achieving this level of accuracy . this counters recent results suggesting that purely finite-state approaches can perform competitively .

a clustering approach for the nearly unsupervised recognition of
in this paper we present trofi ( trope finder ) , a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques . trofi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies . it also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning . we adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners , a voting schema , and additional features like supertags and extrasentential context . detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4 % . using the trofi algorithm , we also build the trofi example base , an extensible resource of annotated literal/nonliteral examples which is freely available to the nlp research community .

reading to learn : constructing features from semantic abstracts
jmachine learning offers a range of tools for training systems from data , but these methods are only as good as the underlying representation . this paper proposes to acquire representations for machine learning by reading text written to accommodate human learning . we propose a novel form of semantic analysis called reading to learn , where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning . we obtain this abstract through a generative model that requires no labeled data , instead leveraging repetition across multiple documents . the semantic abstract is converted into a transformed feature space for learning , resulting in improved generalization on a relational learning task .

can markov models over minimal translation units help phrase-based
the phrase-based and n-gram-based smt frameworks complement each other . while the former is better able to memorize , the latter provides a more principled model that captures dependencies across phrasal boundaries . some work has been done to combine insights from these two frameworks . a recent successful attempt showed the advantage of using phrasebased search on top of an n-gram-based model . we probe this question in the reverse direction by investigating whether integrating n-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption . a large scale evaluation over 8 language pairs shows that performance does significantly improve .

learning to re-rank for interactive problem resolution and query
we study the design of an information retrieval ( ir ) system that assists customer service agents while they interact with end-users . the type of ir needed is difficult because of the large lexical gap between problems as described by customers , and solutions . we describe an approach that bridges this lexical gap by learning semantic relatedness using tensor representations . queries that are short and vague , which are common in practice , result in a large number of documents being retrieved , and a high cognitive load for customer service agents . we show how to reduce this burden by providing suggestions that are selected based on the learned measures of semantic relatedness . experiments show that the approach offers substantial benefit compared to the use of standard lexical similarity .

( re ) ranking meets morphosyntax : state-of-the-art results
this paper describes the ims-szeged-cis contribution to the spmrl 2013 shared task . we participate in both the constituency and dependency tracks , and achieve state-of-theart for all languages . for both tracks we make significant improvements through high quality preprocessing and ( re ) ranking on top of strong baselines . our system came out first for both tracks .

gpkex : genetically programmed keyphrase extraction from croatian texts
we describe gpkex , a keyphrase extraction method based on genetic programming . we represent keyphrase scoring measures as syntax trees and evolve them to produce rankings for keyphrase candidates extracted from text . we apply and evaluate gpkex on croatian newspaper articles . we show that gpkex can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for croatian .

factorizing complex models : a case study in mention
as natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features . one such example , explored in this article , is the mention detection and recognition task in the automatic content extraction project , with the goal of identifying named , nominal or pronominal references to real-world entitiesmentions and labeling them with three types of information : entity type , entity subtype and mention type . in this article , we investigate three methods of assigning these related tags and compare them on several data sets . a system based on the methods presented in this article participated and ranked very competitively in the ace04 evaluation .

multimodal menu-based dialogue with speech cursor in dico ii+
this paper describes dico ii+ , an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a speech cursor which enables menu navigation as well as browsing long list using haptic input and spoken output .

sort : an interactive source-rewriting tool for improved translation
the quality of automatic translation is affected by many factors . one is the divergence between the specific source and target languages . another lies in the source text itself , as some texts are more complex than others . one way to handle such texts is to modify them prior to translation . yet , an important factor that is often overlooked is the source translatability with respect to the specific translation system and the specific model that are being used . in this paper we present an interactive system where source modifications are induced by confidence estimates that are derived from the translation model in use . modifications are automatically generated and proposed for the users approval . such a system can reduce postediting effort , replacing it by cost-effective pre-editing that can be done by monolinguals .

the effects of human variation in duc summarization evaluation
there is a long history of research in automatic text summarization systems by both the text retrieval and the natural language processing communities , but evaluation of such systems output has always presented problems . one critical problem remains how to handle the unavoidable variability in human judgments at the core of all the evaluations . sponsored by the darpa tides project , nist launched a new text summarization evaluation effort , called duc , in 2001 with follow-on workshops in 2002 and 2003. human judgments provided the foundation for all three evaluations and this paper examines how the variation in those judgments does and does not affect the results and their interpretation .

a conceptual framework for inferring implicatures intelligent systems program
while previous sentiment analysis research has concentrated on the interpretation of explicitly stated opinions and attitudes , this work addresses a type of opinion implicature ( i.e. , opinion-oriented default inference ) in real-world text . this work describes a rule-based conceptual framework for representing and analyzing opinion implicatures . in the course of understanding implicatures , the system recognizes implicit sentiments ( and beliefs ) toward various events and entities in the sentence , often of mixed polarities ; thus , it produces a richer interpretation than is typical in opinion analysis .

comparison of similarity models for the relation discovery task
we present results on the relation discovery task , which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods . we describe a detailed experimental design that compares various configurations of conceptual representations and similarity measures across six different subsets of the ace relation extraction data . previous work on relation discovery used a semantic space based on a term-bydocument matrix . we find that representations based on term co-occurrence perform significantly better . we also observe further improvements when reducing the dimensionality of the term co-occurrence matrix using probabilistic topic models , though these are not significant .

biocom usp : tweet sentiment analysis with adaptive boosting
we describe our approach for the semeval-2014 task 9 : sentiment analysis in twitter . we make use of an ensemble learning method for sentiment classification of tweets that relies on varied features such as feature hashing , part-of-speech , and lexical features . our system was evaluated in the twitter message-level task .

machine translation of medical texts in the khresmoi project
this paper presents the participation of the charles university team in the wmt 2014 medical translation task . our systems are developed within the khresmoi project , a large integrated project aiming to deliver a multi-lingual multi-modal search and access system for biomedical information and documents . being involved in the organization of the medical translation task , our primary goal is to set up a baseline for both its subtasks ( summary translation and query translation ) and for all translation directions . our systems are based on the phrasebased moses system and standard methods for domain adaptation . the constrained/unconstrained systems differ in the training data only .

modeling consensus : classifier combination for word sense disambiguation
this paper demonstrates the substantial empirical success of classifier combination for the word sense disambiguation task . it investigates more than 10 classifier combination methods , including second order classifier stacking , over 6 major structurally different base classifiers ( enhanced nave bayes , cosine , bayes ratio , decision lists , transformationbased learning and maximum variance boosted mixture models ) . the paper also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers . when evaluated on the standard senseval1 and 2 data sets on 4 languages ( english , spanish , basque , and swedish ) , classifier combination performance exceeds the best published results on these data sets .

deep processing of honorification phenomena in a typed feature
honorific agreement is one of the main properties of languages like korean or japanese , playing an important role in appropriate communication . this makes the deep processing of honorific information crucial in various computational applications such as spoken language translation and generation . we argue that , contrary to the previous literature , an adequate analysis of korean honorification involves a system that has access not only to morphosyntax but to semantics and pragmatics as well . along these lines , we have developed a typed feature structure grammar of korean ( based on the framework of hpsg ) , and implemented it in the linguistic knowledge builder system ( lkb ) . the results of parsing our experimental test suites show that our grammar provides us with enriched grammatical information that can lead to the development of a robust dialogue system for the language .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

teaching applied natural language processing : triumphs and tribulations
in fall 2004 i introduced a new course called applied natural language processing , in which students acquire an understanding of which text analysis techniques are currently feasible for practical applications . the class was intended for interdisciplinary students with a somewhat technical background . this paper describes the topics covered and the programming exercises , emphasizing which aspects were successful and which problematic , and makes recommendations for future versions of the course .

opensonar : user-driven development of the sonar corpus interfaces
opensonar is an online system that allows for analyzing and searching the large scale dutch reference corpus sonar . due to the size of the corpus , accessing the information contained in the dataset has proven to be difficult for less technically inclined researchers . the opensonar project aims to facilitate the use of the sonar corpus by providing a user-friendly online interface . to make sure that the resulting system is practically useful , several user groups have been identified , who drive the interface development process by providing practical use cases . the current system is already used in educational and research settings .

tree revision learning for dependency parsing dipartimento di informatica
we present a revision learning model for improving the accuracy of a dependency parser . the revision stage corrects the output of the base parser by means of revision rules learned from the mistakes of the base parser itself . revision learning is performed with a discriminative classifier . the revision stage has linear complexity and preserves the efficiency of the base parser . we present empirical evaluations on the treebanks of two languages , which show effectiveness in relative error reduction and state of the art accuracy .

using soundex codes for indexing names in asr documents
in this paper we highlight the problems that arise due to variations of spellings of names that occur in text , as a result of which links between two pieces of text where the same name is spelt differently may be missed . the problem is particularly pronounced in the case of asr text . we propose the use of approximate string matching techniques to normalize names in order to overcome the problem . we show how we could achieve an improvement if we could tag names with reasonable accuracy in asr .

extending a broad-coverage parser for a general nlp toolkit
with the rapid growth of real world applications for nlp systems , there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific nlp systems . such a toolkit should have a parser that is general enough to be used across domains , and yet accurate enough for each specific application . in this paper , we describe a parser that extends a broad-coverage parser , minipar ( lin , 2001 ) , with an adaptable shallow parser so as to achieve both generality and accuracy in handling domain specific nl problems . we test this parser on our corpus and the results show that the accuracy is significantly higher than a system that uses minipar alone .

classifying ellipsis in dialogue : a machine learning approach
this paper presents a machine learning approach to bare sluice disambiguation in dialogue . we extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic horn clauses . we then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : slipper , a rule-based learning algorithm , and timbl , a memory-based system . both learners perform well , yielding similar success rates of approx 90 % . the results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our horn clauses can be learnt automatically from these features .

transfer learning , feature selection and word sense disambguation
we propose a novel approach for improving feature selection for word sense disambiguation by incorporating a feature relevance prior for each word indicating which features are more likely to be selected . we use transfer of knowledge from similar words to learn this prior over the features , which permits us to learn higher accuracy models , particularly for the rarer word senses . results on the ontonotes verb data show significant improvement over the baseline feature selection algorithm and results that are comparable to or better than other state-of-the-art methods .

a graph approach to spelling correction in domain-centric search
spelling correction for keyword-search queries is challenging in restricted domains such as personal email ( or desktop ) search , due to the scarcity of query logs , and due to the specialized nature of the domain . for that task , this paper presents an algorithm that is based on statistics from the corpus data ( rather than the query log ) . this algorithm , which employs a simple graph-based approach , can incorporate different types of data sources with different levels of reliability ( e.g. , email subject vs. email body ) , and can handle complex spelling errors like splitting and merging of words . an experimental study shows the superiority of the algorithm over existing alternatives in the email domain .

a unified local and global model for discourse coherence
we present a model for discourse coherence which combines the local entitybased approach and the hmm-based content model . unlike the mixture model of , we learn local and global features jointly , providing a better theoretical explanation of how they are useful . as the local component of our model we adapt by relaxing independence assumptions so that it is effective when estimated generatively . our model performs the ordering task competitively with , and significantly better than either of the models it is based on .

predicting attrition along the way : the uiuc model
discussion forum and clickstream are two primary data streams that enable mining of student behavior in a massively open online course . a students participation in the discussion forum gives direct access to the opinions and concerns of the student . however , the low participation ( 5-10 % ) in discussion forums , prompts the modeling of user behavior based on clickstream information . here we study a predictive model for learner attrition on a given week using information mined just from the clickstream . features that are related to the quiz attempt/submission and those that capture interaction with various course components are found to be reasonable predictors of attrition in a given week .

semantic topic models : combining word distributional statistics and
in this paper , we propose a novel topic model based on incorporating dictionary definitions . traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning . they infer topics only by observing surface word co-occurrence . however , the co-occurred words may not be semantically related in a manner that is relevant for topic coherence . exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling . we exploit wordnet as a lexical resource for sense definitions . we show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task .

the karamel system and semitic languages : structured multi-tiered
karamel is a system for finite-state morphology which is multi-tape and uses a typed cartesian product to relate tapes in a structured way . it implements statically compiled feature structures . its language allows the use of regular expressions and generalized restriction rules to define multi-tape transducers . both simultaneous and successive application of local constraints are possible . this system is interesting for describing rich and structured morphologies such as the morphology of semitic languages .

developing guidelines for the annotation of anaphors in the
this paper describes the ctb coreference annotation guidelines for annotating pronominal anaphoric expressions in the penn chinese treebank . the goals of the annotation are : to provide training data for learning-based pronoun resolution tools , and to provide a \gold '' standard to be used in the evaluation of pronoun resolution algorithms . the choices that were made concerning the coindexing of pronominal anaphors and their antecedents are discussed , as are some questions that arose in trying to categorize those pronominal expressions that did not refer to speci c nominal entities in the text .

distributed language modeling for n -best list re-ranking
in this paper we describe a novel distributed language model for n -best list re-ranking . the model is based on the client/server paradigm where each server hosts a portion of the data and provides information to the client . this model allows for using an arbitrarily large corpus in a very efficient way . it also provides a natural platform for relevance weighting and selection . we applied this model on a 2.97 billion-word corpus and re-ranked the n -best list from hiero , a state-of-theart phrase-based system . using bleu as a metric , the re-ranked translation achieves a relative improvement of 4.8 % , significantly better than the model-best translation .

a robust and hybrid deep-linguistic theory applied to
modern statistical parsers are robust and quite fast , but their output is relatively shallow when compared to formal grammar parsers . we suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser . the resulting parsing architecture suggested , implemented and evaluated here is highly robust and hybrid on a number of levels , combining statistical and rule-based approaches , constituency and dependency grammar , shallow and deep processing , full and nearfull parsing . with its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article .

data homogeneity and semantic role tagging in chinese
this paper reports on a study of semantic role tagging in chinese in the absence of a parser . we tackle the task by identifying the relevant headwords in a sentence as a first step to partially locate the corresponding constituents to be labelled . we also explore the effect of data homogeneity by experimenting with a textbook corpus and a news corpus , representing simple data and complex data respectively . results suggest that while the headword location method remains to be improved , the homogeneity between the training and testing data is important especially in view of the characteristic syntaxsemantics interface in chinese . we also plan to explore some class-based techniques for the task with reference to existing semantic lexicons , and to modify the method and augment the feature set with more linguistic input .

bootstrapping a stochastic transducer for arabic-english transliteration extraction
we propose a bootstrapping approach to training a memoriless stochastic transducer for the task of extracting transliterations from an english-arabic bitext . the transducer learns its similarity metric from the data in the bitext , and thus can function directly on strings written in different writing scripts without any additional language knowledge . we show that this bootstrapped transducer performs as well or better than a model designed specifically to detect arabic-english transliterations .

automatically learning source-side reordering rules for large scale
we describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine translation . we learn rules for 8 different language pairs , showing bleu improvements for all of them , and demonstrate that many important order transformations ( svo to sov or vso , headmodifier , verb movement ) can be captured by this approach .

distinguishing between positive and negative opinions with complex
topological and dynamic features of complex networks have proven to be suitable for capturing text characteristics in recent years , with various applications in natural language processing . in this article we show that texts with positive and negative opinions can be distinguished from each other when represented as complex networks . the distinction was possible by obtaining several metrics of the networks , including the in-degree , out-degree , shortest paths , clustering coefficient , betweenness and global efficiency . for visualization , the obtained multidimensional dataset was projected into a 2-dimensional space with the canonical variable analysis . the distinction was quantified using machine learning algorithms , which allowed an recall of 70 % in the automatic discrimination for the negative opinions , even without attempts to optimize the pattern recognition process .

kea : expression-level sentiment analysis from twitter data
this paper describes an expression-level sentiment detection system that participated in the subtask a of semeval-2013 task 2 : sentiment analysis in twitter . our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive , negative or neutral . the proposed approach helps to understand the relevant features that contribute most in this classification task .

a first order semantic approach to adjectival inference
as shown in the formal semantics literature , adjectives can display very different inferential patterns depending on whether they are intersective , privative , subsective or plain non-subsective . moreover , many of these classes are often described using second order constructs . in this paper , we adopt hobbss ontologically promiscuous approach and present a first order treatment of adjective semantics which opens the way for a sophisticated treatment of adjectival inference . the approach was implemented and tested using first order automated reasoners .

bestcut : a graph algorithm for coreference resolution
in this paper we describe a coreference resolution method that employs a classification and a clusterization phase . in a novel way , the clusterization is produced as a graph cutting algorithm , in which nodes of the graph correspond to the mentions of the text , whereas the edges of the graph constitute the confidences derived from the coreference classification . in experiments , the graph cutting algorithm for coreference resolution , called bestcut , achieves state-of-the-art performance .

parsing arguments of nominalizations in english and chinese
in this paper , we use a machine learning framework for semantic argument parsing , and apply it to the task of parsing arguments of eventive nominalizations in the framenet database . we create a baseline system using a subset of features introduced by gildea and jurafsky ( 2002 ) , which are directly applicable to nominal predicates . we then investigate new features which are designed to capture the novelties in nominal argument structure and show a significant performance improvement using these new features . we also investigate the parsing performance of nominalizations in chinese and compare the salience of the features for the two languages .

a fast , accurate deterministic parser for chinese
we present a novel classifier-based deterministic parser for chinese constituency parsing . our parser computes parse trees from bottom up in one pass , and uses classifiers to make shift-reduce decisions . trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results . our svm parser is 2-13 times faster than state-of-the-art parsers , while producing more accurate results . our maxent and dtree parsers run at speeds 40-270 times faster than state-of-the-art parsers , but with 5-6 % losses in accuracy .

some aspects of the morphological processing of bulgarian
this paper demonstrates the modelling of morphological knowledge in bulgarian and applications of the created data sets in an integrated framework for production and manipulation of language resources . the production scenario is exemplified by the bulgarian verb as the morphologically richest and most problematic part-of-speech category . the definition of the set of morphosyntactic specifications for verbs in the lexicon is described . the application of the tagset in the automatic morphological analysis of text corpora is accounted for . a type model of bulgarian verbs handling the attachment of short pronominal elements to verbs , is presented .

native language identification using large scale lexical features
this paper describes an effort to perform native language identification ( nli ) using machine learning on a large amount of lexical features . the features were collected from sequences and collocations of bare word forms , suffixes and character n-grams amounting to a feature set of several hundred thousand features . these features were used to train a linear support vector machine ( svm ) classifier for predicting the native language category .

rali : automatic weighting of text window distances
systems using text windows to model word contexts have mostly been using fixed-sized windows and uniform weights . the window size is often selected by trial and error to maximize task results . we propose a non-supervised method for selecting weights for each window distance , effectively removing the need to limit window sizes , by maximizing the mutual generation of two sets of samples of the same word . experiments on semeval word sense disambiguation tasks showed considerable improvements .

cancer stage prediction based on patient online discourse
forums and mailing lists dedicated to particular diseases are increasingly popular online . automatically inferring the health status of a patient can be useful for both forum users and health researchers who study patients online behaviors . in this paper , we focus on breast cancer forums and present a method to predict the stage of patients cancers from their online discourse . we show that what the patients talk about ( content-based features ) and whom they interact with ( social networkbased features ) provide complementary cues to predicting cancer stage and can be leveraged for better prediction . our methods are extendable and can be applied to other tasks of acquiring contextual information about online health forum participants .

fast and accurate arc filtering for dependency parsing
we propose a series of learned arc filters to speed up graph-based dependency parsing . a cascade of filters identify implausible head-modifier pairs , with time complexity that is first linear , and then quadratic in the length of the sentence . the linear filters reliably predict , in context , words that are roots or leaves of dependency trees , and words that are likely to have heads on their left or right . we use this information to quickly prune arcs from the dependency graph . more than 78 % of total arcs are pruned while retaining 99.5 % of the true dependencies . these filters improve the speed of two state-ofthe-art dependency parsers , with low overhead and negligible loss in accuracy .

using language modeling to select useful annotation data
an annotation project typically has an abundant supply of unlabeled data that can be drawn from some corpus , but because the labeling process is expensive , it is helpful to pre-screen the pool of the candidate instances based on some criterion of future usefulness . in many cases , that criterion is to improve the presence of the rare classes in the data to be annotated . we propose a novel method for solving this problem and show that it compares favorably to a random sampling baseline and a clustering algorithm .

kenlm : faster and smaller language model queries
we present kenlm , a library that implements two data structures for efficient language model queries , reducing both time and memory costs . the probing data structure uses linear probing hash tables and is designed for speed . compared with the widelyused srilm , our probing model is 2.4 times as fast while using 57 % of the memory . the trie data structure is a trie with bit-level packing , sorted records , interpolation search , and optional quantization aimed at lower memory consumption . trie simultaneously uses less memory than the smallest lossless baseline and less cpu than the fastest baseline . our code is open-source1 , thread-safe , and integrated into the moses , cdec , and joshua translation systems . this paper describes the several performance techniques used and presents benchmarks against alternative implementations .

using technology transfer to advance automatic lemmatisation for
south african languages ( and indigenous african languages in general ) lag behind other languages in terms of the availability of linguistic resources . efforts to improve or fasttrack the development of linguistic resources are required to bridge this ever-increasing gap . in this paper we emphasize the advantages of technology transfer between two languages to advance an existing linguistic technology/resource . the advantages of technology transfer are illustrated by showing how an existing lemmatiser for setswana can be improved by applying a methodology that was first used in the development of a lemmatiser for afrikaans .

an exact inference algorithm and its efficient approximation
latent conditional models have become popular recently in both natural language processing and vision processing communities . however , establishing an effective and efficient inference method on latent conditional models remains a question . in this paper , we describe the latent-dynamic inference ( ldi ) , which is able to produce the optimal label sequence on latent conditional models by using efficient search strategy and dynamic programming . furthermore , we describe a straightforward solution on approximating the ldi , and show that the approximated ldi performs as well as the exact ldi , while the speed is much faster . our experiments demonstrate that the proposed inference algorithm outperforms existing inference methods on a variety of natural language processing tasks .

v-measure : a conditional entropy-based external cluster evaluation
we present v-measure , an external entropybased cluster evaluation measure . vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1 ) dependence on clustering algorithm or data set , 2 ) the problem of matching , where the clustering of only a portion of data points are evaluated and 3 ) accurate evaluation and combination of two desirable aspects of clustering , homogeneity and completeness . we compare v-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions , using simulated clustering results . finally , we use v-measure to evaluate two clustering tasks : document clustering and pitch accent type clustering .

exploiting social relations and sentiment for stock prediction
in this paper we first exploit cash-tags ( $ followed by stocks ticker symbols ) in twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets . we then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively . this semantic stock network ( ssn ) summarizes discussion topics about stocks and stock relations . we further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stocks market . for prediction , we propose to regress the topic-sentiment time-series and the stocks price time series . experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .

improving arabic dependency parsing with form-based and functional
we explore the contribution of morphological features both lexical and inflectional to dependency parsing of arabic , a morphologically rich language . using controlled experiments , we find that definiteness , person , number , gender , and the undiacritzed lemma are most helpful for parsing on automatically tagged input . we further contrast the contribution of form-based and functional features , and show that functional gender and number ( e.g. , broken plurals ) and the related rationality feature improve over form-based features . it is the first time functional morphological features are used for arabic nlp .

domain adaptation for crf-based chinese word segmentation using
supervised methods have been the dominant approach for chinese word segmentation . the performance can drop significantly when the test domain is different from the training domain . in this paper , we study the problem of obtaining partial annotation from freely available data to help chinese word segmentation on different domains . different sources of free annotations are transformed into a unified form of partial annotation and a variant crf model is used to leverage both fully and partially annotated data consistently . experimental results show that the chinese word segmentation model benefits from free partially annotated data . on the sighan bakeoff 2010 data , we achieve results that are competitive to the best reported in the literature .

creating an annotated corpus for generating walking directions
this work describes first steps towards building a system that synchronously generates multimodal ( textual and visual ) route directions for pedestrians . we pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages . we conducted an empirical study to collect verbal route directions , and annotated the acquired texts on different levels . here we describe the experimental setting and an analysis of the collected data .

a three-step transition-based system for non-projective dependency parsing
this paper presents a non-projective dependency parsing system that is transition-based and operates in three steps . the three steps include one classical method for projective dependency parsing and two inverse methods predicting separately the right and left non-projective dependencies . splitting the parsing allows to increase the scores on both projective and non-projective dependencies compared to state-of-the-art non-projective dependency parsing . moreover , each step is performed in linear time .

interactive gesture in dialogue : a ptt model
gestures are usually looked at in isolation or from an intra-propositional perspective essentially tied to one speaker . the bielefeld multi-modal speech-andgesture-alignment ( saga ) corpus has many interactive gestures relevant for the structure of dialogue ( rieser 2008 , 2009 ) . to describe them , a dialogue theory is needed which can serve as a speechgesture interface . ptt ( poesio and traum 1997 , poesio and rieser submitted a ) can do this job in principle , how this can be achieved is the main topic of this paper . as a precondition , the empirical research procedure from systematic corpus annotation via gesture typology to a partial ontology for gestures is described . it is then explained how ptt is extended to provide an incremental modelling of speech plus gesture in an assertion-acknowledgement adjacency pair where grounding between dialogue participants is obtained through gesture .

translating into morphologically rich languages with synthetic phrases
translation into morphologically rich languages is an important but recalcitrant problem in mt . we present a simple and effective approach that deals with the problem in two phases . first , a discriminative model is learned to predict inflections of target words from rich source-side annotations . then , this model is used to create additional sentencespecific word- and phrase-level translations that are added to a standard translation model as synthetic phrases . our approach relies on morphological analysis of the target language , but we show that an unsupervised bayesian model of morphology can successfully be used in place of a supervised analyzer . we report significant improvements in translation quality when translating from english to russian , hebrew and swahili .

constraints in language processing : do grammars count
one of the central assumptions of optimality theory is the hypothesis of strict domination among constraints . a few studies have suggested that this hypothesis is too strong and should be abandoned in favor of a weaker cumulativity hypothesis . if this suggestion is correct , we should be able to find evidence for cumulativity in the comprehension of gapping sentences , which lack explicit syntactic clues in the form of the presence of a finite verb . on the basis of a comparison between several computational models of constraint evaluation , we conclude that the comprehension of gapping sentences does not yield compelling evidence against the strict domination hypothesis .

pycwn : a python module for chinese wordnet
this presentation introduces a python module ( pycwn ) for accessing and processing chinese lexical resources . in particular , our focus is put on the chinese wordnet ( cwn ) that has been developed and released by cwn group at academia sinica . pycwn provides the access to chinese wordnet ( sense and relation data ) under the python environment . the presenation further demonstrates how this module applies to a variety of lexical processing tasks as well as the potentials for multilingual lexical processing .

combining distant and partial supervision for relation extraction
broad-coverage relation extraction either requires expensive supervised training data , or suffers from drawbacks inherent to distant supervision . we present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples . we compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative . in this way , we combine the benefits of fine-grained supervision for difficult examples with the coverage of a large distantly supervised corpus . our approach gives a substantial increase of 3.9 % endto-end f 1 on the 2013 kbp slot filling evaluation , yielding a net f 1 of 37.7 % .

probabilistic models for pp-attachment resolution and np analysis
we present a general model for pp attachment resolution and np analysis in french . we make explicit the different assumptions our model relies on , and show how it generalizes previously proposed models . we then present a series of experiments conducted on a corpus of newspaper articles , and assess the various components of the model , as well as the different information sources used .

multi-document summarization via budgeted maximization of submodular functions
we treat the text summarization problem as maximizing a submodular function under a budget constraint . we show , both theoretically and empirically , a modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally , and we derive new approximation bounds in doing so . experiments on duc04 task show that our approach is superior to the bestperforming method from the duc04 evaluation on rouge-1 scores .

using collocations for topic segmentation and link detection
we present in this paper a method for achieving in an integrated way two tasks of topic analysis : segmentation and link detection . this method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches . we report an evaluation of our method for segmentation on two corpora , one in french and one in english , and we propose an evaluation measure that specifically suits that kind of systems .

semantic classification with wordnet kernels diarmuid o seaghdha
this paper presents methods for performing graph-based semantic classification using kernel functions defined on the wordnet lexical hierarchy . these functions are evaluated on the semeval task 4 relation classification dataset and their performance is shown to be competitive with that of more complex systems . a number of possible future developments are suggested to illustrate the flexibility of the approach .

the extraction of enriched protein-protein interactions from biomedical
there has been much recent interest in the extraction of ppis ( protein-protein interactions ) from biomedical texts , but in order to assist with curation efforts , the ppis must be enriched with further information of biological interest . this paper describes the implementation of a system to extract and enrich ppis , developed and tested using an annotated corpus of biomedical texts , and employing both machine-learning and rulebased techniques .

a plethora of methods for learning english countability
this paper compares a range of methods for classifying words based on linguistic diagnostics , focusing on the task of learning countabilities for english nouns . we propose two basic approaches to feature representation : distribution-based representation , which simply looks at the distribution of features in the corpus data , and agreement-based representation which analyses the level of tokenwise agreement between multiple preprocessor systems . we additionally compare a single multiclass classifier architecture with a suite of binary classifiers , and combine analyses from multiple preprocessors . finally , we present and evaluate a feature selection method .

latent class transliteration based on source language origin
transliteration , a rich source of proper noun spelling variations , is usually recognized by phonetic- or spelling-based models . however , a single model can not deal with different words from different language origins , e.g. , get in piaget and target . li et al . ( 2007 ) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly . this model , however , requires an explicitly tagged training set with language origins . we propose a novel method which models language origins as latent classes . the parameters are learned from a set of transliterated word pairs via the em algorithm . the experimental results of the transliteration task of western names to japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes .

unsupervised learning of contextual role knowledge for coreference
we present a coreference resolver called babar that uses contextual role knowledge to evaluate possible antecedents for an anaphor . babar uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning . these knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible . babar applies a dempster-shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources . experiments in two domains showed that the contextual role knowledge improved coreference performance , especially on pronouns .

improving statistical machine translation with word class models
automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing . we present a very simple and easy to implement method for using these word classes to improve translation quality . it can be applied across different machine translation paradigms and with arbitrary types of models . we show its efficacy on a small germanenglish and a larger frenchgerman translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models . our results show that with word class models , the baseline can be improved by up to 1.4 % bleu and 1.0 % ter on the frenchgerman task and 0.3 % bleu and 1.1 % ter on the germanenglish task .

a development environment for large-scale multi-lingual parsing systems
we describe the development environment available to linguistic developers in our lab in writing large-scale grammars for multiple languages . the environment consists of the tools that assist writing linguistic rules and running regression testing against large corpora , both of which are indispensable for realistic development of large-scale parsing systems . we also emphasize the importance of parser efficiency as an integral part of efficient parser development . the tools and methods described in this paper are actively used in the daily development of broad-coverage natural language understanding systems in seven languages ( chinese , english , french , german , japanese , korean and spanish ) .

ubc-alm : combining k-nn with svd for wsd
this work describes the university of the basque country system ( ubc-alm ) for lexical sample and all-words wsd subtasks of semeval-2007 task 17 , where it performed in the second and fifth positions respectively . the system is based on a combination of k-nearest neighbor classifiers , with each classifier learning from a distinct set of features : local features ( syntactic , collocations features ) , topical features ( bag-ofwords , domain information ) and latent features learned from a reduced space using singular value decomposition .

donghui feng gully burns jingbo zhu eduard hovy
in this paper , we present an empirical study on adapting conditional random fields ( crf ) models to conduct semantic analysis on biomedical articles using active learning . we explore uncertaintybased active learning with the crf model to dynamically select the most informative training examples . this abridges the power of the supervised methods and expensive human annotation cost .

models for the semantic classification of noun phrases
this paper presents an approach for detecting semantic relations in noun phrases . a learning algorithm , called semantic scattering , is used to automatically label complex nominals , genitives and adjectival noun phrases with the corresponding semantic relation .

example-based paraphrasing for improved phrase-based statistical machine translation
in this article , an original view on how to improve phrase translation estimates is proposed . this proposal is grounded on two main ideas : first , that appropriate examples of a given phrase should participate more in building its translation distribution ; second , that paraphrases can be used to better estimate this distribution . initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance .

detecting code-switching in a multilingual alpine heritage corpus
this paper describes experiments in detecting and annotating code-switching in a large multilingual diachronic corpus of swiss alpine texts . the texts are in english , french , german , italian , romansh and swiss german . because of the multilingual authors ( mountaineers , scientists ) and the assumed multilingual readers , the texts contain numerous code-switching elements . when building and annotating the corpus , we faced issues of language identification on the sentence and subsentential level . we present our strategy for language identification and for the annotation of foreign language fragments within sentences . we report 78 % precision on detecting a subset of code-switches with correct language labels and 92 % unlabeled precision .

feature-rich part-of-speech tagging with a cyclic dependency network
we present a new part-of-speech tagger that demonstrates the following ideas : ( i ) explicit use of both preceding and following tag contexts via a dependency network representation , ( ii ) broad use of lexical features , including jointly conditioning on multiple consecutive words , ( iii ) effective use of priors in conditional loglinear models , and ( iv ) fine-grained modeling of unknown word features . using these ideas together , the resulting tagger gives a 97.24 % accuracy on the penn treebank wsj , an error reduction of 4.4 % on the best previous single automatically learned tagging result .

pku : combining supervised classifiers with features selection
this paper presents the word sense disambiguation system of peking university which was designed for the semeval-2007 competition . the system participated in the web track of task 11 english lexical sample task via english-chinese parallel text . the system is a hybrid model by combining two supervised learning algorithms svm and me . and the method of entropy-based feature chosen was experimented . we obtained precision ( and recall ) of 81.5 % .

enhancement of lexical concepts using cross-lingual web mining
sets of lexical items sharing a significant aspect of their meaning ( concepts ) are fundamental in linguistics and nlp . manual concept compilation is labor intensive , error prone and subjective . we present a web-based concept extension algorithm . given a set of terms specifying a concept in some language , we translate them to a wide range of intermediate languages , disambiguate the translations using web counts , and discover additional concept terms using symmetric patterns . we then translate the discovered terms back into the original language , score them , and extend the original concept by adding backtranslations having high scores . we evaluate our method in 3 source languages and 45 intermediate languages , using both human judgments and wordnet . in all cases , our cross-lingual algorithm significantly improves high quality concept extension .

high-order sequence modeling for language learner error detection
we address the problem of detecting english language learner errors by using a discriminative high-order sequence model . unlike most work in error-detection , this method is agnostic as to specific error types , thus potentially allowing for higher recall across different error types . the approach integrates features from many sources into the error-detection model , ranging from language model-based features to linguistic analysis features . evaluation results on a large annotated corpus of learner writing indicate the feasibility of our approach on a realistic , noisy and inherently skewed set of data . high-order models consistently outperform low-order models in our experiments . error analysis on the output shows that the calculation of precision on the test set represents a lower bound on the real system performance .

a discriminative model for joint morphological disambiguation and
most previous studies of morphological disambiguation and dependency parsing have been pursued independently . morphological taggers operate on n-grams and do not take into account syntactic relations ; parsers use the pipeline approach , assuming that morphological information has been separately obtained . however , in morphologically-rich languages , there is often considerable interaction between morphology and syntax , such that neither can be disambiguated without the other . in this paper , we propose a discriminative model that jointly infers morphological properties and syntactic structures . in evaluations on various highly-inflected languages , this joint model outperforms both a baseline tagger in morphological disambiguation , and a pipeline parser in head selection .

alternative approaches for generating bodies of grammar rules
we compare two approaches for describing and generating bodies of rules used for natural language parsing . in todays parsers rule bodies do not exist a priori but are generated on the fly , usually with methods based on n-grams , which are one particular way of inducing probabilistic regular languages . we compare two approaches for inducing such languages . one is based on n-grams , the other on minimization of the kullback-leibler divergence . the inferred regular languages are used for generating bodies of rules inside a parsing procedure . we compare the two approaches along two dimensions : the quality of the probabilistic regular language they produce , and the performance of the parser they were used to build . the second approach outperforms the first one along both dimensions .

enhancing automatic term recognition through recognition of variation
terminological variation is an integral part of the linguistic ability to realise a concept in many ways , but it is typically considered an obstacle to automatic term recognition ( atr ) and term management . we present a method that integrates term variation in a hybrid atr approach , in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants . we evaluate the effectiveness of incorporating specific types of term variation by comparing it to the performance of a baseline method that treats term variants as separate terms . we show that atr precision is enhanced by considering joint termhoods of all term variants ,

spoken language parsing using phrase-level grammars and trainable
in this paper , we describe a novel approach to spoken language analysis for translation , which uses a combination of grammar-based phrase-level parsing and automatic classification . the job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances . the goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages .

fast and accurate misspelling correction in large corpora
there are several nlp systems whose accuracy depends crucially on finding misspellings fast . however , the classical approach is based on a quadratic time algorithm with 80 % coverage . we present a novel algorithm for misspelling detection , which runs in constant time and improves the coverage to more than 96 % . we use this algorithm together with a cross document coreference system in order to find proper name misspellings . the experiments confirmed significant improvement over the state of the art .

a lightweight terminology verification service for external machine
we propose a demonstration of a domainspecific terminology checking service which works on top of any generic blackbox mt , and only requires access to a bilingual terminology resource in the domain . in cases where an incorrect translation of a source term was proposed by the generic mt service , our service locates the wrong translation of the term in the target and suggests a terminologically correct translation for this term .

towards surface realization with ccgs induced from dependencies
we present a novel algorithm for inducing combinatory categorial grammars from dependency treebanks , along with initial experiments showing that it can be used to achieve competitive realization results using an enhanced version of the surface realization shared task data .

rule filtering by pattern for efficient hierarchical translation
we describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation . rules are put into syntactic classes based on the number of non-terminals and the pattern , and various filtering strategies are then applied to assess the impact on translation speed and quality . results are reported on the 2008 nist arabic-toenglish evaluation task .

inclusive yet selective : supervised distributional hypernymy detection
we test the distributional inclusion hypothesis , which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found . we find that this hypothesis only holds when it is applied to relevant dimensions . we propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion .

online relative margin maximization for statistical machine translation
recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization , such as the spread of the data . however , these solutions are impractical in complex structured prediction problems such as statistical machine translation . we present an online gradient-based algorithm for relative margin maximization , which bounds the spread of the projected data while maximizing the margin . we evaluate our optimizer on chinese-english and arabicenglish translation tasks , each with small and large feature sets , and show that our learner is able to achieve significant improvements of 1.2-2 bleu and 1.7-4.3 ter on average over state-of-the-art optimizers with the large feature set .

composition of semantic relations : model and applications
this paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed . a set of 26 relations is introduced , with their arguments defined on an ontology of sorts . a semantic parser is used to extract these relations from noun phrases and verb argument structures . the method was successfully used in two applications : rapid customization of semantic relations to arbitrary domains and recognizing entailments .

a general-purpose rule extractor for scfg-based machine translation
we present a rule extractor for scfg-based mt that generalizes many of the contraints present in existing scfg extraction algorithms . our methods increased rule coverage comes from allowing multiple alignments , virtual nodes , and multiple tree decompositions in the extraction process . at decoding time , we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set , while our experiments with hierarchical grammar filtering indicate that more intelligent filtering schemes will also provide a key to future gains .

an example-based decoder for spoken language machine translation
in this paper , we propose an example-based decoder for a statistical machine translation ( smt ) system , which is used for spoken language machine translation . in this way , it will help to solve the re-ordering problem and other problems for spoken language mt , such as lots of omissions , idioms etc . through experiments , we show that this approach obtains improvements over the baseline on a chinese-english spoken language translation task .

optimizing word alignment combination for phrase table training
combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications . we propose a novel method that performs combination as an optimization process . our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction . experimental results show that the proposed method leads to significantly better translation quality than existing methods . analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage .

classifying chart cells for quadratic complexity context-free inference
in this paper , we consider classifying word positions by whether or not they can either start or end multi-word constituents . this provides a mechanism for closing chart cells during context-free inference , which is demonstrated to improve efficiency and accuracy when used to constrain the wellknown charniak parser . additionally , we present a method for closing a sufficient number of chart cells to ensure quadratic worst-case complexity of context-free inference . empirical results show that this o ( n 2 ) bound can be achieved without impacting parsing accuracy .

discovering implicit discourse relations through brown cluster pair
sentences form coherent relations in a discourse without discourse connectives more frequently than with connectives . senses of these implicit discourse relations that hold between a sentence pair , however , are challenging to infer . here , we employ brown cluster pairs to represent discourse relation and incorporate coreference patterns to identify senses of implicit discourse relations in naturally occurring text . our system improves the baseline performance by as much as 25 % . feature analyses suggest that brown cluster pairs and coreference patterns can reveal many key linguistic characteristics of each type of discourse relation .

a topic model for building fine-grained domain-specific emotion
emotion lexicons play a crucial role in sentiment analysis and opinion mining . in this paper , we propose a novel emotion-aware lda ( ealda ) model to build a domainspecific lexicon for predefined emotions that include anger , disgust , fear , joy , sadness , surprise . the model uses a minimal set of domain-independent seed words as prior knowledge to discover a domainspecific lexicon , learning a fine-grained emotion lexicon much richer and adaptive to a specific domain . by comprehensive experiments , we show that our model can generate a high-quality fine-grained domain-specific emotion lexicon .

an eye-tracking evaluation of some parser complexity metrics
information theoretic measures of incremental parser load were generated from a phrase structure parser and a dependency parser and then compared with incremental eye movement metrics collected for the same temporarily syntactically ambiguous sentences , focussing on the disambiguating word . the findings show that the surprisal and entropy reduction metrics computed over a phrase structure grammar make good candidates for predictors of text readability for human comprehenders . this leads to a suggestion for the use of such metrics in natural language generation ( nlg ) .

syntactic smt using a discriminative text generation model
we study a novel architecture for syntactic smt . in contrast to the dominant approach in the literature , the system does not rely on translation rules , but treat translation as an unconstrained target sentence generation task , using soft features to capture lexical and syntactic correspondences between the source and target languages . target syntax features and bilingual translation features are trained consistently in a discriminative model . experiments using the iwslt 2010 dataset show that the system achieves bleu comparable to the state-of-the-art syntactic smt systems .

a re-examination of query expansion using lexical resources
query expansion is an effective technique to improve the performance of information retrieval systems . although hand-crafted lexical resources , such as wordnet , could provide more reliable related terms , previous studies showed that query expansion using only wordnet leads to very limited performance improvement . one of the main challenges is how to assign appropriate weights to expanded terms . in this paper , we re-examine this problem using recently proposed axiomatic approaches and find that , with appropriate term weighting strategy , we are able to exploit the information from lexical resources to significantly improve the retrieval performance . our empirical results on six trec collections show that query expansion using only hand-crafted lexical resources leads to significant performance improvement . the performance can be further improved if the proposed method is combined with query expansion using co-occurrence-based resources .

a class of submodular functions for document summarization
we design a class of submodular functions meant for document summarization tasks . these functions each combine two terms , one which encourages the summary to be representative of the corpus , and the other which positively rewards diversity . critically , our functions are monotone nondecreasing and submodular , which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality . when evaluated on duc 2004-2007 corpora , we obtain better than existing state-of-art results in both generic and query-focused document summarization . lastly , we show that several well-established methods for document summarization correspond , in fact , to submodular function optimization , adding further evidence that submodular functions are a natural fit for document summarization .

exploring two biomedical text genres for disease recognition
in the framework of contextual information retrieval in the biomedical domain , this paper reports on the automatic detection of disease concepts in two genres of biomedical text : sentences from the literature and pubmed user queries . a statistical model and a natural language processing algorithm for disease recognition were applied on both corpora . while both methods show good performance ( f=77 % vs. f=76 % ) on the sentence corpus , results on the query corpus indicate that the statistical model is more robust ( f=74 % vs. f=70 % ) .

cognate and misspelling features for natural language identification
we apply support vector machines to differentiate between 11 native languages in the 2013 native language identification shared task . we expand a set of common language identification features to include cognate interference and spelling mistakes . our best results are obtained with a classifier which includes both the cognate and the misspelling features , as well as word unigrams , word bigrams , character bigrams , and syntax production rules .

automatic induction of a ccg grammar for turkish
this paper presents the results of automatically inducing a combinatory categorial grammar ( ccg ) lexicon from a turkish dependency treebank . the fact that turkish is an agglutinating free wordorder language presents a challenge for language theories . we explored possible ways to obtain a compact lexicon , consistent with ccg principles , from a treebank which is an order of magnitude smaller than penn wsj .

on the means for clari cation in dialogue
the ability to request clari cation of utterances is a vital part of the communicative process . in this paper we discuss the range of possible forms for clari cation requests , together with the range of readings they can convey . we present the results of corpus analysis which show a correlation between certain forms and possible readings , together with some indication of maximum likely distance between request and the utterance being clari ed . we then explain the implications of these results for a possible hpsg analysis of clari cation requests and for an ongoing implementation of a clari cation-capable dialogue system .

coupling a linguistic formalism and a script language
this article presents a novel syntactic parser architecture , in which a linguistic formalism can be enriched with all sorts of constraints , included extra-linguistic ones , thanks to the seamless coupling of the formalism with a programming language .

incremental structured prediction using a global learning and
this tutorial discusses a framework for incremental left-to-right structured predication , which makes use of global discriminative learning and beam-search decoding . the method has been applied to a wide range of nlp tasks in recent years , and

predicative nps and the annotation of reference chains
in the development of machine learning systems for identification of reference chains , hand-annotated corpora play a crucial role . this paper concerns the question of how predicative nps should be annotated w.r.t . coreference in corpora for such systems . this question highlights the tension that sometimes appears in the development of corpora between linguistic considerations and the aim for perfection on the one hand and practical applications and the aim for efficiency on the other . many current projects that seek to identify coreferential links automatically , assume an annotation strategy which instructs the annotator to mark a predicative np as coreferential with its subject if it is part of a positive sentence . this paper argues that such a representation is not linguistically plausible , and that it will fail to generate an optimal result .

word buffering models for improved speech repair parsing
this paper describes a time-series model for parsing transcribed speech containing disfluencies . this model differs from previous parsers in its explicit modeling of a buffer of recent words , which allows it to recognize repairs more easily due to the frequent overlap in words between errors and their repairs . the parser implementing this model is evaluated on the standard switchboard transcribed speech parsing task for overall parsing accuracy and edited word detection .

efficient support vector classifiers for named entity recognition
named entity ( ne ) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person , organization , and date . it is a key technology of information extraction and open-domain question answering . first , we show that an ne recognizer based on support vector machines ( svms ) gives better scores than conventional systems . however , off-the-shelf svm classifiers are too inefficient for this task . therefore , we present a method that makes the system substantially faster . this approach can also be applied to other similar tasks such as chunking and part-of-speech tagging . we also present an svm-based feature selection method and an efficient training method .

using generation for grammar analysis and error detection
we demonstrate that the bidirectionality of deep grammars , allowing them to generate as well as parse sentences , can be used to automatically and effectively identify errors in the grammars . the system is tested on two implemented hpsg grammars : jacy for japanese , and the erg for english . using this system , we were able to increase generation coverage in jacy by 18 % ( 45 % to 63 % ) with only four weeks of grammar development .

hierarchical multi-class text categorization with global margin maximization
text categorization is a crucial and wellproven method for organizing the collection of large scale documents . in this paper , we propose a hierarchical multi-class text categorization method with global margin maximization . we not only maximize the margins among leaf categories , but also maximize the margins among their ancestors . experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms .

composition of conditional random fields for transfer learning
many learning tasks have subtasks for which much training data exists . therefore , we want to transfer learning from the old , generalpurpose subtask to a more specific new task , for which there is often less data . while work in transfer learning often considers how the old task should affect learning on the new task , in this paper we show that it helps to take into account how the new task affects the old . specifically , we perform joint decoding of separately-trained sequence models , preserving uncertainty between the tasks and allowing information from the new task to affect predictions on the old task . on two standard text data sets , we show that joint decoding outperforms cascaded decoding .

dual decomposition for parsing with non-projective head automata
this paper introduces algorithms for nonprojective parsing based on dual decomposition . we focus on parsing algorithms for nonprojective head automata , a generalization of head-automata models to non-projective structures . the dual decomposition algorithms are simple and efficient , relying on standard dynamic programming and minimum spanning tree algorithms . they provably solve an lp relaxation of the non-projective parsing problem . empirically the lp relaxation is very often tight : for many languages , exact solutions are achieved on over 98 % of test sentences . the accuracy of our models is higher than previous work on a broad range of datasets .

an extensive empirical study of collocation extraction methods
this paper presents a status quo of an ongoing research study of collocations an essential linguistic phenomenon having a wide spectrum of applications in the field of natural language processing . the core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction methods using precision-recall measures and a proposal of a new approach integrating multiple basic methods and statistical classification . we demonstrate that combining multiple independent techniques leads to a significant performance improvement in comparisonwith individualbasic methods .

computational analysis of move structures in academic abstracts
this paper introduces a method for computational analysis of move structures in abstracts of research articles . in our approach , sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions . the method involves automatically gathering a large number of abstracts from the web and building a language model of abstract moves . we also present a prototype concordancer , care , which exploits the move-tagged abstracts for digital learning . this system provides a promising approach to webbased computer-assisted academic writing .

a simple sentence-level extraction algorithm for comparable data
the paper presents a novel sentence pair extraction algorithm for comparable data , where a large set of candidate sentence pairs is scored directly at the sentence-level . the sentencelevel extraction relies on a very efficient implementation of a simple symmetric scoring function : a computation speed-up by a factor of 30 is reported . on spanish-english data , the extraction algorithm finds the highest scoring sentence pairs from close to 1 trillion candidate pairs without search errors . significant improvements in bleu are reported by including the extracted sentence pairs into the training of a phrase-based smt ( statistical machine translation ) system .

revisiting multi-tape automata for semitic morphological analysis and
various methods have been devised to produce morphological analyzers and generators for semitic languages , ranging from methods based on widely used finitestate technologies to very specific solutions designed for a specific language or problem . since the earliest proposals of how to adopt the elsewhere successful finite-state methods to root-andpattern morphologies , the solution of encoding semitic grammars using multi-tape automata has resurfaced on a regular basis . multi-tape automata , however , require specific algorithms and reimplementation of finite-state operators across the board , and hence such technology has not been readily available to linguists . this paper , using an actual arabic grammar as a case study , describes an approach to encoding multi-tape automata on a single tape that can be implemented using any standard finite-automaton toolkit .

from a surface analysis to a dependency structure
this paper describes how we use the arrows properties from the 5p paradigm to generate a dependency structure from a surface analysis . besides the arrows properties , two modules , algas and ogre , are presented . moreover , we show how we express linguistic descriptions away from parsing decisions .

chinese word segmentation based on mixing multiple preprocessor
this paper describes the chinese word segmenter for our participation in cipssighan-2010 bake-off task of chinese word segmentation . we formalize the tasks as sequence tagging problems , and implemented them using conditional random fields ( crfs ) model . the system contains two modules : multiple preprocessor and basic segmenter . the basic segmenter is designed as a problem of character-based tagging , and using named entity recognition and chunk recognition based on boundary to preprocess . we participated in the open training on simplified chinese text and traditional chinese text , and our system achieved one rank # 5 and four rank # 2 best in all four domain corpus .

memory-based resolution of in-sentence scopes of hedge cues
in this paper we describe the machine learning systems that we submitted to the conll-2010 shared task on learning to detect hedges and their scope in natural language text . task 1 on detecting uncertain information was performed by an svm-based system to process the wikipedia data and by a memory-based system to process the biological data . task 2 on resolving in-sentence scopes of hedge cues , was performed by a memorybased system that relies on information from syntactic dependencies . this system scored the highest f1 ( 57.32 ) of task 2 .

training mrf-based phrase translation models using gradient ascent
this paper presents a general , statistical framework for modeling phrase translation via markov random fields . the model allows for arbituary features extracted from a phrase pair to be incorporated as evidence . the parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an n-best list based expected bleu as the objective function . the model is easy to be incoporated into a standard phrase-based statistical machine translation system , requiring no code change in the runtime engine . evaluation is performed on two europarl translation tasks , germanenglish and french-english . results show that incoporating the markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system , leading to a gain of 0.8-1.3 bleu points .

inducing history representations for broad coverage statistical parsing
we present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser . the resulting statistical parser achieves performance ( 89.1 % f-measure ) on the penn treebank which is only 0.6 % below the best current parser for this task , despite using a smaller vocabulary size and less prior linguistic knowledge . crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history , and no use of hard independence assumptions .

on distance between deep syntax and semantic representation
we present a comparison of two formalisms for representing natural language utterances , namely deep syntactical tectogrammatical layer of functional generative description ( fgd ) and a semantic formalism , multinet . we discuss the possible position of multinet in the fgd framework and present a preliminary mapping of representational means of these two formalisms .

improving data-driven dependency parsing using large-scale lfg grammars
this paper presents experiments which combine a grammar-driven and a datadriven parser . we show how the conversion of lfg output to dependency representation allows for a technique of parser stacking , whereby the output of the grammar-driven parser supplies features for a data-driven dependency parser . we evaluate on english and german and show significant improvements stemming from the proposed dependency structure as well as various other , deep linguistic features derived from the respective grammars .

automatic diacritization for low-resource languages using a hybrid
we are interested in diacritizing semitic languages , especially syriac , using only diacritized texts . previous methods have required the use of tools such as part-of-speech taggers , segmenters , morphological analyzers , and linguistic rules to produce state-of-the-art results . we present a low-resource , data-driven , and language-independent approach that uses a hybrid word- and consonant-level conditional markov model . our approach rivals the best previously published results in arabic ( 15 % wer with case endings ) , without the use of a morphological analyzer . in syriac , we reduce the wer over a strong baseline by 30 % to achieve a wer of 10.5 % . we also report results for hebrew and english .

mood patterns and affective lexicon access in weblogs
the emergence of social media brings chances , but also challenges , to linguistic analysis . in this paper we investigate a novel problem of discovering patterns based on emotion and the association of moods and affective lexicon usage in blogosphere , a representative for social media . we propose the use of normative emotional scores for english words in combination with a psychological model of emotion measurement and a nonparametric clustering process for inferring meaningful emotion patterns automatically from data . our results on a dataset consisting of more than 17 million mood-groundtruthed blogposts have shown interesting evidence of the emotion patterns automatically discovered that match well with the coreaffect emotion model theorized by psychologists . we then present a method based on information theory to discover the association of moods and affective lexicon usage in the new media .

deriving adjectival scales from continuous space word representations
continuous space word representations extracted from neural network language models have been used effectively for natural language processing , but until recently it was not clear whether the spatial relationships of such representations were interpretable . mikolov et al ( 2013 ) show that these representations do capture syntactic and semantic regularities . here , we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales ( e.g. , okay < good < excellent ) . we evaluate the scales on the indirect answers to yes/no questions corpus ( de marneffe et al , 2010 ) . we obtain 72.8 % accuracy , which outperforms previous results ( 60 % ) on this corpus and highlights the quality of the scales extracted , providing further support that the continuous space word representations are meaningful .

towards a hybrid model for chinese word segmentation
this paper describes a hybrid chinese word segmenter that is being developed as part of a larger chinese unknown word resolution system . the segmenter consists of two components : a tagging component that uses the transformation-based learning algorithm to tag each character with its position in a word , and a merging component that transforms a tagged character sequence into a word-segmented sentence . in addition to the position-of-character tags assigned to the characters , the merging component makes use of a number of heuristics to handle non-chinese characters , numeric type compounds , and long words . the segmenter achieved a 92.8 % f-score and a 72.8 % recall for oov words in the closed track of the peking university corpus in the second international chinese word segmentation bakeoff .

a discriminative global training algorithm for statistical mt
this paper presents a novel training algorithm for a linearly-scored block sequence translation model . the key component is a new procedure to directly optimize the global scoring function used by a smt decoder . no translation , language , or distortion model probabilities are used as in earlier work on smt . therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches . moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme . the training algorithm is evaluated on a standard arabic-english translation task .

using three way data for word sense discrimination
in this paper , an extension of a dimensionality reduction algorithm called nonnegative matrix factorization is presented that combines both bag of words data and syntactic data , in order to find semantic dimensions according to which both words and syntactic relations can be classified . the use of three way data allows one to determine which dimension ( s ) are responsible for a certain sense of a word , and adapt the corresponding feature vector accordingly , subtracting one sense to discover another one . the intuition in this is that the syntactic features of the syntax-based approach can be disambiguated by the semantic dimensions found by the bag of words approach . the novel approach is embedded into clustering algorithms , to make it fully automatic . the approach is carried out for dutch , and evaluated against eurowordnet .

co-dispersion : a windowless approach to lexical association
we introduce an alternative approach to extracting word pair associations from corpora , based purely on surface distances in the text . we contrast it with the prevailing windowbased co-occurrence model and show it to be more statistically robust and to disclose a broader selection of significant associative relationships - owing largely to the property of scale-independence . in the process we provide insights into the limiting characteristics of window-based methods which complement the sometimes conflicting application-oriented literature in this area .

a tree transducer model for synchronous tree-adjoining grammars
a characterization of the expressive power of synchronous tree-adjoining grammars ( stags ) in terms of tree transducers ( or equivalently , synchronous tree substitution grammars ) is developed . essentially , a stag corresponds to an extended tree transducer that uses explicit substitution in both the input and output . this characterization allows the easy integration of stag into toolkits for extended tree transducers . moreover , the applicability of the characterization to several representational and algorithmic problems is demonstrated .

semantic interpretation of unrealized syntactic material in ltag
this paper presents a ltag-based analysis of gapping and vp ellipsis , which proposes that resolution of the elided material is part of a general disambiguation procedure , which is also responsible for resolution of underspecified representations of scope .

a comparison of features for automatic readability assessment
several sets of explanatory variables including shallow , language modeling , pos , syntactic , and discourse features are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students . we find that features based on in-domain language models have the highest predictive power . entity-density ( a discourse feature ) and pos-features , in particular nouns , are individually very useful but highly correlated . average sentence length ( a shallow feature ) is more useful and less expensive to compute than individual syntactic features . a judicious combination of features examined here results in a significant improvement over the state of the art .

grammatical error detection and correction using tagger disagreement
this paper presents a rule-based approach for correcting grammatical errors made by non-native speakers of english . the approach relies on the differences in the outputs of two pos taggers . this paper is submitted in response to conll-2014 shared task .

monolingual web-based factoid question answering in chinese ,
in this paper we extend the application of our statistical pattern classification approach to question answering ( qa ) which has previously been applied successfully to english and japanese to develop two prototype qa systems in chinese and swedish . we show what data is necessary to achieve this and also evaluate the performance of the two new systems using a translation of the trec 2003 factoid qa task . while performance for chinese and swedish is found to be lower than that for the more developed english and japanese systems we explain why this is the case and offer solutions for their improvement . all systems form the basis of our publicly accessible web-based multilingual qa system at http : //asked.jp .

experiments in preposition error detection educational testing service
evaluation and annotation are two of the greatest challenges in developing nlp instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers . past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output . in this paper , we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efficiently .

generating and interpreting referring expressions as belief state
planning-based approaches to reference provide a uniform treatment of linguistic decisions , from content selection to lexical choice . in this paper , we show how the issues of lexical ambiguity , vagueness , unspecific descriptions , ellipsis , and the interaction of subsective modifiers can be expressed using a belief-state planner modified to support context-dependent actions . because the number of distinct denotations it searches grows doublyexponentially with the size of the referential domain , we present representational and search strategies that make generation and interpretation tractable .

syntactic complexity measures for detecting mild cognitive impairment
we consider the diagnostic utility of various syntactic complexity measures when extracted from spoken language samples of healthy and cognitively impaired subjects . we examine measures calculated from manually built parse trees , as well as the same measures calculated from automatic parses . we show statistically significant differences between clinical subject groups for a number of syntactic complexity measures , and these differences are preserved with automatic parsing . different measures show different patterns for our data set , indicating that using multiple , complementary measures is important for such an application .

looking up phrase rephrasings via a pivot language
rephrasing text spans is a common task when revising a text . however , traditional dictionaries often can not provide direct assistance to writers in performing this task . in this article , we describe an approach to obtain a monolingual phrase lexicon using techniques used in statistical machine translation . a part to be rephrased is first translated into a pivot language , and then translated back into the original language . models for assessing fluency , meaning preservation and lexical divergence are used to rank possible rephrasings , and their relative weight can be tuned by the user so as to better address her needs . an evaluation shows that these models can be used successfully to select rephrasings that are likely to be useful to a writer .

a generative constituent-context model for improved grammar induction
we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts . parameter search with em produces higher quality analyses than previously exhibited by unsupervised systems , giving the best published unsupervised parsing results on the atis corpus . experiments on penn treebank sentences of comparable length show an even higher f1 of 71 % on nontrivial brackets . we compare distributionally induced and actual part-of-speech tags as input data , and examine extensions to the basic model . we discuss errors made by the system , compare the system to previous models , and discuss upper bounds , lower bounds , and stability for this task .

learning phrase-based spelling error models from clickthrough data
this paper explores the use of clickthrough data for query spelling correction . first , large amounts of query-correction pairs are derived by analyzing users ' query reformulation behavior encoded in the clickthrough data . then , a phrase-based error model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system . experiments are carried out on a human-labeled data set . results show that the system using the phrase-based error model outperforms significantly its baseline systems .

human evaluation of a german surface realisation ranker
in this paper we present a human-based evaluation of surface realisation alternatives . we examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models ( language model , loglinear model ) , as well as the naturalness of the strings chosen by the log-linear model . we also investigate to what extent preceding context has an effect on choice . we show that native speakers do accept quite some variation in word order , but there are also clearly factors that make certain realisation alternatives more natural .

chinese and japanese word segmentation using word-level and
in this paper , we present a hybrid method for chinese and japanese word segmentation . word-level information is useful for analysis of known words , while character-level information is useful for analysis of unknown words , and the method utilizes both these two types of information in order to effectively handle known and unknown words . experimental results show that this method achieves high overall accuracy in chinese and japanese word segmentation .

combining domain adaptation approaches for medical text translation
this paper explores a number of simple and effective techniques to adapt statistical machine translation ( smt ) systems in the medical domain . comparative experiments are conducted on large corpora for six language pairs . we not only compare each adapted system with the baseline , but also combine them to further improve the domain-specific systems . finally , we attend the wmt2014 medical summary sentence translation constrained task and our systems achieve the best bleu scores for czech-english , englishgerman , french-english language pairs and the second best bleu scores for reminding pairs .

dependency-based bracketing transduction grammar for statistical machine translation
in this paper , we propose a novel dependency-based bracketing transduction grammar for statistical machine translation , which converts a source sentence into a target dependency tree . different from conventional bracketing transduction grammar models , we encode target dependency information into our lexical rules directly , and then we employ two different maximum entropy models to determine the reordering and combination of partial dependency structures , when we merge two neighboring blocks . by incorporating dependency language model further , large-scale experiments on chinese-english task show that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases .

temporal analysis of language through neural language models
we provide a method for automatically detecting change in language across time through a chronologically trained neural language model . we train the model on the google books ngram corpus to obtain word vector representations specific to each year , and identify words that have changed significantly from 1900 to 2009. the model identifies words such as cell and gay as having changed during that time period . the model simultaneously identifies the specific years during which such words underwent change .

exploiting structured ontology to organize scattered online opinions
we study the problem of integrating scattered online opinions . for this purpose , we propose to exploit structured ontology to obtain well-formed relevant aspects to a topic and use them to organize scattered opinions to generate a structured summary . particularly , we focus on two main challenges in implementing this idea , ( 1 ) how to select the most useful aspects from a large number of aspects in the ontology and ( 2 ) how to order the selected aspects to optimize the readability of the structured summary . we propose and explore several methods for solving these challenges . experimental results on two different data sets ( us presidents and digital cameras ) show that the proposed methods are effective for selecting aspects that can represent the major opinions and for generating coherent ordering of aspects .

learning local content shift detectors from document-level information
information-oriented document labeling is a special document multi-labeling task where the target labels refer to a specific information instead of the topic of the whole document . these kind of tasks are usually solved by looking up indicator phrases and analyzing their local context to filter false positive matches . here , we introduce an approach for machine learning local content shifters which detects irrelevant local contexts using just the original document-level training labels . we handle content shifters in general , instead of learning a particular language phenomenon detector ( e.g . negation or hedging ) and form a single system for document labeling and content shift detection . our empirical results achieved 24 % error reduction compared to supervised baseline methods on three document labeling tasks .

formal language theory for natural language processing
this paper reports on a course whose aim is to introduce formal language theory to students with little formal background ( mostly linguistics students ) . the course was first taught at the european summer school for logic , language and information to a mixed audience of students , undergraduate , graduate and postgraduate , with various backgrounds . the challenges of teaching such a course include preparation of highly formal , mathematical material for students with relatively little formal background ; attracting the attention of students of different backgrounds ; preparing examples that will emphasize the practical importance of material which is basically theoretical ; and evaluation of students achievements .

an integrated architecture for generating parenthetical constructions
the aim of this research is to provide a principled account of the generation of embedded constructions ( called parentheticals ) and to implement the results in a natural language generation system . parenthetical constructions are frequently used in texts written in a good writing style and have an important role in text understanding . we propose a framework to model the rhetorical properties of parentheticals based on a corpus study and develop a unified natural language generation architecture which integrates syntax , semantics , rhetorical and document structure into a complex representation , which can be easily extended to handle parentheticals .

clause restructuring for smt not absolutely helpful
there are a number of systems that use a syntax-based reordering step prior to phrasebased statistical mt . an early work proposing this idea showed improved translation performance , but subsequent work has had mixed results . speculations as to cause have suggested the parser , the data , or other factors . we systematically investigate possible factors to give an initial answer to the question : under what conditions does this use of syntax help psmt

ltag dependency parsing with bidirectional incremental construction
in this paper , we first introduce a new architecture for parsing , bidirectional incremental parsing . we propose a novel algorithm for incremental construction , which can be applied to many structure learning problems in nlp . we apply this algorithm to ltag dependency parsing , and achieve significant improvement on accuracy over the previous best result on the same data set .

a pipeline model for bottom-up dependency parsing
we present a new machine learning framework for multi-lingual dependency parsing . the framework uses a linear , pipeline based , bottom-up parsing algorithm , with a look ahead local search that serves to make the local predictions more robust . as shown , the performance of the first generation of this algorithm is promising .

efficient and robust lfg parsing : sxlfg
in this paper , we introduce a new parser , called sxlfg , based on the lexicalfunctional grammars formalism ( lfg ) . we describe the underlying context-free parser and how functional structures are efficiently computed on top of the cfg shared forest thanks to computation sharing , lazy evaluation , and compact data representation . we then present various error recovery techniques we implemented in order to build a robust parser . finally , we offer concrete results when sxlfg is used with an existing grammar for french . we show that our parser is both efficient and robust , although the grammar is very ambiguous .

biases in predicting the human language model
we consider the prediction of three human behavioral measures lexical decision , word naming , and picture naming through the lens of domain bias in language modeling . contrasting the predictive ability of statistics derived from 6 different corpora , we find intuitive results showing that , e.g. , a british corpus overpredicts the speed with which an american will react to the words ward and duke , and that the google n-grams overpredicts familiarity with technology terms . this study aims to provoke increased consideration of the human language model by nlp practitioners : biases are not limited to differences between corpora ( i.e . train vs. test ) ; they can exist as well between corpora and the intended user of the resultant technology .

annotating and measuring temporal relations in texts
this paper focuses on the automated processing of temporal information in written texts , more specifically on relations between events introduced by verbs in finite clauses . while this problem has been largely studied from a theoretical point of view , it has very rarely been applied to real texts , if ever , with quantified results . the methodology required is still to be defined , even though there have been proposals in the strictly human annotation case . we propose here both a procedure to achieve this task and a way of measuring the results . we have been testing the feasibility of this on newswire articles , with promising results .

large-scale semantic networks : annotation and evaluation
we introduce a large-scale semantic-network annotation effort based on the mutlinet formalism . annotation is achieved via a process which incorporates several independent tools including a multinet graph editing tool , a semantic concept lexicon , a user-editable knowledge-base for semantic concepts , and a multinet parser . we present an evaluation metric for these semantic networks , allowing us to determine the quality of annotations in terms of inter-annotator agreement . we use this metric to report the agreement rates for a pilot annotation effort involving three annotators .

automatic domain assignment for word sense alignment
this paper reports on the development of a hybrid and simple method based on a machine learning classifier ( naive bayes ) , word sense disambiguation and rules , for the automatic assignment of wordnet domains to nominal entries of a lexicographic dictionary , the senso comune de mauro lexicon . the system obtained an f1 score of 0.58 , with a precision of 0.70. we further used the automatically assigned domains to filter out word sense alignments between multiwordnet and senso comune . this has led to an improvement in the quality of the sense alignments showing the validity of the approach for domain assignment and the importance of domain information for achieving good sense alignments .

a probabilistic generative model for an intermediate
we present a probabilistic model extension to the tesni`ere dependency structure ( tds ) framework formulated in ( sangati and mazza , 2009 ) . this representation incorporates aspects from both constituency and dependency theory . in addition , it makes use of junction structures to handle coordination constructions . we test our model on parsing the english penn wsj treebank using a re-ranking framework . this technique allows us to efficiently test our model without needing a specialized parser , and to use the standard evaluation metric on the original phrase structure version of the treebank . we obtain encouraging results : we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures , on all the evaluation metrics except for chunking .

automatic evaluation method for machine translation using
as described in this paper , we propose a new automatic evaluation method for machine translation using noun-phrase chunking . our method correctly determines the matching words between two sentences using corresponding noun phrases . moreover , our method determines the similarity between two sentences in terms of the noun-phrase order of appearance . evaluation experiments were conducted to calculate the correlation among human judgments , along with the scores produced using automatic evaluation methods for mt outputs obtained from the 12 machine translation systems in ntcir7 . experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency .

prominence and phrasing in spoken discourse processing
we review psycholinguistic research on the use of intonation in dialogue , focusing on our own recent work . in experiments using complex real-world tasks and nave speakers and listeners , we show that speakers reliably specific prosodic cues to signal their intensions , and that listeners use these cues to recognize syntactic and pragmatic aspects of discourse meaning .

subtree mining for relation extraction from wikipedia
in this study , we address the problem of extracting relations between entities from wikipedias english articles . our proposed method first anchors the appearance of entities in wikipedias articles using neither named entity recognizer ( ner ) nor coreference resolution tool . it then classifies the relationships between entity pairs using svm with features extracted from the web structure and subtrees mined from the syntactic structure of text . we evaluate our method on manually annotated data from actual wikipedia articles .

towards building kurdnet , the kurdish wordnet
in this paper we highlight the main challenges in building a lexical database for kurdish , a resource-scarce and diverse language . we also report on our effort in building the first prototype of kurdnet the kurdish wordnet along with a preliminary evaluation of its impact on kurdish information retrieval .

applying morphology generation models to machine translation
we improve the quality of statistical machine translation ( smt ) by applying models that predict word forms from their stems using extensive morphological and syntactic information from both the source and target languages . our inflection generation models are trained independently of the smt system . we investigate different ways of combining the inflection prediction component with the smt system by training the base mt system on fully inflected forms or on word stems . we applied our inflection generation models in translating english into two morphologically complex languages , russian and arabic , and show that our model improves the quality of smt over both phrasal and syntax-based smt systems according to bleu and human judgements .

joint incremental disfluency detection and dependency parsing
we present an incremental dependency parsing model that jointly performs disfluency detection . the model handles speech repairs using a novel non-monotonic transition system , and includes several novel classes of features . for comparison , we evaluated two pipeline systems , using state-of-the-art disfluency detectors . the joint model performed better on both tasks , with a parse accuracy of 90.5 % and 84.0 % accuracy at disfluency detection . the model runs in expected linear time , and processes over 550 tokens a second .

sentiment analyser for tweets - meets semeval
we give a brief overview of our system , sentiadaptron , a domain-sensitive and domain adaptable system for twitter analysis in tweets , and discuss performance on semeval ( in both the constrained and unconstrained scenarios ) , as well as implications arising from comparing the intra- and inter- domain performance on our twitter corpus .

automated tutoring dialogues for training in shipboard
this paper describes an application of state-of-the-art spoken language technology ( oaa/gemini/nuance ) to a new problem domain : engaging students in automated tutorial dialogues in order to evaluate and improve their performance in a training simulator .

intelligent selection of language model training data
we address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation . our approach is based on comparing the cross-entropy , according to domainspecific and non-domain-specifc language models , for each sentence of the text source used to produce the latter language model . we show that this produces better language models , trained on less data , than both random data selection and two other previously proposed methods .

from words to corpora : recognizing translation
this paper presents a technique for discovering translationally equivalent texts . it is comprised of the application of a matching algorithm at two different levels of analysis and a well-founded similarity score . this approach can be applied to any multilingual corpus using any kind of translation lexicon ; it is therefore adaptable to varying levels of multilingual resource availability . experimental results are shown on two tasks : a search for matching thirty-word segments in a corpus where some segments are mutual translations , and classification of candidate pairs of web pages that may or may not be translations of each other . the latter results compare competitively with previous , document-structure-based approaches to the same problem .

is there syntactic adaptation in language comprehension
in this paper we investigate the manner in which the human language comprehension system adapts to shifts in probability distributions over syntactic structures , given experimentally controlled experience with those structures . we replicate a classic reading experiment , and present a model of the behavioral data that implements a form of bayesian belief update over the course of the experiment .

from detecting errors to automatically correcting them
faced with the problem of annotation errors in part-of-speech ( pos ) annotated corpora , we develop a method for automatically correcting such errors . building on top of a successful error detection method , we first try correcting a corpus using two off-the-shelf pos taggers , based on the idea that they enforce consistency ; with this , we find some improvement . after some discussion of the tagging process , we alter the tagging model to better account for problematic tagging distinctions . this modification results in significantly improved performance , reducing the error rate of the corpus .

semi-supervised relation extraction with large-scale word clustering
we present a simple semi-supervised relation extraction system with large-scale word clustering . we focus on systematically exploring the effectiveness of different cluster-based features . we also propose several statistical methods for selecting clusters at an appropriate level of granularity . when training on different sizes of data , our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system .

building a hierarchically aligned chinese-english parallel treebank
we construct a hierarchically aligned chinese-english parallel treebank by manually doing word alignments and phrase alignments simultaneously on parallel phrase-based parse trees . the main innovation of our approach is that we leave words without a translation counterpart ( which are mostly language-particular function words ) unaligned on the word level , and locate and align the appropriate phrases which encapsulate them . in doing so , we harmonize word-level and phraselevel alignments . we show that this type of annotation can be performedwith high inter-annotator consistency and have both linguistic and engineering potentials .

learning to automatically solve algebra word problems
we present an approach for automatically learning to solve algebra word problems . our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text . the learning algorithm uses varied supervision , including either full equations or just the final answers . we evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset . this is , to our knowledge , the first learning result for this task .

a simple baseline for discriminating similar languages
this paper describes an approach to discriminating similar languages using word- and characterbased features , submitted as the queen mary university of london entry to the discriminating similar languages shared task . our motivation was to investigate how well a simple , datadriven , linguistically naive method could perform , in order to provide a baseline by which more linguistically complex or knowledge-rich approaches can be judged . using a standard supervised classifier with word and character n-grams as features , we achieved over 90 % accuracy in the test ; on fixing simple file handling and feature extraction bugs , this improved to over 95 % , comparable to the best submitted systems . similar accuracy is achieved using only word unigram features .

turn-taking cues in a human tutoring corpus
most spoken dialogue systems are still lacking in their ability to accurately model the complex process that is human turntaking . this research analyzes a humanhuman tutoring corpus in order to identify prosodic turn-taking cues , with the hopes that they can be used by intelligent tutoring systems to predict student turn boundaries . results show that while there was variation between subjects , three features were significant turn-yielding cues overall . in addition , a positive relationship between the number of cues present and the probability of a turn yield was demonstrated .

a finite-state model of georgian verbal morphology
georgian is a less commonly studied language with complex , non-concatenative verbal morphology . we present a computational model for generation and recognition of georgian verb conjugations , relying on the analysis of georgian verb structure as a word-level template . the model combines a set of finite-state transducers with a default inheritance mechanism.1

event extraction in a plot advice agent
in this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing . we focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words . the system automatically extracts events from the raw text , formalized as a sequence of temporally ordered predicate-arguments . these events are given to a machine-learner that produces a coarse-grained rating of the story . the results of the machine-learner and the extracted events are then used to generate fine-grained advice for the students .

finding variants of out-of-vocabulary words in arabic
transliteration of a word into another language often leads to multiple spellings . unless an information retrieval system recognises different forms of transliterated words , a significant number of documents will be missed when users specify only one spelling variant . using two different datasets , we evaluate several approaches to finding variants of foreign words in arabic , and show that the longest common subsequence ( lcs ) technique is the best overall .

opinion target extraction in chinese news comments
news comments on the web express readers attitudes or opinions about an event or object in the corresponding news article . and opinion target extraction from news comments is very important for many useful web applications . however , many sentences in the comments are irregular and informal , and sometimes the opinion targets are implicit . thus the task is very challenging and it has not been investigated yet . in this paper , we propose a new approach to uniformly extracting explicit and implicit opinion targets from news comments by using centering theory . the approach uses global information in news articles as well as contextual information in adjacent sentences of comments . our experimental results verify the effectiveness of the proposed approach .

perplexity on reduced corpora yahoo japan corporation
this paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint . based on the assumption that a corpus follows zipfs law , we derive tradeoff formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary . in addition , we show an approximate behavior of each formula under certain conditions . we verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora .

probabilistic human-computer trust handling and wolfgang minker
human-computer trust has shown to be a critical factor in influencing the complexity and frequency of interaction in technical systems . particularly incomprehensible situations in human-computer interaction may lead to a reduced users trust in the system and by that influence the style of interaction . analogous to human-human interaction , explaining these situations can help to remedy negative effects . in this paper we present our approach of augmenting task-oriented dialogs with selected explanation dialogs to foster the humancomputer trust relationship in those kinds of situations . we have conducted a webbased study testing the effects of different goals of explanations on the components of human-computer trust . subsequently , we show how these results can be used in our probabilistic trust handling architecture to augment pre-defined task-oriented dialogs .

unsupervised learning summarization templates from concise summaries
we here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages . we evaluate the two approaches in two different information extraction settings : monolingual and cross-lingual information extraction . the extraction systems are trained on auto-annotated summaries ( containing the induced concepts ) and evaluated on humanannotated documents . extraction results are promising , being close in performance to those achieved when the system is trained on human-annotated summaries .

instance based lexical entailment for ontology population
in this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text . the approach is fully unsupervised and based on kernel methods . we demonstrate the effectiveness of our technique largely surpassing both the random and most frequent baselines and outperforming current state-of-the-art unsupervised approaches on a benchmark ontology available in the literature .

reinforcement learning for mapping instructions to actions
in this paper , we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions . we assume access to a reward function that defines the quality of the executed actions . during training , the learner repeatedly constructs action sequences for a set of documents , executes those actions , and observes the resulting reward . we use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection . we apply our method to interpret instructions in two domains windows troubleshooting guides and game tutorials . our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples.1

separating disambiguation from composition in distributional semantics
most compositional-distributional models of meaning are based on ambiguous vector representations , where all the senses of a word are fused into the same vector . this paper provides evidence that the addition of a vector disambiguation step prior to the actual composition would be beneficial to the whole process , producing better composite representations . furthermore , we relate this issue with the current evaluation practice , showing that disambiguation-based tasks can not reliably assess the quality of composition . using a word sense disambiguation scheme based on the generic procedure of schtze ( 1998 ) , we first provide a proof of concept for the necessity of separating disambiguation from composition . then we demonstrate the benefits of an unambiguous system on a composition-only task .

online entropy-based model of lexical category acquisition
children learn a robust representation of lexical categories at a young age . we propose an incremental model of this process which efficiently groups words into lexical categories based on their local context using an information-theoretic criterion . we train our model on a corpus of childdirected speech from childes and show that the model learns a fine-grained set of intuitive word categories . furthermore , we propose a novel evaluation approach by comparing the efficiency of our induced categories against other category sets ( including traditional part of speech tags ) in a variety of language tasks . we show the categories induced by our model typically outperform the other category sets .

named entity transcription with pair n-gram models
we submitted results for each of the eight shared tasks . except for japanese name kanji restoration , which uses a noisy channel model , our standard run submissions were produced by generative long-range pair ngram models , which we mostly augmented with publicly available data ( either from ldc datasets or mined from wikipedia ) for the non-standard runs .

using phrasal patterns to identify discourse relations
this paper describes a system which identifies discourse relations between two successive sentences in japanese . on top of the lexical information previously proposed , we used phrasal pattern information . adding phrasal information improves the system 's accuracy 12 % , from 53 % to 65 % .

detecting change and emergence for multiword expressions
this work looks at a temporal aspect of multiword expressions ( mwes ) , namely that the behaviour of a given n-gram and its status as a mwe change over time . we propose a model in which context words have particular probabilities given a usage choice for an n-gram , and those usage choices have time dependent probabilities , and we put forward an expectationmaximisation technique for estimating the parameters from data with no annotation of usage choice . for a range of mwe usages of recent coinage , we evaluate whether the technique is able to detect the emerging usage .

extracting important sentences with support vector machines
extracting sentences that contain important information from a document is a form of text summarization . the technique is the key to the automatic generation of summaries similar to those written by humans . to achieve such extraction , it is important to be able to integrate heterogeneous pieces of information . one approach , parameter tuning by machine learning , has been attracting a lot of attention . this paper proposes a method of sentence extraction based on support vector machines ( svms ) . to confirm the methods performance , we conduct experiments that compare our method to three existing methods . results on the text summarization challenge ( tsc ) corpus show that our method offers the highest accuracy . moreover , we clarify the different features effective for extracting different document genres .

subsegmental language detection in celtic language text
this paper describes an experiment to perform language identification on a sub-sentence basis . the typical case of language identification is to detect the language of documents or sentences . however , it may be the case that a single sentence or segment contains more than one language . this is especially the case in texts where code switching occurs .

automatic gap-fill question generation from text books
in this paper , we present an automatic question generation system that can generate gap-fill questions for content in a document . gap-fill questions are fill-in-the-blank questions with multiple choices ( one correct answer and three distractors ) provided . the system finds the informative sentences from the document and generates gap-fill questions from them by first blanking keys from the sentences and then determining the distractors for these keys . syntactic and lexical features are used in this process without relying on any external resource apart from the information in the document . we evaluated our system on two chapters of a standard biology textbook and presented the results .

lithuanian dependency parsing with rich morphological features
we present the first statistical dependency parsing results for lithuanian , a morphologically rich language in the baltic branch of the indo-european family . using a greedy transition-based parser , we obtain a labeled attachment score of 74.7 with gold morphology and 68.1 with predicted morphology ( 77.8 and 72.8 unlabeled ) . we investigate the usefulness of different features and find that rich morphological features improve parsing accuracy significantly , by 7.5 percentage points with gold features and 5.6 points with predicted features . as expected , case is the single most important morphological feature , but virtually all available features bring some improvement , especially under the gold condition .

ollie : on-line learning for information extraction
this paper reports work aimed at developing an open , distributed learning environment , ollie , where researchers can experiment with different machine learning ( ml ) methods for information extraction . once the required level of performance is reached , the ml algorithms can be used to speed up the manual annotation process . ollie uses a browser client while data storage and ml training is performed on servers . the different ml algorithms use a unified programming interface ; the integration of new ones is straightforward .

chart mining-based lexical acquisition with precision grammars
in this paper , we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars . the general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks , and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars . as an illustration of the functionality of our proposed technique , we develop a lexical acquisition model for english verb particle constructions which operates over unlexicalised features mined from a partial parsing chart . the proposed technique is shown to outperform a state-of-the-art parser over the target task , despite being based on relatively simplistic features .

a latent variable model of synchronous parsing
we propose a solution to the challenge of the conll 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies . the submitted model yields 79.1 % macroaverage f1 performance , for the joint task , 86.9 % syntactic dependencies las and 71.0 % semantic dependencies f1 . a larger model trained after the deadline achieves 80.5 % macro-average f1 , 87.6 % syntactic dependencies las , and 73.1 % semantic dependencies f1 .

mining chinese-english parallel corpora from the web
parallel corpora are a crucial resource in research fields such as cross-lingual information retrieval and statistical machine translation , but only a few parallel corpora with high quality are publicly available nowadays . in this paper , we try to solve the problem by developing a system that can automatically mine high quality parallel corpora from the world wide web . the system contains a three-step process . the system uses a web spider to crawl certain hosts at first . then candidate parallel web page pairs are prepared from the downloaded page set . at last , each candidate pair is examined based on multiple standards . we develop novel strategies for the implementation of the system , which are then proved to be rather effective by the experiments towards a multilingual website .

speech-enabled hybrid multilingual translation for mobile devices
this paper presents an architecture and a prototype for speech-to-speech translation on android devices , based on gf ( grammatical framework ) . from the users point of view , the advantage is that the system works off-line and yet has a lean size ; it also gives , as a bonus , grammatical information useful for language learners . from the developers point of view , the advantage is the open architecture that permits the customization of the system to new languages and for special purposes . thus the architecture can be used for controlled-language-like translators that deliver very high quality , which is the traditional strength of gf . however , this paper focuses on a general-purpose system that allows arbitrary input . it covers eight languages .

adapting a lexicalized-grammar parser to contrasting domains
most state-of-the-art wide-coverage parsers are trained on newspaper text and suffer a loss of accuracy in other domains , making parser adaptation a pressing issue . in this paper we demonstrate that a ccg parser can be adapted to two new domains , biomedical text and questions for a qa system , by using manually-annotated training data at the pos and lexical category levels only . this approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain . we find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation .

markovian discriminative modeling for dialog state tracking
discriminative dialog state tracking has become a hot topic in dialog research community recently . compared to generative approach , it has the advantage of being able to handle arbitrary dependent features , which is very appealing . in this paper , we present our approach to the dstc2 challenge . we propose to use discriminative markovian models as a natural enhancement to the stationary discriminative models . the markovian structure allows the incorporation of transitional features , which can lead to more efficiency and flexibility in tracking user goal changes . results on the dstc2 dataset show considerable improvements over the baseline , and the effects of the markovian dependency is tested empirically .

using encyclopedic knowledge for automatic topic identification
this paper presents a method for automatic topic identification using an encyclopedic graph derived from wikipedia . the system is found to exceed the performance of previously proposed machine learning algorithms for topic identification , with an annotation consistency comparable to human annotations .

partial parse selection for robust deep processing
this paper presents an approach to partial parse selection for robust deep processing . the work is based on a bottom-up chart parser for hpsg parsing . following the definition of partial parses in ( kasper et al , 1999 ) , different partial parse selection methods are presented and evaluated on the basis of multiple metrics , from both the syntactic and semantic viewpoints . the application of the partial parsing in spontaneous speech texts processing shows promising competence of the method .

medieval arabic literature in grids and networks
this contribution deals with the use of quotations ( repeated n-grams ) in the works of medieval arabic literature . the analysis is based on a 420 millions of words historical corpus of arabic . based on repeated quotations from work to work , a network is constructed and used for interpretation of various aspects of arabic literature . two short case studies are presented , concentrating on the centrality and relevance of individual works , and the analysis of a time depth and resulting impact of a given work in various periods .

a deep learning approach to machine transliteration
in this paper we present a novel transliteration technique which is based on deep belief networks . common approaches use finite state machines or other methods similar to conventional machine translation . instead of using conventional nlp techniques , the approach presented here builds on deep belief networks , a technique which was shown to work well for other machine learning problems . we show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an arabic-english transliteration task .

assas-band , an affix-exception-list based urdu stemmer
both inflectional and derivational morphology lead to multiple surface forms of a word . stemming reduces these forms back to its stem or root , and is a very useful tool for many applications . there has not been any work reported on urdu stemming . the current work develops an urdu stemmer or assas-band and improves the performance using more precise affix based exception lists , instead of the conventional lexical lookup employed for developing stemmers in other languages . testing shows an accuracy of 91.2 % . further enhancements are also suggested .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

detecting relations in the gene regulation network
the bionlp shared task 2013 is organised to further advance the field of information extraction in biomedical texts . this paper describes our entry in the gene regulation network in bacteria ( grn ) part , for which our system finished in second place ( out of five ) . to tackle this relation extraction task , we employ a basic support vector machine framework . we discuss our findings in constructing local and contextual features , that augment our precision with as much as 7.5 % . we touch upon the interaction type hierarchy inherent in the problem , and the importance of the evaluation procedure to encourage exploration of that structure .

realistic grammar error simulation using markov logic
the development of dialog-based computerassisted language learning ( db-call ) systems requires research on the simulation of language learners . this paper presents a new method for generation of grammar errors , an important part of the language learner simulator . realistic errors are generated via markov logic , which provides an effective way to merge a statistical approach with expert knowledge about the grammar error characteristics of language learners . results suggest that the distribution of simulated grammar errors generated by the proposed model is similar to that of real learners . human judges also gave consistently close judgments on the quality of the real and simulated grammar errors .

machine translation as lexicalized parsing with hooks
we adapt the hook trick for speeding up bilexical parsing to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model with an n-gram language model . this dynamic programming technique yields lower complexity algorithms than have previously been described for an important class of translation models .

max-margin synchronous grammar induction for machine translation
traditional synchronous grammar induction estimates parameters by maximizing likelihood , which only has a loose relation to translation quality . alternatively , we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation , which directly optimizes translation quality measured by bleu . in the max-margin estimation of parameters , we only need to calculate viterbi translations . this further facilitates the incorporation of various non-local features that are defined on the target side . we test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system . experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 bleu points and is also better than previous max-likelihood estimation method .

a bootstrapping approach to named entity classification
this paper presents a new bootstrapping approach to named entity ( ne ) classification . this approach only requires a few common noun/pronoun seeds that correspond to the concept for the target ne type , e.g . he/she/man/woman for person ne . the entire bootstrapping procedure is implemented as training two successive learners : ( i ) a decision list is used to learn the parsing-based high precision ne rules ; ( ii ) a hidden markov model is then trained to learn string sequence-based ne patterns . the second learner uses the training corpus automatically tagged by the first learner . the resulting ne system approaches supervised ne performance for some ne types . the system also demonstrates intuitive support for tagging user-defined ne types . the differences of this approach from the co-training-based ne bootstrapping are also discussed .

an ordering of terms based on semantic
term selection methods typically employ a statistical measure to filter or weight terms . term expansion for ir may also depend on statistics , or use some other , non-metric method based on a lexical resource . at the same time , a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation . this paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure . this semantic order can be exploited by term weighting and term expansion methods .

ubc-as : a graph based unsupervised system
this paper describes a graph-based unsupervised system for induction and classification . the system performs a two stage graph based clustering where a cooccurrence graph is first clustered to compute similarities against contexts . the context similarity matrix is pruned and the resulting associated graph is clustered again by means of a random-walk type algorithm . the system relies on a set of parameters that have been tuned to fit the corpus data . the system has participated in tasks 2 and 13 of the semeval-2007 competition , on word sense induction and web people search , respectively , with mixed results .

fast and robust arabic error correction system
in this paper we describe the implementation of an arabic error correction system developed for the emnlp2014 shared task on automatic error correction for arabic text . we proposed a novel algorithm , where we find some correction rules and calculate their probability based on the training data , they we rank the correction rules , then we apply them on the text to maximize the overall fscore for the provided data . the system achieves and f-score of 0.6573 on the test data .

construction of structurally annotated spoken dialogue corpus
this paper describes the structural annotation of a spoken dialogue corpus . by statistically dealing with the corpus , the automatic acquisition of dialoguestructural rules is achieved . the dialogue structure is expressed as a binary tree and 789 dialogues consisting of 8150 utterances in the ciair speech corpus are annotated . to evaluate the scalability of the corpus for creating dialogue-structural rules , a dialogue parsing experiment was conducted .

real-time stochastic language generation for dialogue systems
this paper describes acorn , a sentence planner and surface realizer for dialogue systems . improvements to previous stochastic word-forest based approaches are described , countering recent criticism of this class of algorithms for their slow speed . an evaluation of the approach with semantic input shows runtimes of a fraction of a second and presents results that suggest it is also portable across domains .

using lda to detect semantically incoherent documents
detecting the semantic coherence of a document is a challenging task and has several applications such as in text segmentation and categorization . this paper is an attempt to distinguish between a semantically coherent true document and a randomly generated false document through topic detection in the framework of latent dirichlet analysis . based on the premise that a true document contains only a few topics and a false document is made up of many topics , it is asserted that the entropy of the topic distribution will be lower for a true document than that for a false document . this hypothesis is tested on several false document sets generated by various methods and is found to be useful for fake content detection applications .

annotated web as corpus united states naval
this paper presents a proposal to facilitate the use of the annotated web as corpus by alleviating the annotation bottleneck for corpus data drawn from the web . we describe a framework for large-scale distributed corpus annotation using peerto-peer ( p2p ) technology to meet this need . we also propose to annotate a large reference corpus in order to evaluate this framework . this will allow us to investigate the affordances offered by distributed techniques to ensure replicability of linguistic research based on web-derived corpora .

a game-theoretic approach to generating spatial descriptions
language is sensitive to both semantic and pragmatic effects . to capture both effects , we model language use as a cooperative game between two players : a speaker , who generates an utterance , and a listener , who responds with an action . specifically , we consider the task of generating spatial references to objects , wherein the listener must accurately identify an object described by the speaker . we show that a speaker model that acts optimally with respect to an explicit , embedded listener model substantially outperforms one that is trained to directly generate spatial descriptions .

a generalized-zero-preserving method for compact encoding of
constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice . join is the central operation any unification-based parser must support . we extend the traditional bit vector encoding , which represents join failure using the zero vector , to count any vector with less than a fixed number of one bits as failure . this allows non-joinable elements to share bits , resulting in a smaller vector size . a constraint solver is used to construct the encoding , and a variety of techniques are employed to find near-optimal solutions and handle timeouts . an evaluation is provided comparing the extended representation of failure with traditional bit vector techniques .

automatic construction of an english-chinese bilingual framenet
we propose a method of automatically constructing an english-chinese bilingual framenet where the english framenet lexical entries are linked to the appropriate chinese word senses . this resource can be used in machine translation and cross-lingual ir systems . we coerce the english framenet into chinese using a bilingual lexicon , frame context in framenet and taxonomy structure in hownet . our approach does not require any manual mapping between framenet and hownet semantic roles . evaluation results show that we achieve a promising 82 % average fmeasure for the most ambiguous lexical entries .

a graph model for unsupervised lexical acquisition
this paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms . the graph model is built by linking pairs of words which participate in particular syntactic relationships . we focus on the symmetric relationship between pairs of nouns which occur together in lists . an incremental cluster-building algorithm using this part of the graph achieves 82 % accuracy at a lexical acquisition task , evaluated against wordnet classes . the model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word .

improved word-level system combination for machine translation
recently , confusion network decoding has been applied in machine translation system combination . due to errors in the hypothesis alignment , decoding may result in ungrammatical combination outputs . this paper describes an improved confusion network based method to combine outputs from multiple mt systems . in this approach , arbitrary features may be added log-linearly into the objective function , thus allowing language model expansion and re-scoring . also , a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed . a generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including ter , bleu and meteor . the experiments using the 2005 arabic to english and chinese to english nist mt evaluation tasks show significant improvements in bleu scores compared to earlier confusion network decoding based methods .

extending the entity grid with entity-specic features
we extend the popular entity grid representation for local coherence modeling . the grid abstracts away information about the entities it models ; we add discourse prominence , named entity type and coreference features to distinguish between important and unimportant entities . we improve the best result for wsj document discrimination by 6 % .

extracting syntactic features from a korean treebank
in this paper , we present a system which can extract syntactic feature structures from a korean treebank ( sejong treebank ) to develop a feature-based lexicalized tree adjoining grammars .

identifying semantic roles using combinatory categorial grammar
we present a system for automatically identifying propbank-style semantic roles based on the output of a statistical parser for combinatory categorial grammar . this system performs at least as well as a system based on a traditional treebank parser , and outperforms it on core argument roles .

collective opinion target extraction in chinese microblogs
microblog messages pose severe challenges for current sentiment analysis techniques due to some inherent characteristics such as the length limit and informal writing style . in this paper , we study the problem of extracting opinion targets of chinese microblog messages . such fine-grained word-level task has not been well investigated in microblogs yet . we propose an unsupervised label propagation algorithm to address the problem . the opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets . topics in microblogs are identified by hashtags or using clustering algorithms . experimental results on chinese microblogs show the effectiveness of our framework and algorithms .

linking events and their participants in discourse
we describe the semeval-2010 shared task on linking events and their participants in discourse . this task is an extension to the classical semantic role labeling task . while semantic role labeling is traditionally viewed as a sentence-internal task , local semantic argument structures clearly interact with each other in a larger context , e.g. , by sharing references to specific discourse entities or events . in the shared task we looked at one particular aspect of cross-sentence links between argument structures , namely linking locally uninstantiated roles to their co-referents in the wider discourse context ( if such co-referents exist ) . this task is potentially beneficial for a number of nlp applications , such as information extraction , question answering or text summarization .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

pos error detection in automatically annotated corpora
recent work on error detection has shown that the quality of manually annotated corpora can be substantially improved by applying consistency checks to the data and automatically identifying incorrectly labelled instances . these methods , however , can not be used for automatically annotated corpora where errors are systematic and can not easily be identified by looking at the variance in the data . this paper targets the detection of pos errors in automatically annotated corpora , so-called silver standards , showing that by combining different measures sensitive to annotation quality we can identify a large part of the errors and obtain a substantial increase in accuracy .

sconeedit : a text-guided domain knowledge editor
we will demonstrate sconeedit , a new tool for exploring and editing knowledge bases ( kbs ) that leverages interaction with domain texts . the tool provides an annotated view of user-selected text , allowing a user to see which concepts from the text are in the kb and to edit the kb directly from this text view . alongside the text view , sconeedit provides a navigable kb view of the knowledge base , centered on concepts that appear in the text . this unified tool gives the user a text-driven way to explore a kb and add new knowledge .

finding non-local dependencies : beyond pattern matching
we describe an algorithm for recovering non-local dependencies in syntactic dependency structures . the patternmatching approach proposed by johnson ( 2002 ) for a similar task for phrase structure trees is extended with machine learning techniques . the algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment . evaluating the algorithm on the penn treebank shows an improvement of both precision and recall , compared to the results presented in ( johnson , 2002 ) .

preemptive information extraction using unrestricted relation discovery
we are trying to extend the boundary of information extraction ( ie ) systems . existing ie systems require a lot of time and human effort to tune for a new scenario . preemptive information extraction is an attempt to automatically create all feasible ie systems in advance without human intervention . we propose a technique called unrestricted relation discovery that discovers all possible relations from texts and presents them as tables . we present a preliminary system that obtains reasonably good results .

named entity transliterations from large comparable corpora
in this paper , we address the problem of mining transliterations of named entities ( nes ) from large comparable corpora . we leverage the empirical fact that multilingual news articles with similar news content are rich in named entity transliteration equivalents ( netes ) . our mining algorithm , mint , uses a cross-language document similarity model to align multilingual news articles and then mines netes from the aligned articles using a transliteration similarity model . we show that our approach is highly effective on 6 different comparable corpora between english and 4 languages from 3 different language families . furthermore , it performs substantially better than a state-of-the-art competitor .

morpho-syntactic clues for terminological processing in serbian
in this paper we discuss morpho-syntactic clues that can be used to facilitate terminological processing in serbian . a method ( called srce ) for automatic extraction of multiword terms is presented . the approach incorporates a set of generic morpho-syntactic filters for recognition of term candidates , a method for conflation of morphological variants and a module for foreign word recognition . morpho-syntactic filters describe general term formation patterns , and are implemented as generic regular expressions . the inner structure together with the agreements within term candidates are used as clues to discover the boundaries of nested terms . the results of the terminological processing of a textbook corpus in the domains of mathematics and computer science are presented .

exploring entity relations for named entity disambiguation
named entity disambiguation is the task of linking an entity mention in a text to the correct real-world referent predefined in a knowledge base , and is a crucial subtask in many areas like information retrieval or topic detection and tracking . named entity disambiguation is challenging because entity mentions can be ambiguous and an entity can be referenced by different surface forms . we present an approach that exploits wikipedia relations between entities co-occurring with the ambiguous form to derive a range of novel features for classifying candidate referents . we find that our features improve disambiguation results significantly over a strong popularity baseline , and are especially suitable for recognizing entities not contained in the knowledge base . our system achieves state-of-the-art results on the tac-kbp 2009 dataset .

active sample selection for named entity transliteration
this paper introduces a new method for identifying named-entity ( ne ) transliterations within bilingual corpora . current state-of-theart approaches usually require annotated data and relevant linguistic knowledge which may not be available for all languages . we show how to effectively train an accurate transliteration classifier using very little data , obtained automatically . to perform this task , we introduce a new active sampling paradigm for guiding and adapting the sample selection process . we also investigate how to improve the classifier by identifying repeated patterns in the training data . we evaluated our approach using english , russian and hebrew corpora .

a fast method for parallel document identification
we present a fast method to identify homogeneous parallel documents . the method is based on collecting counts of identical low-frequency words between possibly parallel documents . the candidate with the most shared low-frequency words is selected as the parallel document . the method achieved 99.96 % accuracy when tested on the europarl corpus of parliamentary proceedings , failing only in anomalous cases of truncated or otherwise distorted documents . while other work has shown similar performance on this type of dataset , our approach presented here is faster and does not require training . apart from proposing an efficient method for parallel document identification in a restricted domain , this paper furnishes evidence that parliamentary proceedings may be inappropriate for testing parallel document identification systems in general .

how is meaning grounded in dictionary definitions
meaning can not be based on dictionary definitions all the way down : at some point the circularity of definitions must be broken in some way , by grounding the meanings of certain words in sensorimotor categories learned from experience or shaped by evolution . this is the symbol grounding problem . we introduce the concept of a reachable set a larger vocabulary whose meanings can be learned from a smaller vocabulary through definition alone , as long as the meanings of the smaller vocabulary are themselves already grounded . we provide simple algorithms to compute reachable sets for any given dictionary .

multi-perspective question answering using the opqa corpus
we investigate techniques to support the answering of opinion-based questions . we first present the opqa corpus of opinion questions and answers . using the corpus , we compare and contrast the properties of fact and opinion questions and answers . based on the disparate characteristics of opinion vs. fact answers , we argue that traditional fact-based qa approaches may have difficulty in an mpqa setting without modification . as an initial step towards the development of mpqa systems , we investigate the use of machine learning and rule-based subjectivity and opinion source filters and show that they can be used to guide mpqa systems .

attribute selection for referring expression generation :
referring expression generation has recently been the subject of the first shared task challenge in nlg . in this paper , we analyse the systems that participated in the challenge in terms of their algorithmic properties , comparing new techniques to classic ones , based on results from a new human task-performance experiment and from the intrinsic measures that were used in the challenge . we also consider the relationship between different evaluation methods , showing that extrinsic taskperformance experiments and intrinsic evaluation methods yield results that are not significantly correlated . we argue that this highlights the importance of including extrinsic evaluation methods in comparative nlg evaluations .

learning summary content units with topic modeling
in the field of multi-document summarization , the pyramid method has become an important approach for evaluating machine-generated summaries . the method is based on the manual annotation of text spans with the same meaning in a set of human model summaries . in this paper , we present an unsupervised , probabilistic topic modeling approach for automatically identifying such semantically similar text spans . our approach reveals some of the structure of model summaries and identifies topics that are good approximations of the summary content units ( scu ) used in the pyramid method . our results show that the topic model identifies topic-sentence associations that correspond to the contributors of scus , suggesting that the topic modeling approach can generate a viable set of candidate scus for facilitating the creation of pyramids .

elaine u dhonnchadha , josef van genabith
this paper looks at how computational linguistics ( cl ) and natural language processing ( nlp ) resources can be deployed in computerassisted language learning ( call ) materials for primary school learners . we draw a broad distinction between cl and nlp technology and briefly review the use of cl/nlp in e-learning in general , how it has been deployed in call to date and specifically in the primary school context . we outline how cl/nlp resources can be used in a project to teach irish and german to primary school children in ireland . this paper focuses on the use of finite state morphological analysis ( fst ) resources for irish and part of speech ( pos ) taggers for german .

clustering comparable corpora for bilingual lexicon extraction
we study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora . we introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus , and finally preserves most of the vocabulary of the original corpus . our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches .

vocabulary decomposition for estonian open vocabulary speech
speech recognition in many morphologically rich languages suffers from a very high out-of-vocabulary ( oov ) ratio . earlier work has shown that vocabulary decomposition methods can practically solve this problem for a subset of these languages . this paper compares various vocabulary decomposition approaches to open vocabulary speech recognition , using estonian speech recognition as a benchmark . comparisons are performed utilizing large models of 60000 lexical items and smaller vocabularies of 5000 items . a large vocabulary model based on a manually constructed morphological tagger is shown to give the lowest word error rate , while the unsupervised morphology discovery method morfessor baseline gives marginally weaker results . only the morfessor-based approach is shown to adequately scale to smaller vocabulary sizes .

classifying particle semantics in english verb-particle constructions
previous computational work on learning the semantic properties of verb-particle constructions ( vpcs ) has focused on their compositionality , and has left unaddressed the issue of which meaning of the component words is being used in a given vpc . we develop a feature space for use in classification of the sense contributed by the particle in a vpc , and test this on vpcs using the particle up . the features that capture linguistic properties of vpcs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test vpcs .

unal : discriminating between literal and figurative
in this paper we describe the system used to participate in the sub task 5b in the phrasal semantics challenge ( task 5 ) in semeval 2013. this sub task consists in discriminating literal and figurative usage of phrases with compositional and non-compositional meanings in context . the proposed approach is based on part-of-speech tags , stylistic features and distributional statistics gathered from the same development-training-test text collection . the system obtained a relative improvement in accuracy against the most-frequentclass baseline of 49.8 % in the unseen contexts ( lexsample ) setting and 8.5 % in unseen phrases ( allwords ) .

generating expressions that refer to visible objects
we introduce a novel algorithm for generating referring expressions , informed by human and computer vision and designed to refer to visible objects . our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs . expressions generated using this method are often overspecified and may be underspecified , akin to expressions produced by people . we call such expressions identifying descriptions . the algorithm outperforms the well-known incremental algorithm ( dale and reiter , 1995 ) and the graphbased algorithm ( krahmer et al , 2003 ; viethen et al , 2008 ) across a variety of images in two domains . we additionally motivate an evaluation method for referring expression generation that takes the proposed algorithms non-determinism into account .

improving mid-range reordering using templates of factors
we extend the factored translation model ( koehn and hoang , 2007 ) to allow translations of longer phrases composed of factors such as pos and morphological tags to act as templates for the selection and reordering of surface phrase translation . we also reintroduce the use of alignment information within the decoder , which forms an integral part of decoding in the alignment template system ( och , 2002 ) , into phrase-based decoding . results show an increase in translation performance of up to 1.0 % bleu for out-of-domain frenchenglish translation . we also show how this method compares and relates to lexicalized reordering .

deriving generalized knowledge from corpora using wordnet
benjamin van durme , phillip michalak and lenhart k. schubert department of computer science university of rochester rochester , ny 14627 , usa abstract existing work in the extraction of commonsense knowledge from text has been primarily restricted to factoids that serve as statements about what may possibly obtain in the world . we present an approach to deriving stronger , more general claims by abstracting over large sets of factoids . our goal is to coalesce the observed nominals for a given predicate argument into a few predominant types , obtained as wordnet synsets . the results can be construed as generically quantified sentences restricting the semantic type of an argument position of a predicate .

consentcanvas : automatic texturing for improved readability
we present consentcanvas , a system which structures and texturizes end-user license agreement ( eula ) documents to be more readable . the system aims to help users better understand the terms under which they are providing their informed consent . consentcanvas receives unstructured text documents as input and uses unsupervised natural language processing methods to embellish the source document using a linked stylesheet . unlike similar usable security projects which employ summarization techniques , our system preserves the contents of the source document , minimizing the cognitive and legal burden for both the end user and the licensor . our system does not require a corpus for training .

large-scale computation of distributional similarities for queries
we present a large-scale , data-driven approach to computing distributional similarity scores for queries . we contrast this to recent webbased techniques which either require the offline computation of complete phrase vectors , or an expensive on-line interaction with a search engine interface . independent of the computational advantages of our approach , we show empirically that our technique is more effective at ranking query alternatives that the computationally more expensive technique of using the results from a web search engine .

cityu-hif : wsd with human-informed feature preference
this paper describes our word sense disambiguation ( wsd ) system participating in the semeval-2007 tasks . the core system is a fully supervised system based on a nave bayes classifier using multiple knowledge sources . toward a larger goal of incorporating the intrinsic nature of individual target words in disambiguation , thus introducing a cognitive element in automatic wsd , we tried to fine-tune the results obtained from the core system with humaninformed feature preference , and compared it with automatic feature selection as commonly practised in statistical wsd . despite the insignificant improvement observed in this preliminary attempt , more systematic analysis remains to be done for a cognitively plausible account of the factors underlying the lexical sensitivity of wsd , which would inform and enhance the development of wsd systems in return .

stochastic contextual edit distance and probabilistic fsts
string similarity is most often measured by weighted or unweighted edit distance d ( x , y ) . ristad and yianilos ( 1998 ) defined stochastic edit distancea probability distribution p ( y | x ) whose parameters can be trained from data . we generalize this so that the probability of choosing each edit operation can depend on contextual features . we show how to construct and train a probabilistic finite-state transducer that computes our stochastic contextual edit distance . to illustrate the improvement from conditioning on context , we model typos found in social media text .

tree-to-string alignment template for statistical machine translation
we present a novel translation model based on tree-to-string alignment template ( tat ) which describes the alignment between a source parse tree and a target string . a tat is capable of generating both terminals and non-terminals and performing reordering at both low and high levels . the model is linguistically syntaxbased because tats are extracted automatically from word-aligned , source side parsed parallel texts . to translate a source sentence , we first employ a parser to produce a source parse tree and then apply tats to transform the tree into a target string . our experiments show that the tat-based model significantly outperforms pharaoh , a state-of-the-art decoder for phrase-based models .

predicting the semantic compositionality of prefix verbs
in many applications , replacing a complex word form by its stem can reduce sparsity , revealing connections in the data that would not otherwise be apparent . in this paper , we focus on prefix verbs : verbs formed by adding a prefix to an existing verb stem . a prefix verb is considered compositional if it can be decomposed into a semantically equivalent expression involving its stem . we develop a classifier to predict compositionality via a range of lexical and distributional features , including novel features derived from web-scale ngram data . results on a new annotated corpus show that prefix verb compositionality can be predicted with high accuracy . our system also performs well when trained and tested on conventional morphological segmentations of prefix verbs .

alignment by soft projection of syntactic dependencies
many syntactic models in machine translation are channels that transform one tree into another , or synchronous grammars that generate trees in parallel . we present a newmodel of the translation process : quasi-synchronous grammar ( qg ) . given a source-language parse tree t1 , a qg defines a monolingual grammar that generates translations of t1 . the trees t2 allowed by this monolingual grammar are inspired by pieces of substructure in t1 and aligned to t1 at those points . we describe experiments learning quasi-synchronous context-free grammars from bitext . as with other monolingual language models , we evaluate the crossentropy of qgs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence . when evaluated on a word alignment task , qg matches standard baselines .

nukti : english-inuktitut word alignment system description
machine translation ( mt ) as well as other bilingual applications strongly rely on word alignment . efficient alignment techniques have been proposed but are mainly evaluated on pairs of languages where the notion of word is mostly clear . we concentrated our effort on the english-inuktitut word alignment shared task and report on two approaches we implemented and a combination of both .

toward opinion summarization : linking the sources
we target the problem of linking source mentions that belong to the same entity ( source coreference resolution ) , which is needed for creating opinion summaries . in this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution , apply a state-of-the-art coreference resolution approach to the transformed data , and evaluate on an available corpus of manually annotated opinions .

simple features for chinese word sense disambiguation
in this paper we report on our experiments on automatic word sense disambiguation using a maximum entropy approach for both english and chinese verbs . we compare the difficulty of the sensetagging tasks in the two languages and investigate the types of contextual features that are useful for each language . our experimental results suggest that while richer linguistic features are useful for english wsd , they may not be as beneficial for chinese .

extending the coverage of a valency dictionary
information on subcategorization and selectional restrictions is very important for natural language processing in tasks such as monolingual parsing , accurate rule-based machine translation and automatic summarization . however , adding this detailed information to a valency dictionary is both time consuming and costly . in this paper we present a method of assigning valency information and selectional restrictions to entries in a bilingual dictionary , based on information in an existing valency dictionary . the method is based on two assumptions : words with similar meaning have similar subcategorization frames and selectional restrictions ; and words with the same translations have similar meanings . based on these assumptions , new valency entries are constructed for words in a plain bilingual dictionary , using entries with similar source-language meaning and the same target-language translations . we evaluate the effects of various measures of similarity .

speaker-independent context update rules for dialogue management
this paper describes a dialogue management system in which an attempt is made to factor out a declarative theory of context updates in dialogue from a procedural theory of generating and interpreting utterances in dialogue .

a machine learning approach to acronym generation
this paper presents a machine learning approach to acronym generation . we formalize the generation process as a sequence labeling problem on the letters in the definition ( expanded form ) so that a variety of markov modeling approaches can be applied to this task . to construct the data for training and testing , we extracted acronym-definition pairs from medline abstracts and manually annotated each pair with positional information about the letters in the acronym . we have built an memm-based tagger using this training data set and evaluated the performance of acronym generation . experimental results show that our machine learning method gives significantly better performance than that achieved by the standard heuristic rule for acronym generation and enables us to obtain multiple candidate acronyms together with their likelihoods represented in probability values .

a knowledge-driven approach to text meaning processing
our goal is to be able to answer questions about text that go beyond facts explicitly stated in the text , a task which inherently requires extracting a deep level of meaning from that text . our approach treats meaning processing fundamentally as a modeling activity , in which a knowledge base of common-sense expectations guides interpretation of text , and text suggests which parts of the knowledge base might be relevant . in this paper , we describe our ongoing investigations to develop this approach into a usable method for meaning processing .

refining generative language models using discriminative learning
we propose a new approach to language modeling which utilizes discriminative learning methods . our approach is an iterative one : starting with an initial language model , in each iteration we generate 'false ' sentences from the current model , and then train a classifier to discriminate between them and sentences from the training corpus . to the extent that this succeeds , the classifier is incorporated into the model by lowering the probability of sentences classified as false , and the process is repeated . we demonstrate the effectiveness of this approach on a natural language corpus and show it provides an 11.4 % improvement in perplexity over a modified kneser-ney smoothed trigram .

raytheon bbn technologies , cambridge , ma
distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models . however , in some cases a small amount of human labeled data is available . in this paper , we demonstrate how a state-of-theart multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database . experiments show that our approach achieves a statistically significant increase of 13.5 % in f-score and 37 % in area under the precision recall curve .

identification of genia events using multiple classifiers
we describe our system to extract genia events that was developed for the bionlp 2013 shared task . our system uses a supervised information extraction platform based on support vector machines ( svm ) and separates the process of event classification into multiple stages . for each event type the svm parameters are adjusted and feature selection carried out . we find that this optimisation improves the performance of our approach . overall our system achieved the highest precision score of all systems and was ranked 6th of 10 participating systems on f-measure ( strict matching ) .

directl : a language-independent approach to transliteration
we present directl : an online discriminative sequence prediction model that employs a many-to-many alignment between target and source . our system incorporates input segmentation , target character prediction , and sequence modeling in a unified dynamic programming framework . experimental results suggest that directl is able to independently discover many of the language-specific regularities in the training data .

isis participation in the romanian-english alignment task
we discuss results on the shared task of romanian-english word alignment . the baseline technique is that of symmetrizing two word alignments automatically generated using ibm model 4. a simple vocabulary reduction technique results in an improvement in performance . we also report on a new alignment model and a new training algorithm based on alternating maximization of likelihood with minimization of error rate .

rali : smt shared task system description
thanks to the profusion of freely available tools , it recently became fairly easy to built a statistical machine translation ( smt ) engine given a bitext . the expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another . we report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the smt sharedtask .

skill inference with personal and skill connections
personal skill information on social media is at the core of many interesting applications . in this paper , we propose a factor graph based approach to automatically infer skills from personal profile incorporated with both personal and skill connections . we first extract personal connections with similar academic and business background ( e.g . co-major , co-university , and co-corporation ) . we then extract skill connections between skills from the same person . to well integrate various kinds of connections , we propose a joint prediction factor graph ( jpfg ) model to collectively infer personal skills with help of personal connection factor , skill connection factor , besides the normal textual attributes . evaluation on a large-scale dataset from linkedin.com validates the effectiveness of our approach .

application adaptive electronic dictionary with intelligent interface
the paper presents an electronic dictionary that can be adapted to the needs of different nlp applications . it suggests some ways to save on software customisation and acquisition effort through an intelligent developer interface . the emphasis is made on the flexibility of data representation , handling and access speed .

mapping concrete entities from parole-simple-clips to italwordnet
this paper describes a work in progress aiming at linking the two largest italian lexical-semantic databases italwordnet and parole-simple-clips . the adopted linking methodology , the software tool devised and implemented for this purpose and the results of the first mapping phase regarding 1storderentities are illustrated here .

quest - a translation quality estimation framework
we describe quest , an open source framework for machine translation quality estimation . the framework allows the extraction of several quality indicators from source segments , their translations , external resources ( corpora , language models , topic models , etc . ) , as well as language tools ( parsers , part-of-speech tags , etc . ) . it also provides machine learning algorithms to build quality estimation models . we benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms .

adding redundant features for crfs-based sentence sentiment
in this paper , we present a novel method based on crfs in response to the two special characteristics of contextual dependency and label redundancy in sentence sentiment classification . we try to capture the contextual constraints on sentence sentiment using crfs . through introducing redundant labels into the original sentimental label set and organizing all labels into a hierarchy , our method can add redundant features into training for capturing the label redundancy . the experimental results prove that our method outperforms the traditional methods like nb , svm , maxent and standard chain crfs . in comparison with the cascaded model , our method can effectively alleviate the error propagation among different layers and obtain better performance in each layer .

kernel-based pronoun resolution with structured syntactic knowledge
syntactic knowledge is important for pronoun resolution . traditionally , the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically . in the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution . specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier . in this way , our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features . the experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task .

incremental parsing with parallel multiple context-free grammars
parallel multiple context-free grammar ( pmcfg ) is an extension of context-free grammar for which the recognition problem is still solvable in polynomial time . we describe a new parsing algorithm that has the advantage to be incremental and to support pmcfg directly rather than the weaker mcfg formalism . the algorithm is also top-down which allows it to be used for grammar based word prediction .

discourse level opinion interpretation intelligent systems program
this work proposes opinion frames as a representation of discourse-level associations which arise from related opinion topics . we illustrate how opinion frames help gather more information and also assist disambiguation . finally we present the results of our experiments to detect these associations .

active learning-based elicitation for semi-supervised word alignment
semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments . motivated by standard active learning query sampling frameworks like uncertainty- , margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task . our experiments show that by active selection of uncertain and informative links , we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner .

semantic role chunking combining complementary syntactic views
this paper describes a semantic role labeling system that uses features derived from different syntactic views , and combines them within a phrase-based chunking paradigm . for an input sentence , syntactic constituent structure parses are generated by a charniak parser and a collins parser . semantic role labels are assigned to the constituents of each parse using support vector machine classifiers . the resulting semantic role labels are converted to an iob representation . these iob representations are used as additional features , along with flat syntactic chunks , by a chunking svm classifier that produces the final srl output . this strategy for combining features from three different syntactic views gives a significant improvement in performance over roles produced by using any one of the syntactic views individually .

active learning with constrained topic model
latent dirichlet allocation ( lda ) is a topic modeling tool that automatically discovers topics from a large collection of documents . it is one of the most popular text analysis tools currently in use . in practice however , the topics discovered by lda do not always make sense to end users . in this extended abstract , we propose an active learning framework that interactively and iteratively acquires user feedback to improve the quality of learned topics . we conduct experiments to demonstrate its effectiveness with simulated user input on a benchmark dataset .

formatting time-aligned asr transcripts for readability
we address the problem of formatting the output of an automatic speech recognition ( asr ) system for readability , while preserving wordlevel timing information of the transcript . our system enriches the asr transcript with punctuation , capitalization and properly written dates , times and other numeric entities , and our approach can be applied to other formatting tasks . the method we describe combines hand-crafted grammars with a class-based language model trained on written text and relies on weighted finite state transducers ( wfsts ) for the preservation of start and end time of each word .

towards domain-independent deep linguistic processing :
in this paper we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and at the same time ensuring maintainability and re-usability of deep lexicalised grammars . using the error mining techniques proposed in ( van noord , 2004 ) we show very convincingly that the main hindrance to portability of deep lexicalised grammars to domains other than the ones originally developed in , as well as to robustness of systems using such grammars is low lexical coverage . to this effect , we develop linguistically-driven methods that use detailed morphosyntactic information to automatically enhance the performance of deep lexicalised grammars maintaining at the same time their usually already achieved high linguistic quality .

hierarchical text classification with latent concepts
recently , hierarchical text classification has become an active research topic . the essential idea is that the descendant classes can share the information of the ancestor classes in a predefined taxonomy . in this paper , we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively . then , we propose a variant passive-aggressive ( pa ) algorithm for hierarchical text classification with latent concepts . experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms .

building test suites for uima components
we summarize our experiences building a comprehensive suite of tests for a statistical natural language processing toolkit , cleartk . we describe some of the challenges we encountered , introduce a software project that emerged from these efforts , summarize our resulting test suite , and discuss some of the lessons learned .

improving decoding generalization for tree-to-string translation
to address the parse error issue for tree-tostring translation , this paper proposes a similarity-based decoding generation ( sdg ) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding . experiments on chinese-english translation demonstrated that our approach can achieve a significant improvement over the standard method , and has little impact on decoding speed in practice . our approach is very easy to implement , and can be applied to other paradigms such as tree-to-tree models .

distributional composition using higher-order dependency vectors
this paper concerns how to apply compositional methods to vectors based on grammatical dependency relation vectors . we demonstrate the potential of a novel approach which uses higher-order grammatical dependency relations as features . we apply the approach to adjective-noun compounds with promising results in the prediction of the vectors for ( held-out ) observed phrases .

leveraging domain-independent information in semantic parsing
semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols . motivated by the observation that interpretation can be decomposed into domain-dependent and independent components , we suggest a novel interpretation model , which augments a domain dependent model with abstract information that can be shared by multiple domains . our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains .

duluth-wsi : senseclusters applied to the
the duluth-wsi systems in semeval-2 built word cooccurrence matrices from the task test data to create a second order cooccurrence representation of those test instances . the senses of words were induced by clustering these instances , where the number of clusters was automatically predicted . the duluth-mix system was a variation of wsi that used the combination of training and test data to create the co-occurrence matrix . the duluth-r system was a series of random baselines .

language resources for a network-based dictionary
in order to facilitate the use of a dictionary for language production and language learning we propose the construction of a new network-based electronic dictionary along the lines of zock ( 2002 ) . however , contrary to zock who proposes just a paradigmatic network with information about the various ways in which words are similar we would like to present several existing language resources ( lrs ) which will be integrated in such a network resulting in more linguistic levels than one with paradigmatically associated words . we argue that just as the mental lexicon exhibits various , possibly interwoven layers of networks , electronic lrs containing syntagmatic , morphological and phonological information need to be integrated into an associative electronic dictionary .

learning a lexical simplifier using wikipedia
in this paper we introduce a new lexical simplification approach . we extract over 30k candidate lexical simplifications by identifying aligned words in a sentencealigned corpus of english wikipedia with simple english wikipedia . to apply these rules , we learn a feature-based ranker using svm rank trained on a set of labeled simplifications collected using amazons mechanical turk . using human simplifications for evaluation , we achieve a precision of 76 % with changes in 86 % of the examples .

lexical query paraphrasing for document retrieval
we describe a mechanism for the generation of lexical paraphrases of queries posed to an internet resource . these paraphrases are generated using wordnet and part-of-speech information to propose synonyms for the content words in the queries . statistical information , obtained from a corpus , is then used to rank the paraphrases . we evaluated our mechanism using 404 queries whose answers reside in the la times subset of the trec-9 corpus . there was a 14 % improvement in performance when paraphrases were used for document retrieval .

interactively exploring a machine translation model
this paper describes a method of interactively visualizing and directing the process of translating a sentence . the method allows a user to explore a model of syntax-based statistical machine translation ( mt ) , to understand the models strengths and weaknesses , and to compare it to other mt systems . using this visualization method , we can find and address conceptual and practical problems in an mt system . in our demonstration at acl , new users of our tool will drive a syntaxbased decoder for themselves .

model-portability experiments for textual temporal analysis
we explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains : we generate additional training examples by substituting temporal expression words with potential synonyms . we explore using synonyms both from wordnet and from the latent words language model ( lwlm ) , which predicts synonyms in context using an unsupervised approach . we evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from tempeval 2010 , reuters and wikipedia . we find that the lwlm provides substan-tial improvements on the reuters corpus , and smaller improvements on the wikipedia corpus . we find that wordnet alne never improves performance , though intersecting the examples from the lwlm and wordnet provides more stable results for wikipedia .

teaching translation tools over the web
this is a description of an ongoing training effort to teach the use of translation tools like translation memories or terminology databases in live online seminars over the internet .

probabilistic generation of weather forecast texts
this paper reports experiments in which pcru a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space is used to semi-automatically create several versions of a weather forecast text generator . the generators are evaluated in terms of output quality , development time and computational efficiency against ( i ) human forecasters , ( ii ) a traditional handcrafted pipelined nlg system , and ( iii ) a halogen-style statistical generator . the most striking result is that despite acquiring all decision-making abilities automatically , the best pcru generators receive higher scores from human judges than forecasts written by experts .

a multi-teraflop constituency parser using gpus
constituency parsing with rich grammars remains a computational challenge . graphics processing units ( gpus ) have previously been used to accelerate cky chart evaluation , but gains over cpu parsers were modest . in this paper , we describe a collection of new techniques that enable chart evaluation at close to the gpus practical maximum speed ( a teraflop ) , or around a half-trillion rule evaluations per second . net parser performance on a 4-gpu system is over 1 thousand length30 sentences/second ( 1 trillion rules/sec ) , and 400 general sentences/second for the berkeley parser grammar . the techniques we introduce include grammar compilation , recursive symbol blocking , and cache-sharing .

arabizi detection and conversion to arabic
arabizi is arabic text that is written using latin characters . arabizi is used to present both modern standard arabic ( msa ) or arabic dialects . it is commonly used in informal settings such as social networking sites and is often with mixed with english . in this paper we address the problems of : identifying arabizi in text and converting it to arabic characters . we used word and sequence-level features to identify arabizi that is mixed with english . we achieved an identification accuracy of 98.5 % . as for conversion , we used transliteration mining with language modeling to generate equivalent arabic text . we achieved 88.7 % conversion accuracy , with roughly a third of errors being spelling and morphological variants of the forms in ground truth .

inducing discourse connectives from parallel texts
discourse connectives ( e.g . however , because ) are terms that explicitly express discourse relations in a coherent text . while a list of discourse connectives is useful for both theoretical and empirical research on discourse relations , few languages currently possess such a resource . in this article , we propose a new method that exploits parallel corpora and collocation extraction techniques to automatically induce discourse connectives . our approach is based on identifying candidates and ranking them using log-likelihood ratio . then , it relies on several filters to filter the list of candidates , namely : word-alignment , pos patterns , and syntax . our experiment to induce french discourse connectives from an english-french parallel text shows that syntactic filter achieves a much higher map value ( 0.39 ) than the other filters , when compared with lexconn resource .

classifying temporal relations with simple features
approaching temporal link labelling as a classification task has already been explored in several works . however , choosing the right feature vectors to build the classification model is still an open issue , especially for event-event classification , whose accuracy is still under 50 % . we find that using a simple feature set results in a better performance than using more sophisticated features based on semantic role labelling and deep semantic parsing . we also investigate the impact of extracting new training instances using inverse relations and transitive closure , and gain insight into the impact of this bootstrapping methodology on classifying the full set of tempeval-3 relations .

corrective modeling for non-projective dependency parsing
we present a corrective model for recovering non-projective dependency structures from trees generated by state-of-theart constituency-based parsers . the continuity constraint of these constituencybased parsers makes it impossible for them to posit non-projective dependency trees . analysis of the types of dependency errors made by these parsers on a czech corpus show that the correct governor is likely to be found within a local neighborhood of the governor proposed by the parser . our model , based on a maxent classifier , improves overall dependency accuracy by .7 % ( a 4.5 % reduction in error ) with over 50 % accuracy for non-projective structures .

part of speech tagging in context
we present a new hmm tagger that exploits context on both sides of a word to be tagged , and evaluate it in both the unsupervised and supervised case . along the way , we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging , noting that published results to date have not been comparable across corpora or lexicons . observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms , we present a method of hmm training that improves accuracy when training of lexical probabilities is unstable . finally , we show how this new tagger achieves state-of-the-art results in a supervised , non-training intensive framework .

text mining for automatic image tagging
this paper introduces several extractive approaches for automatic image tagging , relying exclusively on information mined from texts . through evaluations on two datasets , we show that our methods exceed competitive baselines by a large margin , and compare favorably with the stateof-the-art that uses both textual and image features .

improved tree-to-string transducer for machine translation
we propose three enhancements to the treeto-string ( tts ) transducer for machine translation : first-level expansion-based normalization for tts templates , a syntactic alignment framework integrating the insertion of unaligned target words , and subtree-based ngram model addressing the tree decomposition probability . empirical results show that these methods improve the performance of a tts transducer based on the standard bleu4 metric . we also experiment with semantic labels in a tts transducer , and achieve improvement over our baseline system .

phrasefix : statistical post-editing of tectomt
we present two english-to-czech systems that took part in the wmt 2013 shared task : tectomt and phrasefix . the former is a deep-syntactic transfer-based system , the latter is a more-or-less standard statistical post-editing ( spe ) applied on top of tectomt . in a brief survey , we put spe in context with other system combination techniques and evaluate spe vs. another simple system combination technique : using synthetic parallel data from tectomt to train a statistical mt system ( smt ) . we confirm that phrasefix ( spe ) improves the output of tectomt , and we use this to analyze errors in tectomt . however , we also show that extending data for smt is more effective .

improving word alignment using word similarity
we show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used . in this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data . we present an extension to word alignment models that exploits word similarity . our experiments , in both large-scale and resourcelimited settings , show improvements in word alignment tasks as well as translation tasks .

answer generation with temporal data integration
in this paper , we propose an approach for content determination and surface generation of answers in a question-answering system on the web . the content determination is based on a coherence rate which takes into account coherence with other potential answers . answer generation is made through the use of classical techniques and templates and is based on a certainty degree .

linguistically debatable or just plain wrong
in linguistic annotation projects , we typically develop annotation guidelines to minimize disagreement . however , in this position paper we question whether we should actually limit the disagreements between annotators , rather than embracing them . we present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages . this points to an underlying ambiguity rather than random errors . moreover , a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors . specifically , we show that even in the absence of annotation guidelines only 2 % of annotator choices are linguistically unmotivated .

multiword units in an mt lexicon
multiword units significantly contribute to the robustness of mt systems as they reduce the inevitable ambiguity inherent in word to word matching . the paper focuses on a relatively little studied kind of mw units which are partially fixed and partially productive . in fact , mw units will be shown to form a continuum between completely frozen expression where the lexical elements are specified at the level of particular word forms and those which are produced by syntactic rules defined in terms of general part of speech categories . the paper will argue for the use of local grammars proposed by maurice gross to capture the productive regularity of mw units and will illustrate a uniform implementation of them in the nooj grammar development framework .

investigations on event evolution in tdt
topic detection and tracking approaches monitor broadcast news in order to spot new , previously unreported events and to track the development of the previously spotted ones . the dynamical nature of the events makes the use of state-of-the-art methods difficult . we present a new topic definition that has potential to model evolving events . we also discuss incorporating ontologies into the similarity measures of the topics , and illustrate a dynamic hierarchy that decreases the exhaustive computation performed in the tdt process . this is mainly work-in-progress .

qualities of eventiveness language computer corporation
events are not a discrete linguistic phenomenon . different verbal and nominal predicates express different degrees of eventiveness . in this paper we analyze the qualities that contribute to the overall eventiveness of a predicate , that is , what makes a predicate an event . we provide an in-depth analysis of seven key qualities , along with experimental assessments demonstrating their contributions . we posit that these qualities are an important part of a functional working definition of events .

computing logical form on regulatory texts
the computation of logical form has been proposed as an intermediate step in the translation of sentences to logic . logical form encodes the resolution of scope ambiguities . in this paper , we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form , called abstract syntax trees ( asts ) . the main step in computing asts is to order scope-taking operators . a learning model for ranking is adapted for this ordering . we design features by studying the problem of comparing the scope of one operator to another . the scope comparisons are used to compute asts , with an f-score of 90.6 % on the set of ordering decisons .

rule-based translation with statistical phrase-based post-editing
this article describes a machine translation system based on an automatic post-editing strategy : initially translate the input text into the target-language using a rule-based mt system , then automatically post-edit the output using a statistical phrase-based system . an implementation of this approach based on the systran and portage mt systems was used in the shared task of the second workshop on statistical machine translation . experimental results on the test data of the previous campaign are presented .

reducing the need for double annotation
the quality of annotated data is crucial for supervised learning . to eliminate errors in single annotated data , a second round of annotation is often used . however , is it absolutely necessary to double annotate every example we show that it is possible to reduce the amount of the second round of annotation by more than half without sacrificing the performance .

acquisition system for arabic noun morphology
many papers have discussed different aspects of arabic verb morphology . some of them used patterns ; others used patterns and affixes . but very few have discussed arabic noun morphology particularly for nouns that are not derived from verbs . in this paper we describe a learning system that can analyze arabic nouns to produce their morphological information and their paradigms with respect to both gender and number using a rule base that uses suffix analysis as well as pattern analysis . the system utilizes user-feedback to classify the noun and identify the group that it belongs to .

several directions for minority languages computerization
less than 1 % of the languages spoken in the world are correctly computerized : spell checkers , hyphenation , machine translation are still lacking for the others . in this paper , we present several directions that may help the computerization of minority languages as well as two projects where we apply some of these directions to the lao language .

a discriminative latent variable chinese segmenter
conventional approaches to chinese word segmentation treat the problem as a characterbased tagging task . recently , semi-markov models have been applied to the problem , incorporating features based on complete words . in this paper , we propose an alternative , a latent variable model , which uses hybrid information based on both word sequences and character sequences . we argue that the use of latent variables can help capture long range dependencies and improve the recall on segmenting long words , e.g. , named-entities . experimental results show that this is indeed the case . with this improvement , evaluations on the data of the second sighan cws bakeoff show that our system is competitive with the best ones in the literature .

vector space semantics with frequency-driven motifs
traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items . in this work , we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs . the framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents , and circumvents some problems of data sparsity by design . we design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable , and linguistically disambiguated . hellinger pca embeddings learnt using the framework show competitive results on empirical tasks .

machine translation and its philosophical accounts
this paper attempts to explore the interrelation between philosophical accounts of language and respective technological developments in the field of human language technologies . in doing so , it focuses on the interaction between analytical philosophy and machine translation development , trying to draw the emerging methodological analogies .

mehmet ali yatbaz ahmet engin ural
this paper describes the two algorithms we developed for the conll 2008 shared task joint learning of syntactic and semantic dependencies . both algorithms start parsing the sentence using the same syntactic parser . the first algorithm uses machine learning methods to identify the semantic dependencies in four stages : identification and labeling of predicates , identification and labeling of arguments . the second algorithm uses a generative probabilistic model , choosing the semantic dependencies that maximize the probability with respect to the model . a hybrid algorithm combining the best stages of the two algorithms attains 86.62 % labeled syntactic attachment accuracy , 73.24 % labeled semantic dependency f1 and 79.93 % labeled macro f1 score for the combined wsj and brown test sets1 .

multi-modal question-answering : questions without keyboards
this paper describes our work to allow players in a virtual world to pose questions without relying on textual input . our approach is to create enhanced virtual photographs by annotating them with semantic information from the 3d environments scene graph . the player can then use these annotated photos to interact with inhabitants of the world through automatically generated queries that are guaranteed to be relevant , grammatical and unambiguous . while the range of queries is more limited than a text input system would permit , in the gaming environment that we are exploring these limitations are offset by the practical concerns that make text input inappropriate .

bilingual lexicon generation using non-aligned signatures
bilingual lexicons are fundamental resources . modern automated lexicon generation methods usually require parallel corpora , which are not available for most language pairs . lexicons can be generated using non-parallel corpora or a pivot language , but such lexicons are noisy . we present an algorithm for generating a high quality lexicon from a noisy one , which only requires an independent corpus for each language . our algorithm introduces non-aligned signatures ( nas ) , a cross-lingual word context similarity score that avoids the over-constrained and inefficient nature of alignment-based methods . we use nas to eliminate incorrect translations from the generated lexicon . we evaluate our method by improving the quality of noisy spanish-hebrew lexicons generated from two pivot english lexicons . our algorithm substantially outperforms other lexicon generation methods .

dependency-based syntacticsemantic analysis with propbank and
this paper presents our contribution in the closed track of the 2008 conll shared task ( surdeanu et al , 2008 ) . to tackle the problem of joint syntacticsemantic analysis , the system relies on a syntactic and a semantic subcomponent . the syntactic model is a bottom-up projective parser using pseudo-projective transformations , and the semantic model uses global inference mechanisms on top of a pipeline of classifiers . the complete syntacticsemantic output is selected from a candidate pool generated by the subsystems . the system achieved the top score in the closed challenge : a labeled syntactic accuracy of 89.32 % , a labeled semantic f1 of 81.65 , and a labeled macro f1 of 85.49 .

spoken interactive odqa system : spiqa
we have been investigating an interactive approach for open-domain qa ( odqa ) and have constructed a spoken interactive odqa system , spiqa . the system derives disambiguating queries ( dqs ) that draw out additional information . to test the efficiency of additional information requested by the dqs , the system reconstructs the users initial question by combining the addition information with question . the combination is then used for answer extraction . experimental results revealed the potential of the generated dqs .

improving word alignment quality using morpho-syntactic
in this paper , we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models . existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other . we propose a method which explicitly takes into account such interdependencies during the em training of the statistical alignment models . the evaluation is done by comparing the obtained viterbi alignments with a manually annotated reference alignment . the improvements of the alignment quality compared to the , to our knowledge , best system are reported on the german-english verbmobil corpus .

normalized compression distance based measures for
we present the mt-ncd and mt-mncd machine translation evaluation metrics as submission to the machine translation evaluation shared task ( metricsmatr 2010 ) . the metrics are based on normalized compression distance ( ncd ) , a general information theoretic measure of string similarity , and evaluated against human judgments from the wmt08 shared task . the experiments show that 1 ) our metric improves correlation to human judgments by using flexible matching , 2 ) segment replication is effective , and 3 ) our ncd-inspired method for multiple references indicates improved results . generally , the proposed mt-ncd and mt-mncd methods correlate competitively with human judgments compared to commonly used machine translations evaluation metrics , for instance , bleu .

understanding students explanations in geometry tutoring
precise natural language understanding is needed in geometry tutoring to accurately determine the semantic content of students explanations . the paper presents an nlu system developed in the context of the geometry explanation tutor . the system combines unification-based syntactic processing with description logics based semantics to achieve the necessary accuracy level . solutions to specific semantic problems dealing with equivalence of semantic representations are described . experimental results on classification accuracy are also presented .

the fourth workshop on machine translation
this paper describes the techniques we explored to improve the translation of news text in the german-english and hungarian-english tracks of the wmt09 shared translation task . beginning with a convention hierarchical phrase-based system , we found benefits for using word segmentation lattices as input , explicit generation of beginning and end of sentence markers , minimum bayes risk decoding , and incorporation of a feature scoring the alignment of function words in the hypothesized translation . we also explored the use of monolingual paraphrases to improve coverage , as well as co-training to improve the quality of the segmentation lattices used , but these did not lead to improvements .

guitar-based pronominal anaphora resolution in bengali
this paper attempts to use an off-the-shelf anaphora resolution ( ar ) system for bengali . the language specific preprocessing modules of guitar ( v3.0.3 ) are identified and suitably designed for bengali . anaphora resolution module is also modified or replaced in order to realize different configurations of guitar . performance of each configuration is evaluated and experiment shows that the off-the-shelf ar system can be effectively used for indic languages .

casmacat : a computer-assisted translation workbench
casmacat is a modular , web-based translation workbench that offers advanced functionalities for computer-aided translation and the scientific study of human translation : automatic interaction with machine translation ( mt ) engines and translation memories ( tm ) to obtain raw translations or close tm matches for conventional post-editing ; interactive translation prediction based on an mt engines search graph , detailed recording and replay of edit actions and translators gaze ( the latter via eye-tracking ) , and the support of e-pen as an alternative input device . the system is open source sofware and interfaces with multiple mt systems .

direct orthographical mapping for machine transliteration
machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications . in this paper , a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping ( dom ) between two different languages is presented . under this framework , a joint source-channel transliteration model , also called n-gram transliteration model ( ngram tm ) , is further proposed to model the transliteration process . we evaluate the proposed methods through several transliteration/backtransliteration experiments for english/chinese and english/japanese language pairs . our study reveals that the proposed method not only reduces an extensive system development effort but also improves the transliteration accuracy significantly .

towards technology-assisted co-construction with communication partners
in this paper , we examine the idea of technology-assisted co-construction , where the communication partner of an aac user can make guesses about the intended messages , which are included in the users word completion/prediction interface . we run some human trials to simulate this new interface concept , with subjects predicting words as the users intended message is being generated in real time with specified typing speeds . results indicate that people can provide substantial keystroke savings by providing word completion or prediction , but that the savings are not as high as n-gram language models . interestingly , the language model and human predictions are complementary in certain key ways humans doing a better job in some circumstances on contextually salient nouns . we discuss implications of the enhanced coconstruction interface for real-time message generation in aac direct selection devices .

joint inference for fine-grained opinion extraction
this paper addresses the task of finegrained opinion extraction the identification of opinion-related entities : the opinion expressions , the opinion holders , and the targets of the opinions , and the relations between opinion expressions and their targets and holders . most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner , where the interdependencies among different extraction stages are not captured . we propose a joint inference model that leverages knowledge from predictors that optimize subtasks of opinion extraction , and seeks a globally optimal solution . experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction .

ontology-based distinction between polysemy and homonymy
we consider the problem of distinguishing polysemous from homonymous nouns . this distinction is often taken for granted , but is seldom operationalized in the shape of an empirical model . we present a first step towards such a model , based on wordnet augmented with ontological classes provided by corelex . this model provides a polysemy index for each noun which ( a ) , accurately distinguishes between polysemy and homonymy ; ( b ) , supports the analysis that polysemy can be grounded in the frequency of the meaning shifts shown by nouns ; and ( c ) , improves a regression model that predicts when the one-sense-per-discourse hypothesis fails .

extracting parallel fragments from comparable corpora
building nlg systems , in particular statistical ones , requires parallel data ( paired inputs and outputs ) which do not generally occur naturally . in this paper , we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the web . we describe our comparable corpus of data and texts relating to british hills and the techniques for extracting paired input/output fragments we have developed so far .

phonological context approximation and homophone treatment
this paper describes our systems participating in the news 2009 machine transliteration shared task . two runs were submitted for the english-chinese track . the system for the standard run is based on graphemic approximation of local phonological context . the one for the non-standard run is based on parallel modelling of sound and tone patterns for treating homophones in chinese . official results show that both systems stand in the mid range amongst all participating systems .

refining the most frequent sense baseline
we refine the most frequent sense baseline for word sense disambiguation using a number of novel word sense disambiguation techniques . evaluating on the s-3 english all words task , our combined system focuses on improving every stage of word sense disambiguation : starting with the lemmatization and part of speech tags used , through the accuracy of the most frequent sense baseline , to highly targeted individual systems . our supervised systems include a ranking algorithm and a wikipedia similarity measure .

dependency forest for statistical machine translation
we propose a structure called dependency forest for statistical machine translation . a dependency forest compactly represents multiple dependency trees . we develop new algorithms for extracting string-todependency rules and training dependency language models . our forest-based string-to-dependency system obtains significant improvements ranging from 1.36 to 1.46 bleu points over the tree-based baseline on the nist 2004/2005/2006 chinese-english test sets .

learning extraction patterns for subjective expressions
this paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective ( opinionated ) expressions . high-precision classifiers label unannotated data to automatically create a large training set , which is then given to an extraction pattern learning algorithm . the learned patterns are then used to identify more subjective sentences . the bootstrapping process learns many subjective patterns and increases recall while maintaining high precision .

using domain-specific verbs for term classification
in this paper we present an approach to term classification based on verb complementation patterns . the complementation patterns have been automatically learnt by combining information found in a corpus and an ontology , both belonging to the biomedical domain . the learning process is unsupervised and has been implemented as an iterative reasoning procedure based on a partial order relation induced by the domain-specific ontology . first , term recognition was performed by both looking up the dictionary of terms listed in the ontology and applying the c/nc-value method . subsequently , domain-specific verbs were automatically identified in the corpus . finally , the classes of terms typically selected as arguments for the considered verbs were induced from the corpus and the ontology . this information was used to classify newly recognised terms . the precision of the classification method reached 64 % .

lattice desegmentation for statistical machine translation
morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( smt ) involving morphologically complex languages . when translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder . in this paper , we expand our translation options by desegmenting n-best lists or lattices . our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs , which allows for inclusion of features related to the desegmentation process , as well as an unsegmented language model ( lm ) . we investigate this technique in the context of english-to-arabic and english-to-finnish translation , showing significant improvements in translation quality over desegmentation of 1-best decoder outputs .

chinese word segmentation model using bootstrapping
we participate in the cips-sighan2010 bake-off task of chinese word segmentation . unlike the previous bakeoff series , the purpose of the bakeoff 2010 is to test the crossdomain performance of chinese segmentation model . this paper summarizes our approach and our bakeoff results . we mainly propose to use 2 statistics to increase the oov recall and use bootstrapping strategy to increase the overall f score . as the results shows , the approach proposed in the paper does help , both of the oov recall and the overall f score are improved .

tiedmixture language modeling in continuous space
this paper presents a new perspective to the language modeling problem by moving the word representations and modeling into the continuous space . in a previous work we introduced gaussian-mixture language model ( gmlm ) and presented some initial experiments . here , we propose tied-mixture language model ( tmlm ) , which does not have the model parameter estimation problems that gmlm has . tmlm provides a great deal of parameter tying across words , hence achieves robust parameter estimation . as such , tmlm can estimate the probability of any word that has as few as two occurrences in the training data . the speech recognition experiments with the tmlm show improvement over the word trigram model .

learning latent personas of film characters
we present two latent variable models for learning character types , or personas , in film , in which a persona is defined as a set of mixtures over latent lexical classes . these lexical classes capture the stereotypical actions of which a character is the agent and patient , as well as attributes by which they are described . as the first attempt to solve this problem explicitly , we also present a new dataset for the text-driven analysis of film , along with a benchmark testbed to help drive future work in this area .

porting statistical parsers with data-defined kernels
previous results have shown disappointing performance when porting a parser trained on one domain to another domain where only a small amount of data is available . we propose the use of data-defined kernels as a way to exploit statistics from a source domain while still specializing a parser to a target domain . a probabilistic model trained on the source domain ( and possibly also the target domain ) is used to define a kernel , which is then used in a large margin classifier trained only on the target domain . with a svm classifier and a neural network probabilistic model , this method achieves improved performance over the probabilistic model alone .

co-regression for cross-language review rating prediction
the task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings . in this paper , we aim to investigate a more challenging task of crosslanguage review rating prediction , which makes use of only rated reviews in a source language ( e.g . english ) to predict the rating scores of unrated reviews in a target language ( e.g . german ) . we propose a new coregression algorithm to address this task by leveraging unlabeled reviews . evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results .

named entity recognition with bilingual constraints
different languages contain complementary cues about entities , which can be used to improve named entity recognition ( ner ) systems . we propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple integer linear program , which encourages entity tags to agree via bilingual constraints . bilingual ner experiments on the large ontonotes 4.0 chinese-english corpus show that the proposed method can improve strong baselines for both chinese and english . in particular , chinese performance improves by over 5 % absolute f1 score . we can then annotate a large amount of bilingual text ( 80k sentence pairs ) using our method , and add it as uptraining data to the original monolingual ner training corpus . the chinese model retrained on this new combined dataset outperforms the strong baseline by over 3 % f1 score .

multiword lexical acquisition and dictionary formalization
in this paper , we present the current state of development of a large-scale lexicon built at label1 for portuguese . we will concentrate on multiword expressions ( mwe ) , particularly on multiword nouns , ( i ) illustrating their most relevant morphological features , and ( ii ) pointing out the methods and techniques adopted to generate the inflected forms from lemmas . moreover , we describe a corpus-based aproach for the acquisition of new multiword nouns , which led to a significant enlargement of the existing lexicon . evaluation results concerning lexical coverage in the corpus are also discussed .

rebanking ccgbank for improved np interpretation
once released , treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena . instead , separate resources are created to address such problems . in this paper we show how to improve the quality of a treebank , by integrating resources and implementing improved analyses for specific constructions . we demonstrate this rebanking process by creating an updated version of ccgbank that includes the predicate-argument structure of both verbs and nouns , basenp brackets , verb-particle constructions , and restrictive and non-restrictive nominal modifiers ; and evaluate the impact of these changes on a statistical parser .

a categorial variation database for english
we describe our approach to the construction and evaluation of a large-scale database called catvar which contains categorial variations of english lexemes . due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications . thus , the research reported herein overlaps heavily with that of the machine-translation , lexicon-construction , and information-retrieval communities . we apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard . this evaluation reveals that the categorial database achieves a high degree of precision and recall . additionally , we demonstrate that the database improves on the linkability of porter stemmer by over 30 % .

reordering model for forest-to-string machine translation
in this paper , we present a novel extension of a forest-to-string machine translation system with a reordering model . we predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest . our approach naturally deals with the ambiguity present in the input parse forest , but , at the same time , takes into account only the parts of the input forest used by the current translation hypothesis . the method provides improvement from 0.6 up to 1.0 point measured by ( ter bleu ) /2 metric .

cross-lingual textual entailment for content synchronization
this paper presents the second round of the task on cross-lingual textual entailment for content synchronization , organized within semeval-2013 . the task was designed to promote research on semantic inference over texts written in different languages , targeting at the same time a real application scenario . participants were presented with datasets for different language pairs , where multi-directional entailment relations ( forward , backward , bidirectional , no entailment ) had to be identified . we report on the training and test data used for evaluation , the process of their creation , the participating systems ( six teams , 61 runs ) , the approaches adopted and the results achieved .

word similarity metrics and multilateral comparison
phylogenetic analyses of languages need to explicitly address whether the languages under consideration are related to each other at all . recently developed permutation tests allow this question to be explored by testing whether words in one set of languages are significantly more similar to those in another set of languages when paired up by semantics than when paired up at random . seven different phonetic similarity metrics are implemented and evaluated on their effectiveness within such multilateral comparison systems when deployed to detect genetic relations among the indo-european and uralic language families .

linguistic problems based on text corpora
the paper is focused on self-contained linguistic problems based on text corpora . we argue that corpus-based problems differ from traditional linguistic problems because they make it possible to represent language variation . furthermore , they often require basic statistical thinking from the students . the practical value of using data obtained from text corpora for teaching linguistics through linguistic problems is shown .

classifier combination for contextual idiom detection
we propose a novel unsupervised approach for distinguishing literal and non-literal use of idiomatic expressions . our model combines an unsupervised and a supervised classifier . the former bases its decision on the cohesive structure of the context and labels training data for the latter , which can then take a larger feature space into account . we show that a combination of both classifiers leads to significant improvements over using the unsupervised classifier alone .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

cross-lingual slot filling from comparable corpora
this paper introduces a new task of crosslingual slot filling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language . it is a very challenging task which suffers from both information extraction and machine translation errors . in this paper we analyze the types of errors produced by five different baseline approaches , and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora . without using any additional labeled data this new approach obtained 38.5 % relative improvement in precision and 86.7 % relative improvement in recall over several state-of-the-art approaches . the ultimate system outperformed monolingual slot filling pipelines built on much larger monolingual corpora .

scaling distributional similarity to large corpora
accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words . however , the nave nearestneighbour approach to comparing context vectors extracted from large corpora scales poorly ( o ( n2 ) in the vocabulary size ) . in this paper , we compare several existing approaches to approximating the nearestneighbour search for distributional similarity . we investigate the trade-off between efficiency and accuracy , and find that sash ( houle and sakuma , 2005 ) provides the best balance .

encoding mwes in a conceptual lexicon
the proposed paper reports on work in progress aimed at the development of a conceptual lexicon of modern greek ( mg ) and the encoding of mwes in it . morphosyntactic and semantic properties of these expressions were specified formally and encoded in the lexicon . the resulting resource will be applicable for a number of nlp applications .

adaptive language modeling for word prediction
we present the development and tuning of a topic-adapted language model for word prediction , which improves keystroke savings over a comparable baseline . we outline our plans to develop and integrate style adaptations , building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts .

regularized least-squares classification for word sense
the paper describes rlsc-lin and rlsccomb systems which participated in the senseval-3 english lexical sample task . these systems are based on regularized least-squares classification ( rlsc ) learning method . we describe the reasons of choosing this method , how we applied it to word sense disambiguation , what results we obtained on senseval1 , senseval-2 and senseval-3 data and discuss some possible improvements .

improving classification-based natural language understanding with
although data-driven techniques are commonly used for natural language understanding in dialogue systems , their efficacy is often hampered by the lack of appropriate annotated training data in sufficient amounts . we present an approach for rapid and cost-effective annotation of training data for classification-based language understanding in conversational dialogue systems . experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation .

a comparison between dialog corpora acquired
in this paper , we test the applicability of a stochastic user simulation technique to generate dialogs which are similar to real human-machine spoken interactions . to do so , we present a comparison between two corpora employing a comprehensive set of evaluation measures . the first corpus was acquired from real interactions of users with a spoken dialog system , whereas the second was generated by means of the simulation technique , which decides the next user answer taking into account the previous user turns , the last system answer and the objective of the dialog .

cognitively salient relations for multilingual lexicography
providing sets of semantically related words in the lexical entries of an electronic dictionary should help language learners quickly understand the meaning of the target words . relational information might also improve memorisation , by allowing the generation of structured vocabulary study lists . however , an open issue is which semantic relations are cognitively most salient , and should therefore be used for dictionary construction . in this paper , we present a concept description elicitation experiment conducted with german and italian speakers . the analysis of the experimental data suggests that there is a small set of concept-classdependent relation types that are stable across languages and robust enough to allow discrimination across broad concept domains . our further research will focus on harvesting instantiations of these classes from corpora .

rh : a retro hybrid parser
contemporary parser research is , to a large extent , focused on statistical parsers and deep-unification-based parsers . this paper describes an alternative , hybrid architecture in which an atn-like parser , augmented by many preference tests , builds on the results of a fast chunker . the combination is as efficient as most stochastic parsers , and accuracy is close and continues to improve . these results raise questions about the practicality of deep unification for symbolic parsing .

dependency-based sentence alignment for multiple document
in this paper , we describe a method of automatic sentence alignment for building extracts from abstracts in automatic summarization research . our method is based on two steps . first , we introduce the dependency tree path ( dtp ) . next , we calculate the similarity between dtps based on the esk ( extended string subsequence kernel ) , which considers sequential patterns . by using these procedures , we can derive one-to-many or many-to-one correspondences among sentences . experiments using different similarity measures show that dtp consistently improves the alignment accuracy and that esk gives the best performance .

cognates can improve statistical translation models
we report results of experiments aimed at improving the translation quality by incorporating the cognate information into translation models . the results confirm that the cognate identification approach can improve the quality of word alignment in bitexts without the need for extra resources .

natural language analysis of patent claims
we propose a nlp methodology for analyzing patent claims that combines symbolic grammar formalisms with dataintensive methods while enhancing analysis robustness . the output of our analyzer is a shallow interlingual representation that captures both the structure and content of a claim text . the methodology can be used in any patent-related application , such as machine translation , improving readability of patent claims , information retrieval , extraction , summarization , generation , etc . the methodology should be universal in the sense that it could be applied to any language , other parts of patent documentation and text as such .

multidimensional markup and heterogeneous linguistic resources
the paper discusses two topics : firstly an approach of using multiple layers of annotation is sketched out . regarding the xml representation this approach is similar to standoff annotation . a second topic is the use of heterogeneous linguistic resources ( e.g. , xml annotated documents , taggers , lexical nets ) as a source for semiautomatic multi-dimensional markup to resolve typical linguistic issues , dealing with anaphora resolution as a case study.1

automatic evaluation of summaries using n-gram
following the recent adoption by the machine translation community of automatic evaluation using the bleu/nist scoring process , we conduct an in-depth study of a similar idea for evaluating summaries . the results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations , based on various statistical metrics ; while direct application of the bleu evaluation procedure does not always give good results .

an annotation scheme for citation function
we study the interplay of the discourse structure of a scientific argument with formal citations . one subproblem of this is to classify academic citations in scientific articles according to their rhetorical function , e.g. , as a rival approach , as a part of the solution , or as a flawed approach that justifies the current research . here , we introduce our annotation scheme with 12 categories , and present an agreement study .

contrast and variability in gene names
we studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain . based on our findings , we developed heuristics for mapping weakly matching gene names to their official gene names . we then tested these heuristics against a large body of medline abstracts , and found that using these heuristics can increase recall , with varying levels of precision . our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes , proteins , rna , and a variety of other referents for performing entity identification with high precision .

language identification of names with svms
the task of identifying the language of text or utterances has a number of applications in natural language processing . language identification has traditionally been approached with character-level language models . however , the language model approach crucially depends on the length of the text in question . in this paper , we consider the problem of language identification of names . we show that an approach based on svms with n-gram counts as features performs much better than language models . we also experiment with applying the method to pre-process transliteration data for the training of separate models .

domain-specific coreference resolution with lexicalized features
most coreference resolvers rely heavily on string matching , syntactic properties , and semantic attributes of words , but they lack the ability to make decisions based on individual words . in this paper , we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution . we show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures .

alignment link projection using transformation-based learning
we present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them . by adapting transformationbased learning to the problem of word alignment , we project new alignment links from already existing links , using features such as pos tags . we show that our alignment link projection approach yields a significantly lower alignment error rate than that of the best performing alignment system ( 22.6 % relative reduction on englishspanish data and 23.2 % relative reduction on english-chinese data ) .

a syntactic and lexical-based discourse segmenter
we present a syntactic and lexically based discourse segmenter ( slseg ) that is designed to avoid the common problem of over-segmenting text . segmentation is the first step in a discourse parser , a system that constructs discourse trees from elementary discourse units . we compare slseg to a probabilistic segmenter , showing that a conservative approach increases precision at the expense of recall , while retaining a high f-score across both formal and informal texts .

a finite-state morphological grammar of hebrew
morphological analysis is a crucial component of several natural language processing tasks , especially for languages with a highly productive morphology , where stipulating a full lexicon of surface forms is not feasible . we describe hamsah ( haifa morphological system for analyzing hebrew ) , a morphological processor for modern hebrew , based on finite-state linguistically motivated rules and a broad coverage lexicon . the set of rules comprehensively covers the morphological , morpho-phonological and orthographic phenomena that are observable in contemporary hebrew texts . reliance on finite-state technology facilitates the construction of a highly efficient , completely bidirectional system for analysis and generation . hamsah is currently the broadest-coverage and most accurate freely-available system for hebrew .

dual decomposition with many overlapping components
dual decomposition has been recently proposed as a way of combining complementary models , with a boost in predictive power . however , in cases where lightweight decompositions are not readily available ( e.g. , due to the presence of rich features or logical constraints ) , the original subgradient algorithm is inefficient . we sidestep that difficulty by adopting an augmented lagrangian method that accelerates model consensus by regularizing towards the averaged votes . we show how first-order logical constraints can be handled efficiently , even though the corresponding subproblems are no longer combinatorial , and report experiments in dependency parsing , with state-of-the-art results .

gf parallel resource grammars and russian
a resource grammar is a standard library for the gf grammar formalism . it raises the abstraction level of writing domainspecific grammars by taking care of the general grammatical rules of a language . gf resource grammars have been built in parallel for eleven languages and share a common interface , which simplifies multilingual applications . we reflect on our experience with the russian resource grammar trying to answer the questions : how well russian fits into the common interface and where the line between languageindependent and language-specific should be drawn .

automatic content-based categorization of wikipedia articles
wikipedias article contents and its category hierarchy are widely used to produce semantic resources which improve performance on tasks like text classification and keyword extraction . the reverse using text classification methods for predicting the categories of wikipedia articles has attracted less attention so far . we propose to return the favor and use text classifiers to improve wikipedia . this could support the emergence of a virtuous circle between the wisdom of the crowds and machine learning/nlp methods . we define the categorization of wikipedia articles as a multi-label classification task , describe two solutions to the task , and perform experiments that show that our approach is feasible despite the high number of labels .

morphological analyzer based on online learning
as mobile devices and web applications become popular , lightweight , client-side language analysis is more important than ever . we propose rakuten ma , a chinese/japanese morphological analyzer written in javascript . it employs an online learning algorithm scw , which enables client-side model update and domain adaptation . we have achieved a compact model size ( 5mb ) while maintaining the state-of-the-art performance , via techniques such as feature hashing , fobos , and feature quantization .

parsing conversational speech using enhanced segmentation
the lack of sentence boundaries and presence of disfluencies pose difficulties for parsing conversational speech . this work investigates the effects of automatically detecting these phenomena on a probabilistic parsers performance . we demonstrate that a state-of-the-art segmenter , relative to a pause-based segmenter , gives more than 45 % of the possible error reduction in parser performance , and that presentation of interruption points to the parser improves performance over using sentence boundaries alone .

japanese dependency analysis using cascaded chunking
in this paper , we propose a new statistical japanese dependency parser using a cascaded chunking model . conventional japanese statistical dependency parsers are mainly based on a probabilistic model , which is not always efficient or scalable . we propose a new method that is simple and efficient , since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side . experiments using the kyoto university corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

hybrid document indexing with spectral embedding
document representation has a large impact on the performance of document retrieval and clustering algorithms . we propose a hybrid document indexing scheme that combines the traditional bagof-words representation with spectral embedding . this method accounts for the specifics of the document collection and also uses semantic similarity information based on a large scale statistical analysis . clustering experiments showed improvements over the traditional tf-idf representation and over the spectral methods based solely on the document collection .

hierarchical mt training using max-violation perceptron
large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus ; for example the recent effort in phrase-based mt ( yu et al , 2013 ) significantly outperforms mainstream methods that only train on small tuning sets . however , phrase-based mt suffers from limited reorderings , and thus its training can only utilize a small portion of the bitext due to the distortion limit . to address this problem , we extend yu et al ( 2013 ) to syntax-based mt by generalizing their latent variable violation-fixing perceptron from graphs to hypergraphs . experiments confirm that our method leads to up to +1.2 bleu improvement over mainstream methods such as mert and pro .

complexity assumptions in ontology verbalisation
we describe the strategy currently pursued for verbalising owl ontologies by sentences in controlled natural language ( i.e. , combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals , classes , and properties ) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology . we then show , through analysis of a corpus of ontologies , that although these assumptions could in principle be violated , they are overwhelmingly respected in practice by ontology developers .

episodic memory for companion dialogue
we present an episodic memory component for enhancing the dialogue of artificial companions with the capability to refer to , take up and comment on past interactions with the user , and to take into account in the dialogue long-term user preferences and interests . the proposed episodic memory is based on rdf representations of the agents experiences and is linked to the agents semantic memory containing the agents knowledge base of ontological data and information about the users interests .

incremental grammar induction from child-directed
we describe a method for learning an incremental semantic grammar from data in which utterances are paired with logical forms representing their meaning . working in an inherently incremental framework , dynamic syntax , we show how words can be associated with probabilistic procedures for the incremental projection of meaning , providing a grammar which can be used directly in incremental probabilistic parsing and generation . we test this on child-directed utterances from the childes corpus , and show that it results in good coverage and semantic accuracy , without requiring annotation at the word level or any independent notion of syntax .

itu turkish nlp web service
we present a natural language processing ( nlp ) platform , namely the itu turkish nlp web service by the natural language processing group of istanbul technical university . the platform ( available at tools.nlp.itu.edu.tr ) operates as a saas ( software as a service ) and provides the researchers and the students the state of the art nlp tools in many layers : preprocessing , morphology , syntax and entity recognition . the users may communicate with the platform via three channels : 1. via a user friendly web interface , 2. by file uploads and 3. by using the provided web apis within their own codes for constructing higher level applications .

unsupervised tokenization for machine translation
training a statistical machine translation starts with tokenizing a parallel corpus . some languages such as chinese do not incorporate spacing in their writing system , which creates a challenge for tokenization . moreover , morphologically rich languages such as korean present an even bigger challenge , since optimal token boundaries for machine translation in these languages are often unclear . both rule-based solutions and statistical solutions are currently used . in this paper , we present unsupervised methods to solve tokenization problem . our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation .

search-aware tuning for machine translation
parameter tuning is an important problem in statistical machine translation , but surprisingly , most existing methods such as mert , mira and pro are agnostic about search , while search errors could severely degrade translation quality . we propose a searchaware framework to promote promising partial translations , preventing them from being pruned . to do so we develop two metrics to evaluate partial derivations . our technique can be applied to all of the three above-mentioned tuning methods , and extensive experiments on chinese-to-english and english-to-chinese translation show up to +2.6 bleu gains over search-agnostic baselines .

whats in a translation rule
we propose a theory that gives formal semantics to word-level alignments defined over parallel corpora . we use our theory to introduce a linear algorithm that can be used to derive from word-aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data .

arabic cross-document person name normalization
this paper presents a machine learning approach based on an svm classifier coupled with preprocessing rules for crossdocument named entity normalization . the classifier uses lexical , orthographic , phonetic , and morphological features . the process involves disambiguating different entities with shared name mentions and normalizing identical entities with different name mentions . in evaluating the quality of the clusters , the reported approach achieves a cluster f-measure of 0.93. the approach is significantly better than the two baseline approaches in which none of the entities are normalized or entities with exact name mentions are normalized . the two baseline approaches achieve cluster f-measures of 0.62 and 0.74 respectively . the classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system .

for probabilistic synchronous context-free grammars
we present a method for the computation of prefix probabilities for synchronous contextfree grammars . our framework is fairly general and relies on the combination of a simple , novel grammar transformation and standard techniques to bring grammars into normal forms .

models for inuktitut-english word alignment
this paper presents a set of techniques for bitext word alignment , optimized for a language pair with the characteristics of inuktitut-english . the resulting systems exploit cross-lingual affinities at the sublexical level of syllables and substrings , as well as regular patterns of transliteration and the tendency towards monotonicity of alignment . our most successful systems were based on classifier combination , and we found different combination methods performed best under the target evaluation metrics of f-measure and alignment error rate .

graphical models over multiple strings
we study graphical modeling in the case of stringvalued random variables . whereas a weighted finite-state transducer can model the probabilistic relationship between two strings , we are interested in building up joint models of three or more strings . this is needed for inflectional paradigms in morphology , cognate modeling or language reconstruction , and multiple-string alignment . we propose a markov random field in which each factor ( potential function ) is a weighted finite-state machine , typically a transducer that evaluates the relationship between just two of the strings . the full joint distribution is then a product of these factors . though decoding is actually undecidable in general , we can still do efficient joint inference using approximate belief propagation ; the necessary computations and messages are all finitestate . we demonstrate the methods by jointly predicting morphological forms .

unsupervised ontology induction from text
extracting knowledge from unstructured text is a long-standing goal of nlp . although learning approaches to many of its subtasks have been developed ( e.g. , parsing , taxonomy induction , information extraction ) , all end-to-end solutions to date require heavy supervision and/or manual engineering , limiting their scope and scalability . we present ontousp , a system that induces and populates a probabilistic ontology using only dependency-parsed text as input . ontousp builds on the usp unsupervised semantic parser by jointly forming isa and is-part hierarchies of lambda-form clusters . the isa hierarchy allows more general knowledge to be learned , and the use of smoothing for parameter estimation . we evaluate ontousp by using it to extract a knowledge base from biomedical abstracts and answer questions . ontousp improves on the recall of usp by 47 % and greatly outperforms previous state-of-the-art approaches .

frustratingly easy semi-supervised domain adaptation
in this work , we propose a semisupervised extension to a well-known supervised domain adaptation approach ( ea ) ( daume iii , 2007 ) . our proposed approach ( ea++ ) builds on the notion of augmented space ( introduced in ea ) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target . this semisupervised approach to domain adaptation is extremely simple to implement , and can be applied as a pre-processing step to any supervised learner . experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method .

types and records for predication
this paper studies the use of records and dependent types in gf ( grammatical framework ) to build a grammar for predication with an unlimited number of subcategories , also covering extraction and coordination . the grammar is implemented for chinese , english , finnish , and swedish , sharing the maximum of code to identify similarities and differences between the languages . equipped with a probabilistic model and a large lexicon , the grammar has also been tested in widecoverage machine translation . the first evaluations show improvements in parsing speed , coverage , and robustness in comparison to earlier gf grammars . the study confirms that dependent types , records , and functors are useful in both engineering and theoretical perspectives .

learning to resolve bridging references
we use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references . we find that using first mention , utterance distance , and lexical distance computed using either google or wordnet results in an accuracy significantly higher than obtained in previous experiments .

the ldv-combo system for smt
we describe the ldv-combo system presented at the shared task . our approach explores the possibility of working with alignments at different levels of abstraction using different degrees of linguistic analysis from the lexical to the shallow syntactic level . translation models are built on top of combinations of these alignments . we present results for the spanish-to-english and english-tospanish tasks . we show that liniguistic information may be helpful , specially when the target language has a rich morphology .

morphology induction from term clusters
we address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources . like previous work in this area , our approach exploits orthographic regularities in a search for possible morphological segmentation points . instead of affixes , however , we search for affix transformation rules that express correspondences between term clusters induced from the data . this focuses the system on substrings having syntactic function , and yields clusterto-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately . a stem-weighting algorithm based on hubs and authorities is used to clarify ambiguous segmentation points . we evaluate our approach using the celex database .

conditional responses in information-seeking dialogues
the paper deals with conditional responses of the form not if c/yes if c in reply to a question q in the context of information-seeking dialogues . a conditional response is triggered if the obtainability of q depends on whether c holds : the response indicates a possible need to find alternative solutions , opening a negotiation in the dialogue . the paper discusses the conditions under which conditional responses are appropriate , and proposes a uniform approach to their generation and interpretation .

abstract meaning representation for sembanking
we describe abstract meaning representation ( amr ) , a semantic representation language in which we are writing down the meanings of thousands of english sentences . we hope that a sembank of simple , whole-sentence semantic structures will spur new work in statistical natural language understanding and generation , like the penn treebank encouraged work on statistical parsing . this paper gives an overview of amr and tools associated with it .

landmark classification for route directions
in order for automated navigation systems to operate effectively , the route instructions they produce must be clear , concise and easily understood by users . in order to incorporate a landmark within a coherent sentence , it is necessary to first understand how that landmark is conceptualised by travellers whether it is perceived as point-like , linelike or area-like . this paper investigates the viability of automatically classifying the conceptualisation of landmarks relative to a given city context . we use web data to learn the default conceptualisation of those landmarks , crucially analysing preposition and verb collocations in the classification .

the sentiment analysis of tweets
this paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets . in a semeval-2014 shared task ( task 9 ) , our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets . in the message-level sentiment classification task , our submissions obtained highest scores on the livejournal blog posts test set , sarcastic tweets test set , and the 2013 sms test set . these systems build on our semeval-2013 sentiment analysis systems which ranked first in both the termand message-level subtasks in 2013. key improvements over the 2013 systems are in the handling of negation . we create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts .

confidence driven unsupervised semantic parsing
current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain . this supervision bottleneck is one of the major difficulties in scaling up semantic parsing . we argue that a semantic parser can be trained effectively without annotated data , and introduce an unsupervised learning algorithm . the algorithm takes a self training approach driven by confidence estimation . evaluated over geoquery , a standard dataset for this task , our system achieved 66 % accuracy , compared to 80 % of its fully supervised counterpart , demonstrating the promise of unsupervised approaches for this task .

a unicode based adaptive segmentor
this paper presents a unicode based chinese word segmentor . it can handle chinese text in simplified , traditional , or mixed mode . the system uses the strategy of divide-and-conquer to handle the recognition of personal names , numbers , time and numerical values , etc in the preprocessing stage . the segmentor further uses tagging information to work on disambiguation . adopting a modular design approach , different functional parts are separately implemented using different modules and each module tackles one problem at a time providing more flexibility and extensibility . results show that with added pre-processing modules and accessorial modules , the accuracy of the segmentor is increased and the system is easily adaptive to different applications .

variable bit quantisation for lsh
we introduce a scheme for optimally allocating a variable number of bits per lsh hyperplane . previous approaches assign a constant number of bits per hyperplane . this neglects the fact that a subset of hyperplanes may be more informative than others . our method , dubbed variable bit quantisation ( vbq ) , provides a datadriven non-uniform bit allocation across hyperplanes . despite only using a fraction of the available hyperplanes , vbq outperforms uniform quantisation by up to 168 % for retrieval across standard text and image datasets .

semantic structure from correspondence analysis
a common problem for clustering techniques is that clusters overlap , which makes graphing the statistical structure in the data difficult . a related problem is that we often want to see the distribution of factors ( variables ) as well as classes ( objects ) . correspondence analysis ( ca ) offers a solution to both these problems . the structure that ca discovers may be an important step in representing similarity . we have performed an analysis for italian verbs and nouns , and confirmed that similar structures are found for english .

comparing language similarity across genetic
recent studies have shown the potential benefits of leveraging resources for resource-rich languages to build tools for similar , but resource-poor languages . we examine what constitutes similarity by comparing traditional phylogenetic language groups , which are motivated largely by genetic relationships , with language groupings formed by clustering methods using typological features only . using data from the world atlas of language structures ( wals ) , our preliminary experiments show that typologically-based clusters look quite different from genetic groups , but perform as good or better when used to predict feature values of member languages .

learning to annotate scientific publications
annotating scientific publications with keywords and phrases is of great importance to searching , indexing , and cataloging such documents . unlike previous studies that focused on usercentric annotation , this paper presents our investigation of various annotation characteristics on service-centric annotation . using a large number of publicly available annotated scientific publications , we characterized and compared the two different types of annotation processes . furthermore , we developed an automatic approach of annotating scientific publications based on a machine learning algorithm and a set of novel features . when compared to other methods , our approach shows significantly improved performance . experimental data sets and evaluation results are publicly available at the supplementary website1 .

quantitative portraits of lexical elements
this paper clarifies the basic concepts and theoretical perspectives by and from which quantitative weighting of lexical elements are defined , and then draws , quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined .

symbolic preference using simple scoring
despite the popularity of stochastic parsers , symbolic parsing still has some advantages , but is not practical without an effective mechanism for selecting among alternative analyses . this paper describes the symbolic preference system of a hybrid parser that combines a shallow parser with an overlay parser that builds on the chunks . the hybrid currently equals or exceeds most stochastic parsers in speed and is approaching them in accuracy . the preference system is novel in using a simple , three-valued scoring method ( -1 , 0 , or +1 ) for assigning preferences to constituents viewed in the context of their containing constituents . the approach addresses problems associated with earlier preference systems , and has considerably facilitated development . it is ultimately based on viewing preference scoring as an engineering mechanism , and only indirectly related to cognitive principles or corpus-based frequencies .

translating dialectal arabic to english
we present a dialectal egyptian arabic to english statistical machine translation system that leverages dialectal to modern standard arabic ( msa ) adaptation . in contrast to previous work , we first narrow down the gap between egyptian and msa by applying an automatic characterlevel transformational model that changes egyptian to eg , which looks similar to msa . the transformations include morphological , phonological and spelling changes . the transformation reduces the out-of-vocabulary ( oov ) words from 5.2 % to 2.6 % and gives a gain of 1.87 bleu points . further , adapting large msa/english parallel data increases the lexical coverage , reduces oovs to 0.7 % and leads to an absolute bleu improvement of 2.73 points .

knowledge-free induction of inflectional morphologies
we propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input . our algorithm combines cues from orthography , semantics , and syntactic distributions to induce morphological relationships in german , dutch , and english . using celex as a gold standard for evaluation , we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed .

translating from english into german
we present the cims submissions to the 2014 shared task for the language pair ende . we address the major problems that arise when translating into german : complex nominal and verbal morphology , productive compounding and flexible word ordering . our morphologyaware translation systems handle word formation issues on different levels of morpho-syntactic modeling .

wide coverage symbolic surface realization
recent evaluation techniques applied to corpusbased systems have been introduced that can predict quantitatively how well surface realizers will generate unseen sentences in isolation . we introduce a similar method for determining the coverage on the fuf/surge symbolic surface realizer , report that its coverage and accuracy on the penn treebank is higher than that of a similar statistics-based generator , describe several bene ts that can be used in other areas of computational linguistics , and present an updated version of surge for use in the nlg community .

discovering topical aspects in microblogs
we address the problem of discovering topical phrases or aspects from microblogging sites like twitter , that correspond to key talking points or buzz around a particular topic or entity of interest . inferring such topical aspects enables various applications such as trend detection and opinion mining for business analytics . however , mining high-volume microblog streams for aspects poses unique challenges due to the inherent noise , redundancy and ambiguity in users social posts . we address these challenges by using a probabilistic model that incorporates various global and local indicators such as uniqueness , diversity and burstiness of phrases , to infer relevant aspects . our model is learned using an em algorithm that uses automatically generated noisy labels , without requiring manual effort or domain knowledge . we present results on three months of twitter data across different types of entities to validate our approach .

the stages of event extraction
event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty . in this paper , we present a simple , modular approach to event extraction that allows us to experiment with a variety of machine learning methods for these sub-tasks , as well as to evaluate the impact on performance these sub-tasks have on the overall task .

a recursive statistical translation model
a new model for statistical translation is presented . a novel feature of this model is that the alignments it produces are hierarchically arranged . the generative process begins by splitting the input sentence in two parts . each of the parts is translated by a recursive application of the model and the resulting translation are then concatenated . if the sentence is small enough , a simpler model ( in our case ibms model 1 ) is applied . the training of the model is explained . finally , the model is evaluated using the corpora from a large vocabulary shared task .

citation summarization through keyphrase extraction
this paper presents an approach to summarize single scientific papers , by extracting its contributions from the set of citation sentences written in other papers . our methodology is based on extracting significant keyphrases from the set of citation sentences and using these keyphrases to build the summary . comparisons show how this methodology excels at the task of single paper summarization , and how it out-performs other multi-document summarization methods .

wide-coverage parsing of speech transcripts
this paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts . two parsers ( c & c ccg and rasp ) are tested on the speech transcripts of two different domains ( parent-child language , and picture descriptions ) . the performance difference between the domain-independent parsers and two domain-trained parsers ( mstparser and megrasp ) is substantial , with a difference of at least 30 percent point in accuracy . despite this gap , some of the grammatical relations can still be recovered reliably .

k-means clustering with feature hashing
one of the major problems of k-means is that one must use dense vectors for its centroids , and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional . we address this issue by using feature hashing ( weinberger et al. , 2009 ) , a dimension-reduction technique , which can reduce the size of dense vectors while retaining sparsity of sparse vectors . our analysis gives theoretical motivation and justification for applying feature hashing to kmeans , by showing how much will the objective ofk-means be ( additively ) distorted . furthermore , to empirically verify our method , we experimented on a document clustering task .

descriptive question answering in encyclopedia
recently there is a need for a qa system to answer not only factoid questions but also descriptive questions . descriptive questions are questions which need answers that contain definitional information about the search term or describe some special events . we have proposed a new descriptive qa model and presented the result of a system which we have built to answer descriptive questions . we defined 10 descriptive answer type ( dat ) s as answer types for descriptive questions . we discussed how our proposed model was applied to the descriptive question with some experiments .

prosodic correlates of rhetorical relations
this paper investigates the usefulness of prosodic features in classifying rhetorical relations between utterances in meeting recordings . five rhetorical relations of contrast , elaboration , summary , question and cause are explored . three training methods - supervised , unsupervised , and combined - are compared , and classification is carried out using support vector machines . the results of this pilot study are encouraging but mixed , with pairwise classification achieving an average of 68 % accuracy in discerning between relation pairs using only prosodic features , but multi-class classification performing only slightly better than chance .

parser combination by reparsing
we present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers . we apply this idea to dependency and constituent parsing , generating results that surpass state-of-theart accuracy levels for individual parsers .

concept discovery from text
broad-coverage lexical resources such as wordnet are extremely useful . however , they often include many rare senses while missing domain-specific senses . we present a clustering algorithm called cbc ( clustering by committee ) that automatically discovers concepts from text . it initially discovers a set of tight clusters called committees that are well scattered in the similarity space . the centroid of the members of a committee is used as the feature vector of the cluster . we proceed by assigning elements to their most similar cluster . evaluating cluster quality has always been a difficult task . we present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from wordnet ( the answer key ) . our experiments show that cbc outperforms several well-known clustering algorithms in cluster quality .

transforming lexica as trees
we investigate the problem of structurally changing lexica , while preserving the information . we present a type of lexicon transformation that is complete on an interesting class of lexica . our work is motivated by the problem of merging one or more lexica into one lexicon . lexica , lexicon schemas , and lexicon transformations are all seen as particular kinds of trees .

modelling atypical syntax processing
we evaluate the inferences that can be drawn from dissociations in syntax processing identified in developmental disorders and acquired language deficits . we use an srn to simulate empirical data from dick et al ( 2001 ) on the relative difficulty of comprehending different syntactic constructions under normal conditions and conditions of damage . we conclude that task constraints and internal computational constraints interact to predict patterns of difficulty . difficulty is predicted by frequency of constructions , by the requirement of the task to focus on local vs. global sequence information , and by the ability of the system to maintain sequence information . we generate a testable prediction on the empirical pattern that should be observed under conditions of developmental damage .

english-to-czech factored machine translation
this paper describes experiments with english-to-czech phrase-based machine translation . additional annotation of input and output tokens ( multiple factors ) is used to explicitly model morphology . we vary the translation scenario ( the setup of multiple factors ) and the amount of information in the morphological tags . experimental results demonstrate significant improvement of translation quality in terms of bleu .

paraphrasing for automatic evaluation
this paper studies the impact of paraphrases on the accuracy of automatic evaluation . given a reference sentence and a machine-generated sentence , we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference . we apply our paraphrasing method in the context of machine translation evaluation . our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation . we also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation .

segmentation as dictionary stress
stress is a useful cue for english word segmentation . a wide range of computational models have found that stress cues enable a 2-10 % improvement in segmentation accuracy , depending on the kind of model , by using input that has been annotated with stress using a pronouncing dictionary . however , stress is neither invariably produced nor unambiguously identifiable in real speech . heavy syllables , i.e . those with long vowels or syllable codas , attract stress in english . we devise adaptor grammar word segmentation models that exploit either stress , or syllable weight , or both , and evaluate the utility of syllable weight as a cue to word boundaries . our results suggest that syllable weight encodes largely the same information for word segmentation in english that annotated dictionary stress does .

living up to standards
this paper attacks one part of the question `` are evaluation methods , metrics and resources reusable '' by arguing that a set of iso standards developed for the evaluation of software in general are as applicable to natural language processing software as to any other . main features of the iso proposals are presented , and a number of applications where they have been applied are mentioned , although not discussed in any detail . acknowledgements the work recorded here is far from being all my own . i would like first to record my thanks to nigel bevan , technical editor of the iso standards discussed for much interesting and enlightening discussion . then many thanks must go to all my colleagues in the eagles and isle projects , especially sandra manzi and andrei popescu-belis .

ranking paraphrases in context
we present a vector space model that supports the computation of appropriate vector representations for words in context , and apply it to a paraphrase ranking task . an evaluation on the semeval 2007 lexical substitution task data shows promising results : the model significantly outperforms a current state of the art model , and our treatment of context is effective .

semi-automatic entity set refinement
state of the art set expansion algorithms produce varying quality expansions for different entity types . even for the highest quality expansions , errors still occur and manual refinements are necessary for most practical uses . in this paper , we propose algorithms to aide this refinement process , greatly reducing the amount of manual labor required . the methods rely on the fact that most expansion errors are systematic , often stemming from the fact that some seed elements are ambiguous . using our methods , empirical evidence shows that average r-precision over random entity sets improves by 26 % to 51 % when given from 5 to 10 manually tagged errors . both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems .

online-monitoring of security-related events
this paper presents a fully operational real-time event extraction system which is capable of accurately and efficiently extracting violent and natural disaster events from vast amount of online news articles per day in different languages . due to the requirement that the system must be multilingual and easily extendable , it is based on a shallow linguistic analysis . the event extraction results can be viewed on a publicly accessible website .

identifying multi-word expressions by
multi-word expressions constitute a significant portion of the lexicon of every natural language , and handling them correctly is mandatory for various nlp applications . yet such entities are notoriously hard to define , and are consequently missing from standard lexicons and dictionaries . multi-word expressions exhibit idiosyncratic behavior on various levels : orthographic , morphological , syntactic and semantic . in this work we take advantage of the morphological and syntactic idiosyncrasy of hebrew noun compounds and employ it to extract such expressions from text corpora . we show that relying on linguistic information dramatically improves the accuracy of compound extraction , reducing over one third of the errors compared with the best baseline .

calendar expressions in texts
temporal expressions that refer to a part of a calendar area in terms of common calendar divisions are studied . our claim is that such a calendar expression '' ( ce ) can be described by a succession of operators operating on a calendar base ( cb ) . these operators are categorized : a pointing operator that transform a cb into a ce ; a focalizing/shifting operator that reduces or shifts the ce into another ce , and finally a zoning operator that provides the wanted ce from this last ce . relying on these operators , a set of annotations is presented which are used to automatically annotate biographic texts . a software application , plugged in the platformnavitext , is described that builds a calendar view of a biographic text . 365 366 battistelli , couto , minel , and schwer

mildly context-sensitive dependency languages
dependency-based representations of natural language syntax require a fine balance between structural flexibility and computational complexity . in previous work , several constraints have been proposed to identify classes of dependency structures that are wellbalanced in this sense ; the best-known but also most restrictive of these is projectivity . most constraints are formulated on fully specified structures , which makes them hard to integrate into models where structures are composed from lexical information . in this paper , we show how two empirically relevant relaxations of projectivity can be lexicalized , and how combining the resulting lexicons with a regular means of syntactic composition gives rise to a hierarchy of mildly context-sensitive dependency languages .

automatically predicting peer-review helpfulness
identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers . as a first step towards enhancing existing peerreview systems with new functionality based on helpfulness detection , we examine whether standard product review analysis techniques also apply to our new context of peer reviews . in addition , we investigate the utility of incorporating additional specialized features tailored to peer review . our preliminary results show that the structural features , review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews , while peer-review specific auxiliary features can further improve helpfulness prediction .

active learning with confidence
active learning is a machine learning approach to achieving high-accuracy with a small amount of labels by letting the learning algorithm choose instances to be labeled . most of previous approaches based on discriminative learning use the margin for choosing instances . we present a method for incorporating confidence into the margin by using a newly introduced online learning algorithm and show empirically that confidence improves active learning .

some considerations on guidelines
despite progress in the development of computational means , human input is still critical in the production of consistent and useable aligned corpora and term banks . this is especially true for specialized corpora and term banks whose end-users are often professionals with very stringent requirements for accuracy , consistency and coverage . in the compilation of a high quality chinese-english legal glossary for eldos project , we have identified a number of issues that make the role human input critical for term alignment and extraction . they include the identification of low frequency terms , paraphrastic expressions , discontinuous units , and maintaining consistent term granularity , etc . although manual intervention can more satisfactorily address these issues , steps must also be taken to address intra- and inter-annotator inconsistency . keyword : legal terminology , bilingual terminology , bilingual alignment , corpus-based linguistics

shrinking exponential language models
in ( chen , 2009 ) , we show that for a variety of language models belonging to the exponential family , the test set cross-entropy of a model can be accurately predicted from its training set cross-entropy and its parameter values . in this work , we show how this relationship can be used to motivate two heuristics for shrinking the size of a language model to improve its performance . we use the first heuristic to develop a novel class-based language model that outperforms a baseline word trigram model by 28 % in perplexity and 1.9 % absolute in speech recognition word-error rate on wall street journal data . we use the second heuristic to motivate a regularized version of minimum discrimination information models and show that this method outperforms other techniques for domain adaptation .

automatically discovering word senses
we will demonstrate the output of a distributional clustering algorithm called clustering by committee that automatically discovers word senses from text1 .

inference and computational semantics
this paper discusses inference in computational semantics. we argue that state of - the - art methods in rst-order theorem proving and model building are of direct relevance to inference for natural language processing. we support our claim by discussing our implementation of van der sandt ' s presupposition pro jection algorithm in discourse representation theory, an algorithm which demands sustained use of powerful inference mechanisms .

wrapping of trees
we explore the descriptive power , in terms of syntactic phenomena , of a formalism that extends treeadjoining grammar ( tag ) by adding a fourth level of hierarchical decomposition to the three levels tag already employs . while extending the descriptive power minimally , the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass , an account that employs unitary elementary structures and eschews synchronized derivation operations , and which is , in many respects , closer to the spirit of the intuitions underlying tag-based linguistic theory than previously considered extensions to tag .

chunk parsing revisited
chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use . in this paper we show that chunk parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking . experimental results with the penn treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed ( 14 msec/sentence ) . we also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .

k-best a parsing
a parsing makes 1-best search efficient by suppressing unlikely 1-best items . existing kbest extraction methods can efficiently search for top derivations , but only after an exhaustive 1-best pass . we present a unified algorithm for k-best a parsing which preserves the efficiency of k-best extraction while giving the speed-ups of a methods . our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best a parser . empirically , optimal k-best lists can be extracted significantly faster than with other approaches , over a range of grammar types .

generalized multitext grammars
generalized multitext grammar ( gmtg ) is a synchronous grammar formalism that is weakly equivalent to linear context-free rewriting systems ( lcfrs ) , but retains much of the notational and intuitive simplicity of context-free grammar ( cfg ) . gmtg allows both synchronous and independent rewriting . such flexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalisms . this paper investigates the generative capacity of gmtg , proves that each component grammar of a gmtg retains its generative power , and proposes a generalization of chomsky normal form , which is necessary for synchronous cky-style parsing .

discourse-constrained temporal annotation
we describe an experiment on a temporal ordering task in this paper . we show that by selecting event pairs based on discourse structure and by modifying the pre-existent temporal classification scheme to fit the data better , we significantly improve inter-annotator agreement , as well as broaden the coverage of the task . we also present analysis of the current temporal classification scheme and propose ways to improve it in future work .

multi-domain sentiment classification
this paper addresses a new task in sentiment classification , called multi-domain sentiment classification , that aims to improve performance through fusing training data from multiple domains . to achieve this , we propose two approaches of fusion , feature-level and classifier-level , to use training data from multiple domains simultaneously . experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification ( using the training data individually ) .

beyond the correlation
this paper is part of our broader investigation into the utility of discourse structure for performance analysis . in our previous work , we showed that several interaction parameters that use discourse structure predict our performance metric . here , we take a step forward and show that these correlations are not only a surface relationship . we show that redesigning the system in light of an interpretation of a correlation has a positive impact .

almut silja hildebrand
in this paper we present our entry to the wmt13 shared task : quality estimation ( qe ) for machine translation ( mt ) . we participated in the 1.1 , 1.2 and 1.3 sub-tasks with our qe system trained on features from diverse information sources like mt decoder features , n-best lists , mono- and bi-lingual corpora and giza training models . our system shows competitive results in the workshop shared task .

punjabi machine transliteration
machine transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language . it is useful for machine translation , cross-lingual information retrieval , multilingual text and speech processing . punjabi machine transliteration ( pmt ) is a special case of machine transliteration and is a process of converting a word from shahmukhi ( based on arabic script ) to gurmukhi ( derivation of landa , shardha and takri , old scripts of indian subcontinent ) , two scripts of punjabi , irrespective of the type of word . the punjabi machine transliteration system uses transliteration rules ( character mappings and dependency rules ) for transliteration of shahmukhi words into gurmukhi . the pmt system can transliterate every word written in shahmukhi .

simone paolo ponzetto
we present our work on using wikipedia as a knowledge source for natural language processing . we first describe our previous work on computing semantic relatedness from wikipedia , and its application to a machine learning based coreference resolution system . our results suggest that wikipedia represents a semantic resource to be treasured for nlp applications , and accordingly present the work directions to be explored in the future .

cross-linguistic phoneme correspondences
cross-linguistic phoneme correspondences , or metaphonemes1 , can be defined across languages which are relatively closely related in exactly the same way as correspondences can be defined for dialects , or accents , of a single language ( e.g . oconnor , 1973 ; fitt , 2001 ) . in this paper we present the theory of metaphonemes , comparing them with traditional archi- and morphophonemes as well as with similar work using keysymbols done for accents of english . we describe the metaphoneme inventory defined for dutch , english and german , comparing the results for vowels and consonants . we also describe some of the unexpected information that arose from the analysis of cognate forms we undertook to find the metaphoneme correspondences .

processing spontaneous orthography
in cases in which there is no standard orthography for a language or language variant , written texts will display a variety of orthographic choices . this is problematic for natural language processing ( nlp ) because it creates spurious data sparseness . we study the transformation of spontaneously spelled egyptian arabic into a conventionalized orthography which we have previously proposed for nlp purposes . we show that a two-stage process can reduce divergences from this standard by 69 % , making subsequent processing of egyptian arabic easier .

alignment by agreement
we present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models . compared to the standard practice of intersecting predictions of independently-trained models , joint training provides a 32 % reduction in aer . moreover , a simple and efficient pair of hmm aligners provides a 29 % reduction in aer over symmetrized ibm model 4 predictions .

parsing biomedical literature
techniques evaluated on the genia corpus of medline abstracts [ 1,2 ] . we begin by observing that the penn treebank ( ptb ) is lexically impoverished when measured on various genres of scientific and technical writing , and that this significantly impacts parse accuracy . to resolve this without requiring in-domain treebank data , we show how existing domain-specific lexical resources may be leveraged to augment ptb-training : part-of-speech tags , dictionary collocations , and namedentities . using a state-of-the-art statistical parser [ 3 ] as our baseline , our lexically-adapted parser achieves a 14.2 % reduction in error . with oracleknowledge of named-entities , this error reduction improves to 21.2 % .

arabic tokenization system
tokenization is a necessary and non-trivial step in natural language processing . in the case of arabic , where a single word can comprise up to four independent tokens , morphological knowledge needs to be incorporated into the tokenizer . in this paper we describe a rule-based tokenizer that handles tokenization as a full-rounded process with a preprocessing stage ( white space normalizer ) , and a post-processing stage ( token filter ) . we also show how it handles multiword expressions , and how ambiguity is resolved .

polysemy-aware verb classes
we present an unsupervised method for inducing verb classes from verb uses in gigaword corpora . our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames . by taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering . in our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger comprising 20 billion words . the effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data .

symmetric probabilistic alignment
we recently decided to develop a new alignment algorithm for the purpose of improving our example-based machine translation ( ebmt ) systems performance , since subsentential alignment is critical in locating the correct translation for a matched fragment of the input . unlike most algorithms in the literature , this new symmetric probabilistic alignment ( spa ) algorithm treats the source and target languages in a symmetric fashion . in this short paper , we outline our basic algorithm and some extensions for using context and positional information , and compare its alignment accuracy on the romanian-english data for the shared task with ibm model 4 and the reported results from the prior workshop .

